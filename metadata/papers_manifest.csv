"Item type","Authors","Editors","Title","Journal","Journal (full title)","Book title","Website name","Proceedings title","Source","How published","Publication year","Volume","Issue","Pages","Folders filed in","Labels filed in","Institution","Number","Publisher","Publisher place","Conference","Conference location","Conference date","Date published","Date","Date accessed","ISBN","ISBN (alt.)","ISSN","ISSN (alt.)","URLs","DOI","PMID","Arxiv ID","Associated DOI","Abstract","Keywords","Notes","Copyright","Affiliation","Language","Subtype","Series","Series number","Archive prefix","Eprint ID","Primary class","Page count","University","Department","Collection","Inventors","Assignee","Issuing authority","Patent number","Date filed","Type of patent","Issuing office full name","Application number","Presenter","Attachments","University place","Level of thesis","Series Number","Library catalog/database"
"Preprint","Xia P,Zeng K,Liu J,Qin C,Wu F,Zhou Y,Xiong C,Yao H","","Agent0: Unleashing self-evolving agents from zero data via tool-integrated reasoning","","","","","","arXiv [cs.LG]","","2025","","","","All","","","","","","","","","2025-11-20","","","","","","","http://dx.doi.org/10.48550/arXiv.2511.16043;http://arxiv.org/abs/2511.16043","10.48550/arXiv.2511.16043","","2511.16043","","Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2511.16043","cs.LG","","","","","","","","","","","","","","All Papers/X/Xia et al. 2025 - Agent0 - Unleashing self-evolving agents from zero data via tool-integrated reasoning.pdf","","","",""
"Preprint","Sarkar S","","AI agents, productivity, and higher-order thinking: Early evidence from software development","","","","","","","","2025","","","","All","","","","","","","","","2025","","","","","","","http://dx.doi.org/10.2139/ssrn.5713646","10.2139/ssrn.5713646","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Liao CC,Liao D,Gadiraju SS","","AgentMaster: A multi-agent conversational framework using A2A and MCP protocols for multimodal information retrieval and analysis","","","","","","arXiv [cs.IR]","","2025","","","","All","","","","","","","","","2025-09-19","","","","","","","http://arxiv.org/abs/2507.21105","","","2507.21105","","The rise of Multi-Agent Systems (MAS) in Artificial Intelligence (AI), especially integrated with Large Language Models (LLMs), has greatly facilitated the resolution of complex tasks. However, current systems are still facing challenges of inter-agent communication, coordination, and interaction with heterogeneous tools and resources. Most recently, the Model Context Protocol (MCP) by Anthropic and Agent-to-Agent (A2A) communication protocol by Google have been introduced, and to the best of our knowledge, very few applications exist where both protocols are employed within a single MAS framework. We present a pilot study of AgentMaster, a novel modular multi-protocol MAS framework with self-implemented A2A and MCP, enabling dynamic coordination, flexible communication, and rapid development with faster iteration. Through a unified conversational interface, the system supports natural language interaction without prior technical expertise and responds to multimodal queries for tasks including information retrieval, question answering, and image analysis. The experiments are validated through both human evaluation and quantitative metrics, including BERTScore F1 (96.3%) and LLM-as-a-Judge G-Eval (87.1%). These results demonstrate robust automated inter-agent coordination, query decomposition, task allocation, dynamic routing, and domain-specific relevant responses. Overall, our proposed framework contributes to the potential capabilities of domain-specific, cooperative, and scalable conversational AI powered by MAS.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2507.21105","cs.IR","","","","","","","","","","","","","","All Papers/L/Liao et al. 2025 - AgentMaster - A multi-agent conversational framework ... A and MCP protocols for multimodal information retrieval and analysis.pdf","","","",""
"Website","","","","","","","","","","","","","","","All","","","","","","","","","","","2025-10-15","","","","","https://scholar.google.com/scholar?cites=9546718639074168570&as_sdt=2005&sciodt=0,5&hl=en#d=gs_qabs&t=1760494616785&u=#p=Y7QOjqbFEHMJ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Lemos F,Alves V,Ferraz F","","Is it time to treat prompts as code? A multi-use case study for prompt optimization using DSPy","","","","","","arXiv [cs.SE]","","2025","","","","All","","","","","","","","","2025-07-04","","","","","","","http://arxiv.org/abs/2507.03620","","","2507.03620","","Although prompt engineering is central to unlocking the full potential of Large Language Models (LLMs), crafting effective prompts remains a time-consuming trial-and-error process that relies on human intuition. This study investigates Declarative Self-improving Python (DSPy), an optimization framework that programmatically creates and refines prompts, applied to five use cases: guardrail enforcement, hallucination detection in code, code generation, routing agents, and prompt evaluation. Each use case explores how prompt optimization via DSPy influences performance. While some cases demonstrated modest improvements - such as minor gains in the guardrails use case and selective enhancements in hallucination detection - others showed notable benefits. The prompt evaluation criterion task demonstrated a substantial performance increase, rising accuracy from 46.2% to 64.0%. In the router agent case, the possibility of improving a poorly performing prompt and of a smaller model matching a stronger one through optimized prompting was explored. Although prompt refinement increased accuracy from 85.0% to 90.0%, using the optimized prompt with a cheaper model did not improve performance. Overall, this study's findings suggest that DSPy's systematic prompt optimization can enhance LLM performance, particularly when instruction tuning and example selection are optimized together. However, the impact varies by task, highlighting the importance of evaluating specific use cases in prompt optimization research.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2507.03620","cs.SE","","","","","","","","","","","","","","All Papers/L/Lemos et al. 2025 - Is it time to treat prompts as code - A multi-use case study for prompt optimization using DSPy.pdf","","","",""
"Preprint","Singhvi A,Shetty M,Tan S,Potts C,Sen K,Zaharia M,Khattab O","","DSPy Assertions: Computational constraints for self-refining language model pipelines","","","","","","arXiv [cs.CL]","","2023","","","","All","","","","","","","","","2023-12-20","","","","","","","http://arxiv.org/abs/2312.13382","","","2312.13382","","Chaining language model (LM) calls as composable modules is fueling a new powerful way of programming. However, ensuring that LMs adhere to important constraints remains a key challenge, one often addressed with heuristic ""prompt engineering"". We introduce LM Assertions, a new programming construct for expressing computational constraints that LMs should satisfy. We integrate our constructs into the recent DSPy programming model for LMs, and present new strategies that allow DSPy to compile programs with arbitrary LM Assertions into systems that are more reliable and more accurate. In DSPy, LM Assertions can be integrated at compile time, via automatic prompt optimization, and/or at inference time, via automatic selfrefinement and backtracking. We report on two early case studies for complex question answering (QA), in which the LM program must iteratively retrieve information in multiple hops and synthesize a long-form answer with citations. We find that LM Assertions improve not only compliance with imposed rules and guidelines but also enhance downstream task performance, delivering intrinsic and extrinsic gains up to 35.7% and 13.3%, respectively. Our reference implementation of LM Assertions is integrated into DSPy at https://github.com/stanfordnlp/dspy","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2312.13382","cs.CL","","","","","","","","","","","","","","All Papers/S/Singhvi et al. 2023 - DSPy Assertions - Computational constraints for self-refining language model pipelines.pdf","","","",""
"Preprint","Khattab O,Singhvi A,Maheshwari P,Zhang Z,Santhanam K,Vardhamanan S,Haq S,Sharma A,Joshi TT,Moazam H,Miller H,Zaharia M,Potts C","","DSPy: Compiling declarative language model calls into self-improving pipelines","","","","","","arXiv [cs.CL]","","2023","","","","All","","","","","","","","","2023-10-05","","","","","","","http://arxiv.org/abs/2310.03714","","","2310.03714","","The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded ""prompt templates"", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tackle multi-hop retrieval, answer complex questions, and control agent loops. Within minutes of compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-bootstrap pipelines that outperform standard few-shot prompting (generally by over 25% and 65%, respectively) and pipelines with expert-created demonstrations (by up to 5-46% and 16-40%, respectively). On top of that, DSPy programs compiled to open and relatively small LMs like 770M-parameter T5 and llama2-13b-chat are competitive with approaches that rely on expert-written prompt chains for proprietary GPT-3.5. DSPy is available at https://github.com/stanfordnlp/dspy","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2310.03714","cs.CL","","","","","","","","","","","","","","All Papers/K/Khattab et al. 2023 - DSPy - Compiling declarative language model calls into self-improving pipelines.pdf","","","",""
"Website","Corporation EMC","","EMC Data Domain Global Deduplication Array","","","","","","","","2012","","","","All","","","","","","","","","2012","","","","","","","https://www.delltechnologies.com/asset/en-us/products/storage/industry-market/h11731-wp-data-domain-global-dedupe-array.pdf","","","","","","","Aggregate ingest into tens of TB/h across multiple controllers","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Commvault,Storage P","","Modernize Data Protection with Commvault and Pure Storage FlashBlade","","","","","","","","2023","","","","All","","","","","","","","","2023","","","","","","","https://learn.purestorage.com/asset/887dce5a-7c3f-11ee-9ae5-0675c7f6b47a/Commvault_Pure_FlashBlade_Solution.pdf","","","","","","","Reference architecture showing multi-GB/s backup/restore; up to 270 TB/h in large configs","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","America K","","KIOXIA CM6 Series Enterprise NVMe SSDs","","","","","","","","2020","","","","p-scailbib;All","","","","","","","","","2020","","","","","","","https://www.businesswire.com/news/home/20200305005148/en/KIOXIA-America-Announces-PCIe-4.0-NVMe-SSDs","","","","","","","Sequential reads up to 6.9 GB/s","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Systems C","","Cisco UCS C220 M5 Rack Server Disk I/O Characterization","","","","","","","","2018","","","","p-scailbib;All","","","","","","","","","2018","","","","","","","https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/unified-computing/whitepaper-c11-741592.html","","","","","","","Sequential 100% read/write up to $\sim$2400 MB/s for 10 HDD JBOD; SSD RAID up to $\sim$7000 MB/s","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Digital W","","Ultrastar DC HC550 Product Page","","","","","","","","2023","","","","p-scailbib;All","","","","","","","","","2023","","","","","","","https://documents.westerndigital.com/content/dam/doc-library/en_us/assets/public/western-digital/collateral/data-sheet/data-sheet-ultrastar-dc-hc550.pdf","","","","","","","Transfer rate up to 269 MB/s","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Llc ST","","Exos X16: Data Sheet","","","","","","","","2019","","","","All","","","","","","","","","2019","","","","","","","https://device.report/m/2a0b0fce4a8b6d2e05118ee5d2f8d408b4ae070e4f42aab03e187fff2cf2b4cb.pdf","","","","","","","Max sustained transfer rate 261 MB/s (outer zone)","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Llc ST","","Exos X20: Data Sheet","","","","","","","","2022","","","","p-scailbib;All","","","","","","","","","2022","","","","","","","https://www.seagate.com/files/www-content/datasheets/pdfs/exos-x20-DS2079-4-2210US-en_US.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Tang J,Fan T,Huang C","","AutoAgent: A Fully-Automated and zero-code framework for LLM agents","","","","","","arXiv [cs.AI]","","2025","","","","All","Agentic","","","","","","","","2025-02-09","","","","","","","http://arxiv.org/abs/2502.05957","","","2502.05957","","Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2502.05957","cs.AI","","","","","","","","","","","","","","All Papers/T/Tang et al. 2025 - AutoAgent - A Fully-Automated and zero-code framework for LLM agents.pdf","","","",""
"Preprint","Levi E,Kadar I","","IntellAgent: A multi-agent framework for evaluating conversational AI systems","","","","","","arXiv [cs.CL]","","2025","","","","All","Agentic","","","","","","","","2025-01-19","","","","","","","http://arxiv.org/abs/2501.11067","","","2501.11067","","Large Language Models (LLMs) are transforming artificial intelligence, evolving into task-oriented systems capable of autonomous planning and execution. One of the primary applications of LLMs is conversational AI systems, which must navigate multi-turn dialogues, integrate domain-specific APIs, and adhere to strict policy constraints. However, evaluating these agents remains a significant challenge, as traditional methods fail to capture the complexity and variability of real-world interactions. We introduce IntellAgent, a scalable, open-source multi-agent framework designed to evaluate conversational AI systems comprehensively. IntellAgent automates the creation of diverse, synthetic benchmarks by combining policy-driven graph modeling, realistic event generation, and interactive user-agent simulations. This innovative approach provides fine-grained diagnostics, addressing the limitations of static and manually curated benchmarks with coarse-grained metrics. IntellAgent represents a paradigm shift in evaluating conversational AI. By simulating realistic, multi-policy scenarios across varying levels of complexity, IntellAgent captures the nuanced interplay of agent capabilities and policy constraints. Unlike traditional methods, it employs a graph-based policy model to represent relationships, likelihoods, and complexities of policy interactions, enabling highly detailed diagnostics. IntellAgent also identifies critical performance gaps, offering actionable insights for targeted optimization. Its modular, open-source design supports seamless integration of new domains, policies, and APIs, fostering reproducibility and community collaboration. Our findings demonstrate that IntellAgent serves as an effective framework for advancing conversational AI by addressing challenges in bridging research and deployment. The framework is available at https://github.com/plurai-ai/intellagent","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2501.11067","cs.CL","","","","","","","","","","","","","","All Papers/L/Levi and Kadar 2025 - IntellAgent - A multi-agent framework for evaluating conversational AI systems.pdf","","","",""
"Preprint","Ackerman L","","Perceptions of Agentic AI in Organizations: Implications for Responsible AI and ROI","","","","","","arXiv [cs.CY]","","2025","","","","All","Agentic","","","","","","","","2025-04-15","","","","","","","http://arxiv.org/abs/2504.11564","","","2504.11564","","As artificial intelligence (AI) systems rapidly gain autonomy, the need for robust responsible AI frameworks becomes paramount. This paper investigates how organizations perceive and adapt such frameworks amidst the emerging landscape of increasingly sophisticated agentic AI. Employing an interpretive qualitative approach, the study explores the lived experiences of AI professionals. Findings highlight that the inherent complexity of agentic AI systems and their responsible implementation, rooted in the intricate interconnectedness of responsible AI dimensions and the thematic framework (an analytical structure developed from the data), combined with the novelty of agentic AI, contribute to significant challenges in organizational adaptation, characterized by knowledge gaps, a limited emphasis on stakeholder engagement, and a strong focus on control. These factors, by hindering effective adaptation and implementation, ultimately compromise the potential for responsible AI and the realization of ROI.","","","http://creativecommons.org/licenses/by-sa/4.0/","","","","","","arXiv","2504.11564","cs.CY","","","","","","","","","","","","","","All Papers/A/Ackerman 2025 - Perceptions of Agentic AI in Organizations - Implications for Responsible AI and ROI.pdf","","","",""
"Preprint","Wei J,Sun Z,Papay S,McKinney S,Han J,Fulford I,Chung HW,Passos AT,Fedus W,Glaese A","","BrowseComp: A simple yet challenging benchmark for browsing agents","","","","","","arXiv [cs.CL]","","2025","","","","All","Agentic","","","","","","","","2025-04-16","","","","","","","http://arxiv.org/abs/2504.12516","","","2504.12516","","We present BrowseComp, a simple yet challenging benchmark for measuring the ability for agents to browse the web. BrowseComp comprises 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. BrowseComp for browsing agents can be seen as analogous to how programming competitions are an incomplete but useful benchmark for coding agents. While BrowseComp sidesteps challenges of a true user query distribution, like generating long answers or resolving ambiguity, it measures the important core capability of exercising persistence and creativity in finding information. BrowseComp can be found at https://github.com/openai/simple-evals.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2504.12516","cs.CL","","","","","","","","","","","","","","All Papers/W/Wei et al. 2025 - BrowseComp - A simple yet challenging benchmark for browsing agents.pdf","","","",""
"Preprint","Klein L,Potamitis N,Aydin R,West R,Gulcehre C,Arora A","","Fleet of agents: Coordinated problem solving with large language models","","","","","","arXiv [cs.CL]","","2024","","","","All","Agentic","","","","","","","","2024-05-07","","","","","","","http://arxiv.org/abs/2405.06691","","","2405.06691","","While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2405.06691","cs.CL","","","","","","","","","","","","","","All Papers/K/Klein et al. 2024 - Fleet of agents - Coordinated problem solving with large language models.pdf","","","",""
"Preprint","Feng KJ,Pu K,Latzke M,August T,Siangliulue P,Bragg J,Weld DS,Zhang AX,Chang JC","","Cocoa: Co-planning and co-execution with AI agents","","","","","","arXiv [cs.HC]","","2024","","","","All","Agentic","","","","","","","","2024-12-14","","","","","","","http://arxiv.org/abs/2412.10999","","","2412.10999","","Human collaboration benefits from continuous coordination -- planning, delegating tasks, sharing progress, and adjusting objectives -- to align on shared goals. However, agentic AI systems often limit users to previewing or reviewing an agent's plans for fully autonomous execution. While this may be useful for confirmation and correction, it does not support deeper collaboration between humans and AI agents. We present Cocoa, a system that introduces a novel design pattern -- interactive plans -- for collaborating with an AI agent on complex, multi-step tasks. Informed by a formative study ($n=9$), Cocoa builds on interaction designs from computational notebooks and document editors to support flexible delegation of agency through Co-planning and Co-execution, where users collaboratively compose and execute plans with an Agent. Using scientific research as a sample domain, our lab (n=16) and field deployment (n=7) studies found that Cocoa improved agent steerability without sacrificing ease-of-use compared to a strong chat baseline. Additionally, researchers valued Cocoa for real-world projects and saw the interleaving of co-planning and co-execution as an effective novel paradigm for human-AI collaboration.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2412.10999","cs.HC","","","","","","","","","","","","","","All Papers/F/Feng et al. 2024 - Cocoa - Co-planning and co-execution with AI agents.pdf","","","",""
"Preprint","Salama R,Cai J,Yuan M,Currey A,Sunkara M,Zhang Y,Benajiba Y","","MemInsight: Autonomous Memory Augmentation for LLM Agents","","","","","","arXiv [cs.CL]","","2025","","","","All","Agentic","","","","","","","","2025-03-27","","","","","","","http://arxiv.org/abs/2503.21760","","","2503.21760","","Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.","","","http://creativecommons.org/licenses/by-nc-nd/4.0/","","","","","","arXiv","2503.21760","cs.CL","","","","","","","","","","","","","","All Papers/S/Salama et al. 2025 - MemInsight - Autonomous Memory Augmentation for LLM Agents.pdf","","","",""
"Conference paper","Leighton L","","No title","","","","","","","","2013","","","1-1","All","Agentic","","","","","","","","2013","","2025-07-15","","","","","https://openreview.net/pdf?id=yYQLvofQ1k;http://dx.doi.org/10.1192/s0140078900025244","10.1192/s0140078900025244","","","","One of the purposes of Demand Side Management (DSM) is to reduce peak loads due to consumers' daily consumption behavior. This can be achieved by introducing dynamic pricing for demand flattening. However, uncoordinated consumer response may lead to a “herding effect”, where the majority of consumers adjust their electricity consumption toward the same cheap time slots (hence, creating new peaks). To overcome this problem, in this paper, we explore strategies of assigning non-uniform participation rates to consumers. We employ a generic method to find a nearly-optimal distribution setting for participation rates, which can be tuned based on DSM designer's objective. Based on simulation experiments, we show that our approach is very effective as compared with existing ones in the literature. Moreover, we evaluate our approach in nonstationary environment, when consumers change their consumption behavior from day to day. In order to maximize consumer convenience, we also propose another method that assigns low participation rates to more consumers (which means less-frequent changes in consumption schedules). Based on our methods, DSM designers are able to tune their systems toward performance or consumer convenience.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Cemri M,Pan MZ,Yang S,Agrawal LA,Chopra B,Tiwari R,Keutzer K,Parameswaran A,Klein D,Ramchandran K,Zaharia M,Gonzalez JE,Stoica I","","Why Do Multi-Agent LLM Systems Fail?","","","","","","arXiv [cs.AI]","","2025","","","","All","Agentic","","","","","","","","2025-03-17","","","","","","","http://arxiv.org/abs/2503.13657","","","2503.13657","","Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness. We present MAST (Multi-Agent System Failure Taxonomy), the first empirically grounded taxonomy designed to understand MAS failures. We analyze seven popular MAS frameworks across over 200 tasks, involving six expert human annotators. Through this process, we identify 14 unique failure modes, organized into 3 overarching categories, (i) specification issues, (ii) inter-agent misalignment, and (iii) task verification. MAST emerges iteratively from rigorous inter-annotator agreement studies, achieving a Cohen's Kappa score of 0.88. To support scalable evaluation, we develop a validated LLM-as-a-Judge pipeline integrated with MAST. We leverage two case studies to demonstrate MAST's practical utility in analyzing failures and guiding MAS development. Our findings reveal that identified failures require more complex solutions, highlighting a clear roadmap for future research. We open source our comprehensive dataset and LLM annotator to facilitate further development of MAS.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2503.13657","cs.AI","","","","","","","","","","","","","","All Papers/C/Cemri et al. 2025 - Why Do Multi-Agent LLM Systems Fail.pdf","","","",""
"Journal article","Peng W,Liu H,Li R,Zhou Z,Xiong W","","WorkTeam: Constructing workflows from natural language with multi-agents","alphaXiv","","","","","","","2025","","","","All","Agentic","","","arXiv","","","","","2025-03-28","","2025-07-15","","","","","https://www.alphaxiv.org/overview/2503.22473v1;https://www.alphaxiv.org/abs/67ea13803a235fbda971628f","","","","","Workflows play a crucial role in enhancing enterprise efficiency by orchestrating complex processes with multiple tools or components. However, hand-crafted workflow construction requires expert knowledge, presenting significant technical barriers. Recent advancements in Large Language Models (LLMs) have improved the generation of workflows from natural language instructions (aka NL2Workflow), yet existing single LLM agent-based methods face performance degradation on complex tasks due to the need for specialized knowledge and the strain of task-switching. To tackle these challenges, we propose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor, orchestrator, and filler agent, each with distinct roles that collaboratively enhance the conversion process. As there are currently no publicly available NL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, which includes 3,695 real-world business samples for training and evaluation. Experimental results show that our approach significantly increases the success rate of workflow construction, providing a novel and effective solution for enterprise NL2Workflow services.","","","","","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Tur AD,Meade N,Lù XH,Zambrano A,Patel A,Durmus E,Gella S,Stańczak K,Reddy S","","SafeArena: Evaluating the safety of autonomous web agents","","","","","","arXiv [cs.LG]","","2025","","","","All","Agentic","","","","","","","","2025-03-06","","","","","","","http://arxiv.org/abs/2503.04957","","","2503.04957","","LLM-based agents are becoming increasingly proficient at solving web-based tasks. With this capability comes a greater risk of misuse for malicious purposes, such as posting misinformation in an online forum or selling illicit substances on a website. To evaluate these risks, we propose SafeArena, the first benchmark to focus on the deliberate misuse of web agents. SafeArena comprises 250 safe and 250 harmful tasks across four websites. We classify the harmful tasks into five harm categories -- misinformation, illegal activity, harassment, cybercrime, and social bias, designed to assess realistic misuses of web agents. We evaluate leading LLM-based web agents, including GPT-4o, Claude-3.5 Sonnet, Qwen-2-VL 72B, and Llama-3.2 90B, on our benchmark. To systematically assess their susceptibility to harmful tasks, we introduce the Agent Risk Assessment framework that categorizes agent behavior across four risk levels. We find agents are surprisingly compliant with malicious requests, with GPT-4o and Qwen-2 completing 34.7% and 27.3% of harmful requests, respectively. Our findings highlight the urgent need for safety alignment procedures for web agents. Our benchmark is available here: https://safearena.github.io","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2503.04957","cs.LG","","","","","","","","","","","","","","All Papers/T/Tur et al. 2025 - SafeArena - Evaluating the safety of autonomous web agents.pdf","","","",""
"Preprint","Zhang C,He S,Li L,Qin S,Kang Y,Lin Q,Rajmohan S,Zhang D","","API Agents vs. GUI Agents: Divergence and Convergence","","","","","","arXiv [cs.AI]","","2025","","","","All","Agentic","","","","","","","","2025-06-23","","","","","","","http://arxiv.org/abs/2503.11069","","","2503.11069","","Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models. This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2503.11069","cs.AI","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2025 - API Agents vs. GUI Agents - Divergence and Convergence.pdf","","","",""
"Preprint","Bousetouane F","","Agentic systems: A guide to transforming industries with vertical AI agents","","","","","","arXiv [cs.MA]","","2025","","","","All","","","","","","","","","2025-01-01","","","","","","","http://arxiv.org/abs/2501.00881","","","2501.00881","","The evolution of agentic systems represents a significant milestone in artificial intelligence and modern software systems, driven by the demand for vertical intelligence tailored to diverse industries. These systems enhance business outcomes through adaptability, learning, and interaction with dynamic environments. At the forefront of this revolution are Large Language Model (LLM) agents, which serve as the cognitive backbone of these intelligent systems. In response to the need for consistency and scalability, this work attempts to define a level of standardization for Vertical AI agent design patterns by identifying core building blocks and proposing a \textbf{Cognitive Skills } Module, which incorporates domain-specific, purpose-built inference capabilities. Building on these foundational concepts, this paper offers a comprehensive introduction to agentic systems, detailing their core components, operational patterns, and implementation strategies. It further explores practical use cases and examples across various industries, highlighting the transformative potential of LLM agents in driving industry-specific applications.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2501.00881","cs.MA","","","","","","","","","","","","","","All Papers/B/Bousetouane 2025 - Agentic systems - A guide to transforming industries with vertical AI agents.pdf","","","",""
"Preprint","Belcak P,Heinrich G,Diao S,Fu Y,Dong X,Muralidharan S,Lin YC,Molchanov P","","Small Language Models are the Future of Agentic AI","","","","","","arXiv [cs.AI]","","2025","","","","All","","","","","","","","","2025-06-02","","","","","","","http://arxiv.org/abs/2506.02153","","","2506.02153","","Large language models (LLMs) are often praised for exhibiting near-human performance on a wide range of tasks and valued for their ability to hold a general conversation. The rise of agentic AI systems is, however, ushering in a mass of applications in which language models perform a small number of specialized tasks repetitively and with little variation. Here we lay out the position that small language models (SLMs) are sufficiently powerful, inherently more suitable, and necessarily more economical for many invocations in agentic systems, and are therefore the future of agentic AI. Our argumentation is grounded in the current level of capabilities exhibited by SLMs, the common architectures of agentic systems, and the economy of LM deployment. We further argue that in situations where general-purpose conversational abilities are essential, heterogeneous agentic systems (i.e., agents invoking multiple different models) are the natural choice. We discuss the potential barriers for the adoption of SLMs in agentic systems and outline a general LLM-to-SLM agent conversion algorithm. Our position, formulated as a value statement, highlights the significance of the operational and economic impact even a partial shift from LLMs to SLMs is to have on the AI agent industry. We aim to stimulate the discussion on the effective use of AI resources and hope to advance the efforts to lower the costs of AI of the present day. Calling for both contributions to and critique of our position, we commit to publishing all such correspondence at https://research.nvidia.com/labs/lpr/slm-agents.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2506.02153","cs.AI","","","","","","","","","","","","","","All Papers/B/Belcak et al. 2025 - Small Language Models are the Future of Agentic AI.pdf","","","",""
"Journal article","Ghemawat S,Gobioff H,Leung ST","","The Google file system","Symp Oper Syst Princ","Symposium on Operating Systems Principles","","","","","","2003","","","29-43","All","","","","","","","","","2003-10-19","","","","","","","http://dx.doi.org/10.1145/945445.945450;https://dl.acm.org/doi/10.1145/945445.945450;https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf","10.1145/945445.945450","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Ghemawat et al. 2003 - The Google file system.pdf","","","",""
"Journal article","Dayalan M","","MapReduce: simplified data processing on large clusters","Commun. ACM","","","","","","","2008","51","","107-113","All","","","","","","","","","2008","","","","","","","http://dx.doi.org/10.1145/1327452.1327492;https://dl.acm.org/doi/10.1145/1327452.1327492;https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf","10.1145/1327452.1327492","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dayalan 2008 - MapReduce - simplified data processing on large clusters.pdf","","","",""
"Miscellaneous","","","2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/2025-01-18-pdf-1-TechAI-Goolge-whitep... - 2025-01-18-pdf-1-TechAI-Goolge-whitepaper_Prompt Engineering_v4-af36dcc7a49bb7269a58b1c9b89a8ae1.pdf","","","",""
"Preprint","Dalal K,Koceja D,Hussein G,Xu J,Zhao Y,Song Y,Han S,Cheung KC,Kautz J,Guestrin C,Hashimoto T,Koyejo S,Choi Y,Sun Y,Wang X","","One-minute video generation with test-Time Training","","","","","","arXiv [cs.CV]","","2025","","","","All","","","","","","","","","2025-04-07","","","","","","","http://arxiv.org/abs/2504.05298","","","2504.05298","","Transformers today still struggle to generate one-minute videos because self-attention layers are inefficient for long context. Alternatives such as Mamba layers struggle with complex multi-scene stories because their hidden states are less expressive. We experiment with Test-Time Training (TTT) layers, whose hidden states themselves can be neural networks, therefore more expressive. Adding TTT layers into a pre-trained Transformer enables it to generate one-minute videos from text storyboards. For proof of concept, we curate a dataset based on Tom and Jerry cartoons. Compared to baselines such as Mamba~2, Gated DeltaNet, and sliding-window attention layers, TTT layers generate much more coherent videos that tell complex stories, leading by 34 Elo points in a human evaluation of 100 videos per method. Although promising, results still contain artifacts, likely due to the limited capability of the pre-trained 5B model. The efficiency of our implementation can also be improved. We have only experimented with one-minute videos due to resource constraints, but the approach can be extended to longer videos and more complex stories. Sample videos, code and annotations are available at: https://test-time-training.github.io/video-dit","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2504.05298","cs.CV","","","","","","","","","","","","","","All Papers/D/Dalal et al. 2025 - One-minute video generation with test-Time Training.pdf","","","",""
"Preprint","Ai E,Shah DJ,Rushton P,Singla S,Parmar M,Smith K,Vanjani Y,Vaswani A,Chaluvaraju A,Hojel A,Ma A,Thomas A,Polloreno A,Tanwer A,Sibai BD,Mansingka DS,Shivaprasad D,Shah I,Stratos K,Nguyen K,Callahan M,Pust M,Iyer M,Monk P,Mazarakis P,Kapila R,Srivastava S,Romanski T","","Rethinking Reflection in Pre-Training","","","","","","arXiv [cs.CL]","","2025","","","","All","","","","","","","","","2025-04-04","","","","","","","http://arxiv.org/abs/2504.04022","","","2504.04022","","A language model's ability to reflect on its own reasoning provides a key advantage for solving complex problems. While most recent research has focused on how this ability develops during reinforcement learning, we show that it actually begins to emerge much earlier - during the model's pre-training. To study this, we introduce deliberate errors into chains-of-thought and test whether the model can still arrive at the correct answer by recognizing and correcting these mistakes. By tracking performance across different stages of pre-training, we observe that this self-correcting ability appears early and improves steadily over time. For instance, an OLMo2-7B model pre-trained on 4 trillion tokens displays self-correction on our six self-reflection tasks.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2504.04022","cs.CL","","","","","","","","","","","","","","All Papers/A/Ai et al. 2025 - Rethinking Reflection in Pre-Training.pdf","","","",""
"Journal article","Ammons J,Fenner T,Weston D","","Improving encrypted deduplication using segment chunks and index locality","ACM Trans. Storage","ACM transactions on storage","","","","","","","X","X","","All","","","","Association for Computing Machinery","","","","","","","","","","1553-3077","1553-3093","http://dx.doi.org/10.1145/XXXXXXX.XXXXXXX","10.1145/XXXXXXX.XXXXXXX","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Ammons et al. - Improving encrypted deduplication using segment chunks and index locality.pdf","","","",""
"Preprint","Hu J,Zhang Y,Han Q,Jiang D,Zhang X,Shum HY","","Open-Reasoner-Zero: An open source approach to scaling up reinforcement learning on the base model","","","","","","arXiv [cs.LG]","","2025","","","","All","","","","","","","","","2025-03-31","","","","","","","http://arxiv.org/abs/2503.24290","","","2503.24290","","We introduce Open-Reasoner-Zero, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility. Through extensive experiments, we demonstrate that a minimalist approach, vanilla PPO with GAE ($\lambda=1$, $\gamma=1$) and straightforward rule-based rewards, without any KL regularization, is sufficient to scale up both response length and benchmark performance, similar to the phenomenon observed in DeepSeek-R1-Zero. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency -- requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline. In the spirit of open source, we release our source code, parameter settings, training data, and model weights across various sizes.","","","http://creativecommons.org/licenses/by/4.0/","","","","","","arXiv","2503.24290","cs.LG","","","","","","","","","","","","","","All Papers/H/Hu et al. 2025 - Open-Reasoner-Zero - An open source approach to scaling up reinforcement learning on the base model.pdf","","","",""
"Book","Carlson JL","","Redis in Action","","","","","","","","2013","","","","p-scailbib;Thesis;All","","","","Manning Publications","New York, NY","","","","2013-05-28","","","9781617290855","","","","","","","","","","","","","","en","","","","","","","438","","","","","","","","","","","","","","","","",""
"Journal article","Gan C,Wang W,Hu Y,Zhao X,Dun S,Xiao Q,Wang W,Huang H","","Coupling secret sharing with decentralized server-aided encryption in encrypted deduplication","Appl. Sci. (Basel)","Applied sciences (Basel, Switzerland)","","","","","","2025","15","3","1245","All","","","","MDPI AG","","","","","2025-01-26","","","","","2076-3417","","http://dx.doi.org/10.3390/app15031245;https://www.mdpi.com/2076-3417/15/3/1245","10.3390/app15031245","","","","Outsourcing storage to the cloud can save storage costs and is commonly used in businesses. It should fulfill two major goals: storage efficiency and data confidentiality. Encrypted deduplication can achieve both goals via performing deduplication to eliminate the duplicate data within encrypted data. Traditional encrypted deduplication generates the encryption key on the client side, which poses a risk of offline brute-force cracking of the outsourced data. Server-aided encryption schemes have been proposed to strengthen the confidentiality of encrypted deduplication by distributing the encryption process to dedicated servers. Existing schemes rely on expensive cryptographic primitives to provide a decentralized setting on the dedicated servers for scalability. However, this incurs substantial performance slowdown and can not be applied in practical encrypted deduplication storage systems. In this paper, we propose a new decentralized server-aided encrypted deduplication approach for outsourced storage, called ECDedup, which leverages secret sharing to achieve secure and efficient key management. We are the first to use the coding matrix as the encryption key to couple the encryption and encoding processes in encrypted deduplication. We also propose a acceleration scheme to speed up the encryption process of our ECDedup. We prototype ECDedup in cloud environments, and our experimental results based on the real-world backup datasets show that ECDedup can improve the client throughput by up to 51.9% compared to the state-of-the-art encrypted deduplication schemes.","","","https://creativecommons.org/licenses/by/4.0/","","en","","","","","","","","","","","","","","","","","","","","All Papers/G/Gan et al. 2025 - Coupling secret sharing with decentralized server-aided encryption in encrypted deduplication.pdf","","","",""
"Book chapter","Lucani DE,Nielsen L,Orlandi C,Pagnin E,Vestergaard R","","Secure generalized deduplication via multi-key revealing encryption","","","Lecture Notes in Computer Science","","","","","2020","","","298-318","All","","","","Springer International Publishing","Cham","","","","2020","","","9783030579890","9783030579906","0302-9743","1611-3349","https://link.springer.com/10.1007/978-3-030-57990-6_15;http://dx.doi.org/10.1007/978-3-030-57990-6_15","10.1007/978-3-030-57990-6_15","","","","","","","","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/L/Lucani et al. 2020 - Secure generalized deduplication via multi-key revealing encryption.pdf","","","",""
"Miscellaneous","Li PF,Hua Y,Distinguished Member,Senior Member","","An Enhanced Physical-Locality Deduplication System for Space Efficiency","","","","","","","","","","","","All","","","","","","","","","","","","","","","","http://dx.doi.org/10.1007/s11390-023-2646-7","10.1007/s11390-023-2646-7","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. - An Enhanced Physical-Locality Deduplication System for Space Efficiency.pdf","","","",""
"Preprint","Geiping J,McLeish S,Jain N,Kirchenbauer J,Singh S,Bartoldson BR,Kailkhura B,Bhatele A,Goldstein T","","Scaling up test-time compute with latent reasoning: A recurrent depth approach","","","","","","arXiv [cs.LG]","","2025","","","","All","","","","","","","","","2025-02-07","","","","","","","http://arxiv.org/abs/2502.05171","","","2502.05171","","We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2502.05171","cs.LG","","","","","","","","","","","","","","All Papers/G/Geiping et al. 2025 - Scaling up test-time compute with latent reasoning - A recurrent depth approach.pdf","","","",""
"Book chapter","Jiang FN","","RapidCDC: Leveraging Duplicate Locality Accelerate Chunking CDC-Based Deduplication Systems","","","Proc. ACM SoCC’19. Association Computing Machinery","","","","","2019","","","220-232","All","","","","","New York, NY, USA","","","","2019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jiang 2019 - socc19-slides-s5-ni.pdf;All Papers/J/Jiang 2019 - RapidCDC - Leveraging Duplicate Locality Accelerate Chunking CDC-Based Deduplication Systems.pdf","","","",""
"Journal article","Gan C,Wang W,Hu Y,Zhao X,Dun S,Xiao Q,Wang W,Huang H","","Coupling secret sharing with decentralized server-aided encryption in encrypted deduplication","Appl. Sci. (Basel)","Applied sciences (Basel, Switzerland)","","","","","","2025","","","","All","","","","mdpi.com","","","","","2025-01-26","","","","","2076-3417","","https://www.mdpi.com/2076-3417/15/3/1245;http://dx.doi.org/10.3390/app15031245","10.3390/app15031245","","","","Outsourcing storage to the cloud can save storage costs and is commonly used in businesses. It should fulfill two major goals: storage efficiency and data confidentiality. Encrypted deduplication can achieve both goals via performing deduplication to eliminate the duplicate data within encrypted data. Traditional encrypted deduplication generates the encryption key on the client side, which poses a risk of offline brute-force cracking of the outsourced data. Server-aided encryption schemes have been proposed to strengthen the confidentiality of encrypted deduplication by distributing the encryption process to dedicated servers. Existing schemes rely on expensive cryptographic primitives to provide a decentralized setting on the dedicated servers for scalability. However, this incurs substantial performance slowdown and can not be applied in practical encrypted deduplication storage systems. In this paper, we propose a new decentralized server-aided encrypted deduplication approach for outsourced storage, called ECDedup, which leverages secret sharing to achieve secure and efficient key management. We are the first to use the coding matrix as the encryption key to couple the encryption and encoding processes in encrypted deduplication. We also propose a acceleration scheme to speed up the encryption process of our ECDedup. We prototype ECDedup in cloud environments, and our experimental results based on the real-world backup datasets show that ECDedup can improve the client throughput by up to 51.9% compared to the state-of-the-art encrypted deduplication schemes.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Gan et al. 2025 - Coupling secret sharing with decentralized server-aided encryption in encrypted deduplication.pdf","","","",""
"Preprint","DeepSeek-AI,Liu A,Feng B,Xue B,Wang B,Wu B,Lu C,Zhao C,Deng C,Zhang C,Ruan C,Dai D,Guo D,Yang D,Chen D,Ji D,Li E,Lin F,Dai F,Luo F,Hao G,Chen G,Li G,Zhang H,Bao H,Xu H,Wang H,Zhang H,Ding H,Xin H,Gao H,Li H,Qu H,Cai JL,Liang J,Guo J,Ni J,Li J,Wang J,Chen J,Chen J,Yuan J,Qiu J,Li J,Song J,Dong K,Hu K,Gao K,Guan K,Huang K,Yu K,Wang L,Zhang L,Xu L,Xia L,Zhao L,Wang L,Zhang L,Li M,Wang M,Zhang M,Zhang M,Tang M,Li M,Tian N,Huang P,Wang P,Zhang P,Wang Q,Zhu Q,Chen Q,Du Q,Chen RJ,Jin RL,Ge R,Zhang R,Pan R,Wang R,Xu R,Zhang R,Chen R,Li SS,Lu S,Zhou S,Chen S,Wu S,Ye S,Ye S,Ma S,Wang S,Zhou S,Yu S,Zhou S,Pan S,Wang T,Yun T,Pei T,Sun T,Xiao WL,Zeng W,Zhao W,An W,Liu W,Liang W,Gao W,Yu W,Zhang W,Li XQ,Jin X,Wang X,Bi X,Liu X,Wang X,Shen X,Chen X,Zhang X,Chen X,Nie X,Sun X,Wang X,Cheng X,Liu X,Xie X,Liu X,Yu X,Song X,Shan X,Zhou X,Yang X,Li X,Su X,Lin X,Li YK,Wang YQ,Wei YX,Zhu YX,Zhang Y,Xu Y,Xu Y,Huang Y,Li Y,Zhao Y,Sun Y,Li Y,Wang Y,Yu Y,Zheng Y,Zhang Y,Shi Y,Xiong Y,He Y,Tang Y,Piao Y,Wang Y,Tan Y,Ma Y,Liu Y,Guo Y,Wu Y,Ou Y,Zhu Y,Wang Y,Gong Y,Zou Y,He Y,Zha Y,Xiong Y,Ma Y,Yan Y,Luo Y,You Y,Liu Y,Zhou Y,Wu ZF,Ren ZZ,Ren Z,Sha Z,Fu Z,Xu Z,Huang Z,Zhang Z,Xie Z,Zhang Z,Hao Z,Gou Z,Ma Z,Yan Z,Shao Z,Xu Z,Wu Z,Zhang Z,Li Z,Gu Z,Zhu Z,Liu Z,Li Z,Xie Z,Song Z,Gao Z,Pan Z","","DeepSeek-V3 Technical Report","","","","","","arXiv [cs.CL]","","2024","","","","All","","","","","","","","","2024-12-26","","","","","","","http://arxiv.org/abs/2412.19437","","","2412.19437","","We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2412.19437","cs.CL","","","","","","","","","","","","","","All Papers/D/DeepSeek-AI et al. 2024 - DeepSeek-V3 Technical Report.pdf","","","",""
"Preprint","DeepSeek-AI,Guo D,Yang D,Zhang H,Song J,Zhang R,Xu R,Zhu Q,Ma S,Wang P,Bi X,Zhang X,Yu X,Wu Y,Wu ZF,Gou Z,Shao Z,Li Z,Gao Z,Liu A,Xue B,Wang B,Wu B,Feng B,Lu C,Zhao C,Deng C,Zhang C,Ruan C,Dai D,Chen D,Ji D,Li E,Lin F,Dai F,Luo F,Hao G,Chen G,Li G,Zhang H,Bao H,Xu H,Wang H,Ding H,Xin H,Gao H,Qu H,Li H,Guo J,Li J,Wang J,Chen J,Yuan J,Qiu J,Li J,Cai JL,Ni J,Liang J,Chen J,Dong K,Hu K,Gao K,Guan K,Huang K,Yu K,Wang L,Zhang L,Zhao L,Wang L,Zhang L,Xu L,Xia L,Zhang M,Zhang M,Tang M,Li M,Wang M,Li M,Tian N,Huang P,Zhang P,Wang Q,Chen Q,Du Q,Ge R,Zhang R,Pan R,Wang R,Chen RJ,Jin RL,Chen R,Lu S,Zhou S,Chen S,Ye S,Wang S,Yu S,Zhou S,Pan S,Li SS,Zhou S,Wu S,Ye S,Yun T,Pei T,Sun T,Wang T,Zeng W,Zhao W,Liu W,Liang W,Gao W,Yu W,Zhang W,Xiao WL,An W,Liu X,Wang X,Chen X,Nie X,Cheng X,Liu X,Xie X,Liu X,Yang X,Li X,Su X,Lin X,Li XQ,Jin X,Shen X,Chen X,Sun X,Wang X,Song X,Zhou X,Wang X,Shan X,Li YK,Wang YQ,Wei YX,Zhang Y,Xu Y,Li Y,Zhao Y,Sun Y,Wang Y,Yu Y,Zhang Y,Shi Y,Xiong Y,He Y,Piao Y,Wang Y,Tan Y,Ma Y,Liu Y,Guo Y,Ou Y,Wang Y,Gong Y,Zou Y,He Y,Xiong Y,Luo Y,You Y,Liu Y,Zhou Y,Zhu YX,Xu Y,Huang Y,Li Y,Zheng Y,Zhu Y,Ma Y,Tang Y,Zha Y,Yan Y,Ren ZZ,Ren Z,Sha Z,Fu Z,Xu Z,Xie Z,Zhang Z,Hao Z,Ma Z,Yan Z,Wu Z,Gu Z,Zhu Z,Liu Z,Li Z,Xie Z,Song Z,Pan Z,Huang Z,Xu Z,Zhang Z,Zhang Z","","DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning","","","","","","arXiv [cs.CL]","","2025","","","","All","","","","","","","","","2025-01-22","","","","","","","http://arxiv.org/abs/2501.12948","","","2501.12948","","We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.","","","http://arxiv.org/licenses/nonexclusive-distrib/1.0/","","","","","","arXiv","2501.12948","cs.CL","","","","","","","","","","","","","","All Papers/D/DeepSeek-AI et al. 2025 - DeepSeek-R1 - Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf","","","",""
"Thesis","Ammons JM","","Scaling data capacity and throughput in encrypted deduplication with segment chunks and index locality","","","","","","","","2024","","","","p-scailbib;All","","","","","","","","","2024","","","","","","","https://eprints.bbk.ac.uk/id/eprint/54728/","","","","","","","","","","","Ph.D. Thesis","","","","","","","Birkbeck, University of London","","","","","","","","","","","","","London, United Kingdom","PhD","",""
"Journal article","Mao B,Jiang H,Wu S,Fu Y,Tian L","","Read-performance optimization for deduplication-based storage systems in the cloud","ACM Trans. Storage","ACM transactions on storage","","","","","","2014","10","2","1-22","Grouped by Publication/ACM TOS;All","SSD","","","Association for Computing Machinery (ACM)","","","","","2014-03-01","","2025-01-04","","","1553-3077","1553-3093","https://dl.acm.org/doi/10.1145/2512348;http://dx.doi.org/10.1145/2512348;https://dl.acm.org/doi/abs/10.1145/2512348","10.1145/2512348","","","","Data deduplication has been demonstrated to be an effective technique in reducing the total data transferred over the network and the storage space in cloud backup, archiving, and primary storage systems, such as VM (virtual machine) platforms. However, the performance of restore operations from a deduplicated backup can be significantly lower than that without deduplication. The main reason lies in the fact that a file or block is split into multiple small data chunks that are often located in different disks after deduplication, which can cause a subsequent read operation to invoke many disk IOs involving multiple disks and thus degrade the read performance significantly. While this problem has been by and large ignored in the literature thus far, we argue that the time is ripe for us to pay significant attention to it in light of the emerging cloud storage applications and the increasing popularity of the VM platform in the cloud. This is because, in a cloud storage or VM environment, a simple read request on the client side may translate into a restore operation if the data to be read or a VM suspended by the user was previously deduplicated when written to the cloud or the VM storage server, a likely scenario considering the network bandwidth and storage capacity concerns in such an environment. To address this problem, in this article, we propose SAR, an SSD (solid-state drive)-Assisted Read scheme, that effectively exploits the high random-read performance properties of SSDs and the unique data-sharing characteristic of deduplication-based storage systems by storing in SSDs the unique data chunks with high reference count, small size, and nonsequential characteristics. In this way, many read requests to HDDs are replaced by read requests to SSDs, thus significantly improving the read performance of the deduplication-based storage systems in the cloud. The extensive trace-driven and VM restore evaluations on the prototype implementation of SAR show that SAR outperforms the traditional deduplication-based and flash-based cache schemes significantly, in terms of the average response times.","","","http://www.acm.org/publications/policies/copyright_policy#Background","","en","","","","","","","","","","","","","","","","","","","","All Papers/M/Mao et al. 2014 - Read-performance optimization for deduplication-based storage systems in the cloud.pdf","","","",""
"Journal article","Zhang T,Chen R,Li Z,Gao C,Wang C,Shu J","","Design and implementation of deduplication on F2FS","ACM Trans. Storage","ACM transactions on storage","","","","","","2024","20","4","1-50","Grouped by Publication/ACM TOS;All","SSD","","","Association for Computing Machinery (ACM)","","","","","2024-11-30","","","","","1553-3077","1553-3093","https://dl.acm.org/doi/pdf/10.1145/3662735;https://dl.acm.org/doi/10.1145/3662735;http://dx.doi.org/10.1145/3662735","10.1145/3662735","","","","Data deduplication technology has gained popularity in modern file systems due to its ability to eliminate redundant writes and improve storage space efficiency. In recent years, the flash-friendly file system (F2FS) has been widely adopted in flash memory-based storage devices, including smartphones, fast-speed servers, and Internet of Things. In this article, we propose F2DFS (deduplication-based F2FS), which introduces three main design contributions. First, F2DFS integrates inline and offline hybrid deduplication. Inline deduplication eliminates redundant writes and enhances flash device endurance, while offline deduplication mitigates the negative I/O performance impact and saves more storage space. Second, F2DFS follows the file system coupling design principle, effectively leveraging the potentials and benefits of both deduplication and native F2FS. Also, with the aid of this principle, F2DFS achieves high-performance and space-efficient incremental deduplication. Third, F2DFS adopts virtual indexing to mitigate deduplication-induced many-to-one mapping updates during the segment cleaning. We conducted comprehensive experimental comparisons between F2DFS, native F2FS, and other state-of-the-art deduplication schemes, using both synthetic and real-world workloads. For inline deduplication, F2DFS outperforms SmartDedup, Dmdedup, and ZFS, in terms of both I/O bandwidth performance and deduplication rates. And for offline deduplication, compared to SmartDedup, XFS, and BtrFS, F2DFS shows higher execution efficiency, lower resource usage, and greater storage space savings. Moreover, F2DFS demonstrates more efficient segment cleanings than native F2FS.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2024 - Design and implementation of deduplication on F2FS.pdf","","","",""
"Miscellaneous","","","P4TechReport.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/P4TechReport.pdf - P4TechReport.pdf","","","",""
"Journal article","Guo X,Xian H","","AF-dedup: Secure encrypted data deduplication based on adaptive dynamic Merkle hash forest PoW for cloud storage","IEEE Trans. Industr. Inform.","IEEE transactions on industrial informatics","","","","","","2024","20","10","12294-12304","All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2024-10","","2024-11-29","","","1551-3203","1941-0050","https://ieeexplore.ieee.org/abstract/document/10588962;http://dx.doi.org/10.1109/TII.2024.3417327;https://ieeexplore.ieee.org/document/10588962/;http://dx.doi.org/10.1109/tii.2024.3417327","10.1109/tii.2024.3417327","","","","For encrypted data deduplication, proof of ownership (PoW) verifies a client's ownership of an entire file, preventing malicious users from exploiting a single segment of information to gain access to the file. By establishing the identity of two users who possess the same file, cloud service provider (CSP) can maintain a single copy for the file, enabling deduplication. However, existing PoW schemes based on Merkle hash tree (MHT) cannot guarantee the security of small files. Therefore, we propose a novel data structure named a daptive d ynamic M erkle h ash f orest (ADMHF) for PoW, and present an encrypted data deduplication scheme called AF-Dedup. It reduces the risks of data content exposure resulting from multiple ownership verification attempts in traditional schemes. Specifically, we first construct the file tag as a unique identifier of the file. Second, different encryption schemes are employed depending on the popularity of the data. Then, the corresponding ADMHF is generated for subsequent ownership verifications. After security analysis and simulation experiments, our scheme is proven to significantly enhance the security of small files. In a given situation for files with only two blocks, our scheme achieves the same level of security as the existing scheme for a file with 91 blocks.","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","en","","","","","","","","","","","","","","","","","","","","All Papers/G/Guo and Xian 2024 - AF-dedup - Secure encrypted data deduplication based on adaptive dynamic Merkle hash forest PoW for cloud storage.pdf","","","",""
"Book","","","Secure Encrypted Data Deduplication Based Adaptive Dynamic Proof Ownership","","","","","","","","2023","","","","All","","","","","","","","","2023","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Fu M,Han S,Lee PP,Feng D,Chen Z,Xiao Y","","A simulation analysis of redundancy and reliability in primary storage deduplication","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2018","67","9","1259-1272","MS Traces;FSL Traces;Thesis;All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2018-09-01","","","","","0018-9340","1557-9956","https://ieeexplore.ieee.org/abstract/document/8300656/;https://www.cse.cuhk.edu.hk/~pclee/www/pubs/tc18simdedup.pdf;http://dx.doi.org/10.1109/TC.2018.2808496;https://ieeexplore.ieee.org/document/8300656/;http://dx.doi.org/10.1109/tc.2018.2808496","10.1109/tc.2018.2808496","","","","","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2018 - A simulation analysis of redundancy and reliability in primary storage deduplication.pdf","","","",""
"Book chapter","Nayak S,Patgiri R","","Dr. Hadoop: In search of a needle in a haystack","","","Distributed Computing and Internet Technology","","","","","2019","","","99-107","MS Traces;Thesis;All","","","","Springer International Publishing","Cham","","","","2019","","","9783030053659","9783030053666","0302-9743","1611-3349","https://link.springer.com/chapter/10.1007/978-3-030-05366-6_8;https://www.uobabylon.edu.iq/eprints/paper_10_15902_1402.pdf;http://dx.doi.org/10.1007/978-3-030-05366-6_8;http://link.springer.com/10.1007/978-3-030-05366-6_8","10.1007/978-3-030-05366-6_8","","","","","","","","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/N/Nayak and Patgiri 2019 - Dr. Hadoop - In search of a needle in a haystack.pdf","","","",""
"Journal article","Kolosov O,Yadgar G,Maheshwari S,Soljanin E","","Benchmarking in the dark: On the absence of comprehensive edge datasets","HotEdge","USENIX Workshop on Hot Topics in Edge Computing","","","","","","2020","","","","FSL Traces;MS Traces;Thesis;All","","","","usenix.org","","","","","2020","","","","","","","https://www.usenix.org/conference/hotedge20/presentation/kolosov;https://www.usenix.org/system/files/hotedge20_paper_kolosov.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kolosov et al. 2020 - Benchmarking in the dark - On the absence of comprehensive edge datasets.pdf","","","",""
"Conference paper","Li P,Hua Y,Cao Q,Zhang M","","Improving the restore performance via physical-locality middleware for backup systems","","","","","Proceedings of the 21st International Middleware Conference","","","2020","","","","FSL Traces;Thesis;All","","","","ACM","New York, NY, USA","Middleware '20: 21st International Middleware Conference","Delft Netherlands","","2020-12-07","","","9781450381536","","","","https://dl.acm.org/doi/abs/10.1145/3423211.3425691;https://csyhua.github.io/csyhua/hua-middleware2020.pdf;http://dx.doi.org/10.1145/3423211.3425691;https://dl.acm.org/doi/10.1145/3423211.3425691","10.1145/3423211.3425691","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2020 - Improving the restore performance via physical-locality middleware for backup systems.pdf","","","",""
"Book chapter","Tang X,Li J","","Improving online restore performance of backup storage via historical file access pattern","","","Communications in Computer and Information Science","","","","","2022","","","365-376","FSL Traces;Thesis;All","","","","Springer Nature Singapore","Singapore","","","","2022","","","9789811984440","9789811984457","1865-0929","1865-0937","https://link.springer.com/chapter/10.1007/978-981-19-8445-7_23;http://dx.doi.org/10.1007/978-981-19-8445-7_23;https://link.springer.com/10.1007/978-981-19-8445-7_23","10.1007/978-981-19-8445-7_23","","","","","","","https://www.springernature.com/gp/researchers/text-and-data-mining","","en","","Communications in computer and information science","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Wong T,Thakkar S,Hsieh KF,Tom Z,Saraiya H,Shilane P","","Dataset similarity detection for global deduplication in the DD file system","","","","","2023 IEEE 39th International Conference on Data Engineering (ICDE)","","","2023","","","3322-3335","FSL Traces;Thesis;All","","","","IEEE","","2023 IEEE 39th International Conference on Data Engineering (ICDE)","Anaheim, CA, USA","2023/4/3-2023/4/7","2023-04","","","","","2375-026X","1063-6382","https://ieeexplore.ieee.org/abstract/document/10184604/;https://www.researchgate.net/profile/Smriti-Thakkar/publication/370466941_Dataset_Similarity_Detection_for_Global_Deduplication_in_the_DD_File_System/links/645176cd5762c95ac368ddea/Dataset-Similarity-Detection-for-Global-Deduplication-in-the-DD-File-System.pdf;http://dx.doi.org/10.1109/ICDE55515.2023.00255;https://ieeexplore.ieee.org/document/10184604/;http://dx.doi.org/10.1109/icde55515.2023.00255","10.1109/icde55515.2023.00255","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wong et al. 2023 - Dataset similarity detection for global deduplication in the DD file system.pdf","","","",""
"Journal article","Richardson E","","Enhancing Data Recovery in Deduplication Backup Systems: A Novel Rewriting Algorithm to Minimize Fragmentation Effects","Journal of Computer Science and Software Applications","","","","","","","2024","4","2","15–19","FSL Traces;Thesis;All","","","","","","","","","2024-03","","","","","","","https://www.mfacademia.org/index.php/jcssa/article/view/96;http://dx.doi.org/10.5281/zenodo.12200408","10.5281/zenodo.12200408","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Richardson 2024 - Enhancing Data Recovery in Deduplication Backup Systems - A Novel Rewriting Algorithm to Minimize Fragmentation Effects.pdf","","","",""
"Journal article","Sun Zjason,Kuenning G,Mandal S,Shilane P,Tarasov V,Xiao N,Zadok E","","Cluster and single-node analysis of long-term deduplication patterns","ACM Trans. Storage","ACM transactions on storage","","","","","","2018","14","2","1-27","FSL Traces;Thesis;Grouped by Publication/ACM TOS;All","","","","Association for Computing Machinery (ACM)","","","","","2018-05-31","","","","","1553-3077","1553-3093","https://d1wqtxts1xzle7.cloudfront.net/80425695/3183890-libre.pdf?1644247433=&response-content-disposition=inline%3B+filename%3DCluster_and_Single_Node_Analysis_of_Long.pdf&Expires=1731347663&Signature=bSplFugDq8b7pLXur-9kjvVof2w-ZJSEGF2QuQjVWxR~1bpVaTb2AS7Y4g8He~AsxJaaAFvHPw70rb3W9ckG5hiYnieLctJ3MP0~erRaw31U624w5b7GMITiOYVEdJ1636iO358d3wR7SGlv3U-ozrUspvG752oxlqE1V3XaOi4Ytp~DBg5w4mi5O6Pjk0fSaj8iXO2Wd6L4cPRNmhpOrRzBrMVWJfVUDf0Qr9q3dBtW-kMQFQsgo6Nw8gz5XnZd~wkI5eZTfjRW0inoJxhi5D-fTGU6DtLfbV5H-2xVkpQZnuIAGqJyV1xYecsnQacb1d~Pji0r~j2PqUvcCx1MSw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA;http://dx.doi.org/10.1145/3183890;https://dl.acm.org/doi/10.1145/3183890","10.1145/3183890","","","","Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this article, we first collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We then analyzed the dataset, examining a variety of essential characteristics across two dimensions: single-node deduplication and cluster deduplication. For single-node deduplication analysis, our primary focus was individual-user data. Despite apparently similar roles and behavior among all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. For cluster deduplication analysis, we implemented seven published data-routing algorithms and created a detailed comparison of their performance with respect to deduplication ratio, load distribution, and communication overhead. We found that per-file routing achieves a higher deduplication ratio than routing by super-chunk (multiple consecutive chunks), but it also leads to high data skew (imbalance of space usage across nodes). We also found that large chunking sizes are better for cluster deduplication, as they significantly reduce data-routing overhead, while their negative impact on deduplication ratios is small and acceptable. We draw interesting conclusions from both single-node and cluster deduplication analysis and make recommendations for future deduplication systems design.","","","http://www.acm.org/publications/policies/copyright_policy#Background","","en","","","","","","","","","","","","","","","","","","","","All Papers/S/Sun et al. 2018 - Cluster and single-node analysis of long-term deduplication patterns.pdf","","","",""
"Journal article","Yao W,Hao M,Hou Y,Li X","","FASR: An efficient feature-aware deduplication method in distributed storage systems","IEEE Access","IEEE Access: Practical Innovations, Open Solutions","","","","","","2022","10","","15311-15321","FSL Traces;Thesis;All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2022","","","","","2169-3536","","https://ieeexplore.ieee.org/abstract/document/9696338/;https://ieeexplore.ieee.org/iel7/6287639/6514899/09696338.pdf;https://ieeexplore.ieee.org/document/9696338/;http://dx.doi.org/10.1109/access.2022.3147545","10.1109/access.2022.3147545","","","","Deduplication technology can obtain higher space utilization by keeping only one duplicate. But in a distributed storage system, the overall deduplication ratio will be limited due to redundancy elimination across nodes. The traditional deduplication methods usually utilize data similarity and data locality to improve the deduplication ratio. However, higher system overhead is caused by frequent similarity calculations. To deal with this problem, this paper proposes a new Feature-Aware Stateful Routing method (FASR), aiming to reduce the system overhead and keep a high deduplication ratio in the distributed environment. Firstly, we design a feature-aware nodes selection strategy to choose similar nodes by extracting data feature and data distribution characteristics. This strategy will save the similarity calculation with the nodes that are not similar to the data. Then, we present a stateful routing algorithm to determine the target node using super-chunk and handprint technology. Meanwhile, the algorithm maintain load balance of the entire distributed system. Finally, the data is deduplicated locally based on similarity index and fingerprint cache. Extensive experiments demonstrate that FASR can reduce system overhead by around 30% at most and also effectively obtain a higher deduplication ratio.","","","https://creativecommons.org/licenses/by-nc-nd/4.0/","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yao et al. 2022 - FASR - An efficient feature-aware deduplication method in distributed storage systems.pdf","","","",""
"Journal article","Dagnaw G,Zhou K,Wang H","","SACRO : Solid state drive‐assisted chunk caching for restore optimization","Concurr. Comput.","Concurrency and computation: practice & experience","","","","","","2023","35","18","","FSL Traces;Thesis;All","","","","Wiley","","","","","2023-08-15","","","","","1532-0626","1532-0634","https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6162;http://dx.doi.org/10.1002/cpe.6162;https://onlinelibrary.wiley.com/doi/10.1002/cpe.6162","10.1002/cpe.6162","","","","","","","http://onlinelibrary.wiley.com/termsAndConditions#vor","","en","","","","","","","","","","","","","","","","","","","","All Papers/D/Dagnaw et al. 2023 - SACRO - Solid state drive‐assisted chunk caching for restore optimization.pdf","","","",""
"Conference paper","Vaughn C,Miller C,Ekenta O,Sun H,Bhadkamkar M,Efstathopoulos P,Kardes E","","Soothsayer: Predicting capacity usage in backup storage systems","","","","","2015 IEEE 23rd International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems","","","2015","","","208-217","FSL Traces;Thesis;All","","","","IEEE","","2015 IEEE 23rd International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)","Atlanta, GA, USA","2015/10/5-2015/10/7","2015-10","","","9781467377201","9781467377195","1526-7539","","https://ieeexplore.ieee.org/abstract/document/7330192/;https://www.academia.edu/download/46893412/Soothsayer_Predicting_Capacity_Usage_in_20160629-15407-uti0bm.pdf;http://dx.doi.org/10.1109/MASCOTS.2015.40;https://ieeexplore.ieee.org/document/7330192;http://dx.doi.org/10.1109/mascots.2015.40","10.1109/mascots.2015.40","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Lin X,Hibler M,Eide E,Ricci R","","Using deduplicating storage for efficient disk image deployment","","","","","Proceedings of the 10th EAI International Conference on Testbeds and Research Infrastructures for the Development of Networks & Communities","","","2015","","","","FSL Traces;Thesis;All","","","","ACM","","10th EAI International Conference on Testbeds and Research Infrastructures for the Development of Networks & Communities","Vancouver, Canada","2015/6/24-2015/6/25","2015","","","9781631900709","","","","https://eudl.eu/doi/10.4108/icst.tridentcom.2015.259963;https://eudl.eu/pdf/10.4108/icst.tridentcom.2015.259963;http://dx.doi.org/10.4108/icst.tridentcom.2015.259963","10.4108/icst.tridentcom.2015.259963","","","","Many clouds and network testbeds use disk images to initialize local storage on their compute devices. Large facilities must manage thousands or more images, requiring significant amounts of storage. At the same time, to provide a good user experience, they must be able to deploy those images quickly. Driven by our experience in operating the Emulab site at the University of Utah—a long-lived and heavily-used testbed—we have created a new service for efficiently storing and deploying disk images. This service exploits the redundant data found in similar images, using deduplication to greatly reduce the amount of physical storage required. In addition to space savings, our system is also designed for highly efficient image deployment—it integrates with an existing highly-optimized disk image deployment system, Frisbee, without significantly increasing the time required to distribute and install images. In this paper, we explain the design of our system and discuss the trade-offs we made to strike a balance between efficient storage and fast disk image deployment. We also propose a new chunking algorithm, called AFC, which enables fixed-size chunking for deduplicating allocated disk sectors. Experimental results show that our system reduces storage requirements by up to 3 while imposing only a negligible runtime overhead on the end-to-end disk-deployment process.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2015 - Using deduplicating storage for efficient disk image deployment.pdf","","","",""
"Journal article","Zuo C,Wang F,Tang X,Zhang Y,Feng D","","SRSC: Improving restore performance for deduplication-based storage systems","ZTE Communications","","","","","","","2019","17","2","59","FSL Traces;Thesis;All","","","","zte.magtechjournal.com","","","","","2019","","","","","","","https://zte.magtechjournal.com/EN/abstract/abstract18.shtml;https://zte.magtechjournal.com/EN/article/downloadArticleFile.do?attachType=PDF&id=18","","","","","Modern backup systems exploit data deduplication technology to save storage space whereas suffering from the fragmentation problem caused by deduplication. Fragmentation degrades the restore performance because of restoring the chunks that are scattered all over different containers. To improve the restore performance, the state-of-the-art History Aware Rewriting Algorithm (HAR) is proposed to collect fragmented chunks in the last backup and rewrite them in the next backup. However, due to rewriting fragmented chunks in the next backup, HAR fails to eliminate internal fragmentation caused by self-referenced chunks (that exist more than two times in a backup) in the current backup, thus degrading the restore performance. In this paper, we propose Selectively Rewriting Self-Referenced Chunks (SRSC), a scheme that designs a buffer to simulate a restore cache, identify internal fragmentation in the cache and selectively rewrite them. Our experimental results based on two real-world datasets show that SRSC improves the restore performance by 45% with an acceptable sacrifice of the deduplication ratio.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zuo et al. 2019 - SRSC - Improving restore performance for deduplication-based storage systems.pdf","","","",""
"Conference paper","Fu M,C. Lee PP,Feng D,Chen Z,Xiao Y","","A simulation analysis of reliability in primary storage deduplication","","","","","2016 IEEE International Symposium on Workload Characterization (IISWC)","","","2016","","","1-10","FSL Traces;Thesis;All","","","","IEEE","","2016 IEEE International Symposium on Workload Characterization (IISWC)","Providence, RI, USA","2016/9/25-2016/9/27","2016-09","","","9781509038961","9781509038954","","","https://ieeexplore.ieee.org/abstract/document/7581280/;https://www.cse.cuhk.edu.hk/~pclee/www/pubs/iiswc16.pdf;http://dx.doi.org/10.1109/IISWC.2016.7581280;http://ieeexplore.ieee.org/document/7581280/;http://dx.doi.org/10.1109/iiswc.2016.7581280","10.1109/iiswc.2016.7581280","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2016 - A simulation analysis of reliability in primary storage deduplication.pdf","","","",""
"Book chapter","Zhou Y,Feng D,Xia W,Fu M,Xiao Y","","DARM: A deduplication-aware redundancy management approach for reliable-enhanced storage systems","","","Algorithms and Architectures for Parallel Processing","","","","","2018","","","445-461","FSL Traces;Thesis;All","","","","Springer International Publishing","Cham","","","","2018","","","9783030050535","9783030050542","0302-9743","1611-3349","https://link.springer.com/chapter/10.1007/978-3-030-05054-2_35;http://dx.doi.org/10.1007/978-3-030-05054-2_35;http://link.springer.com/10.1007/978-3-030-05054-2_35","10.1007/978-3-030-05054-2_35","","","","","","","","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2018 - DARM - A deduplication-aware redundancy management approach for reliable-enhanced storage systems.pdf","","","",""
"Book chapter","Wu S,Jia C,Wang D","","UP-MLE: Efficient and practical updatable block-level message-locked encryption scheme based on update properties","","","ICT Systems Security and Privacy Protection","","","","","2022","","","251-269","FSL Traces;Thesis;All","","","","Springer International Publishing","Cham","","","","2022","","","9783031069741","9783031069758","1868-4238","1868-422X","https://link.springer.com/chapter/10.1007/978-3-031-06975-8_15;http://dx.doi.org/10.1007/978-3-031-06975-8_15;https://link.springer.com/10.1007/978-3-031-06975-8_15","10.1007/978-3-031-06975-8_15","","","","","","","","","","","IFIP advances in information and communication technology","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Gan C,Hu Y,Zhao L,Zhao X,Gong P,Zhang W,Wang L,Feng D","","Enabling encrypted delta compression for outsourced storage systems via preserving similarity","","","","","2023 IEEE 41st International Conference on Computer Design (ICCD)","","","2023","","","231-238","All","","","","IEEE","","2023 IEEE 41st International Conference on Computer Design (ICCD)","Washington, DC, USA","2023/11/6-2023/11/8","2023-11-06","","","","","2576-6996","1063-6404","https://ieeexplore.ieee.org/abstract/document/10361026/;http://dx.doi.org/10.1109/ICCD58817.2023.00043;https://ieeexplore.ieee.org/document/10361026/;http://dx.doi.org/10.1109/iccd58817.2023.00043","10.1109/iccd58817.2023.00043","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Gan et al. 2023 - Enabling encrypted delta compression for outsourced storage systems via preserving similarity.pdf","","","",""
"Journal article","Hua Z,Yao Y,Song M,Zheng Y,Zhang Y,Wang C","","Blockchain-assisted secure deduplication for large-scale cloud storage service","IEEE Trans. Serv. Comput.","IEEE transactions on services computing","","","","","","2024","17","3","821-835","All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2024-05","","","","","1939-1374","2372-0204","http://dx.doi.org/10.1109/TSC.2024.3350086;https://ieeexplore.ieee.org/document/10381765/;http://dx.doi.org/10.1109/tsc.2024.3350086","10.1109/tsc.2024.3350086","","","","","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hua et al. 2024 - Blockchain-assisted secure deduplication for large-scale cloud storage service.pdf","","","",""
"Conference paper","Dobre D,Viotti P,Vukolić M","","Hybris: Robust Hybrid Cloud Storage","","","","","Proceedings of the ACM Symposium on Cloud Computing","","","2014","","","","Thesis;p-scailbib;All","","","","ACM","New York, NY, USA","SOCC '14: ACM Symposium on Cloud Computing","Seattle WA USA","2014/11/3-2014/11/5","2014-11-03","","","9781450332521","","","","https://dl.acm.org/doi/abs/10.1145/2670979.2670991;https://www.eurecom.fr/fr/publication/4414/download/rs-publi-4414.pdf;http://dx.doi.org/10.1145/2670979.2670991;https://dl.acm.org/doi/10.1145/2670979.2670991","10.1145/2670979.2670991","","","","","","","http://www.acm.org/publications/policies/copyright_policy#Background","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dobre et al. 2014 - Hybris - Robust Hybrid Cloud Storage.pdf","","","",""
"Book","Feng D","","Data Deduplication for High Performance Storage System","","","","","","","","2022","","","","Parallel Schemes;p-scailbib;SCAIL Bibliography;FSL Traces;Thesis;All","","","","Springer Nature","","","","","2022-06-02","","","9789811901126","","","","https://play.google.com/store/books/details?id=YBZzEAAAQBAJ","","","","","This book comprehensively introduces data deduplication technologies for storage systems. It first presents the overview of data deduplication including its theoretical basis, basic workflow, application scenarios and its key technologies, and then the book focuses on each key technology of the deduplication to provide an insight into the evolution of the technology over the years including chunking algorithms, indexing schemes, fragmentation reduced schemes, rewriting algorithm and security solution. In particular, the state-of-the-art solutions and the newly proposed solutions are both elaborated. At the end of the book, the author discusses the fundamental trade-offs in each of deduplication design choices and propose an open-source deduplication prototype. The book with its fundamental theories and complete survey can guide the beginners, students and practitioners working on data deduplication in storage system. It also provides a compact reference in the perspective of key data deduplication technologies for those researchers in developing high performance storage solutions.","","","","","en","","","","","","","162","","","","","","","","","","","","","","","","",""
"Journal article","Wu X,Li J,Wang H,Liu X,Wu W,Li F","","A randomized encryption deduplication method against frequency attack","J. Inf. Secur. Appl.","Journal of information security and applications","","","","","","2024","83","103774","103774","All","","","","Elsevier BV","","","","","2024-06","","","","","2214-2126","2214-2134","http://dx.doi.org/10.1016/j.jisa.2024.103774;https://linkinghub.elsevier.com/retrieve/pii/S2214212624000772","10.1016/j.jisa.2024.103774","","","","","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2024 - A randomized encryption deduplication method against frequency attack.pdf","","","",""
"Journal article","Yu H,Zhang X,Huang W,Zheng W","","PDFS: Partially Dedupped File System for Primary Workloads","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2017","28","3","863-876","All","","","","IEEE","","","","","2017-03-01","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2016.2594070","10.1109/TPDS.2016.2594070","","","","Primary storage dedup is difficult to be accomplished because of challenges to achieve low IO latency and high throughput while eliminating data redundancy effectively in the critical IO Path. In this paper, we design and implement the PDFS, a partially dedupped file system for primary workloads, which is built on a generalized framework using partial data lookup for efficient searching of redundant data in quickly chosen data subsets instead of the whole data. PDFS improves IO latency and throughput systematically by techniques including write path optimization, data dedup parallelization and write order preserving. Such design choices bring dedup to the masses for general primary workloads. Experimental results show that PDFS achieves 74-99 percent of the theoretical maximum dedup ratio with very small or even negative performance degradations compared with main stream file systems without dedup support. Discussions about varied configuring experiences of PDFS are also carried out.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu et al. 2017 - PDFS - Partially Dedupped File System for Primary Workloads.pdf","","","",""
"Journal article","Boneh D,Franklin M","","Identity-Based Encryption from the Weil Pairing","SIAM J. Comput.","SIAM Journal on Computing","","","","","","2003","32","3","586-615","Thesis;All","","","","Society for Industrial and Applied Mathematics","","","","","2003-01-01","","","","","0097-5397","1095-7111","https://doi.org/10.1137/S0097539701398521;http://dx.doi.org/10.1137/S0097539701398521;http://epubs.siam.org/doi/10.1137/S0097539701398521;http://dx.doi.org/10.1137/s0097539701398521","10.1137/S0097539701398521","","","","We propose a fully functional identity-based encryption (IBE) scheme. The scheme has chosen ciphertext security in the random oracle model assuming a variant of the computational Diffie--Hellman problem. Our system is based on bilinear maps between groups. The Weil pairing on elliptic curves is an example of such a map. We give precise definitions for secure IBE schemes and give several applications for such systems.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boneh and Franklin 2003 - Identity-Based Encryption from the Weil Pairing.pdf","","","",""
"Journal article","Zhang S,Xian H,Li Z,Wang L","","SecDedup: Secure Encrypted Data Deduplication With Dynamic Ownership Updating","IEEE Access","","","","","","","2020","8","","186323-186334","All","Ownership Management","","","IEEE","","","","","2020","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2020.3023387;https://ieeexplore.ieee.org/abstract/document/9194768/;https://ieeexplore.ieee.org/iel7/6287639/8948470/09194768.pdf","10.1109/ACCESS.2020.3023387","","","","Deduplication eliminates duplicated data copies and reduces storage costs of cloud service providers. However, deduplication of encrypted data is difficult. Current solutions rely heavily on trusted third parties, and do not address the popularity of data, resulting in unsatisfying security and efficiency. A secure encrypted data deduplication scheme based on data popularity is proposed. Check tags are calculated via bilinear mapping to determine whether different encrypted data originate from the same plaintext. Ciphertext policy attribute-based encryption is applied to protect the tags. A secure key delivery scheme is designed to pass the data encryption key from an initial data uploader to subsequent uploaders via the cloud server in an offline manner. The cloud server can perform deduplication without the assistance of any online third party. Security analysis and simulation experiments are provided, proving the practicability and efficiency of the proposed scheme.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2020 - SecDedup - Secure Encrypted Data Deduplication With Dynamic Ownership Updating.pdf","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194768","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Unknown Title - Unknown Title.pdf","","","",""
"Miscellaneous","Yu CM","","XDedup: Efficient Provably-Secure Cross-User Chunk-Level Client-Side Deduplicated Cloud Storage of Encrypted Data","","","","","","","Cryptology ePrint Archive, Paper 2016/1041","2016","","","","All","PoW","","","","","","","","2016","","","","","","","https://eprint.iacr.org/2016/1041","","","","","","","https://eprint.iacr.org/2016/1041","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu 2016 - XDedup - Efficient Provably-Secure Cross-User Chunk-Level Client-Side Deduplicated Cloud Storage of Encrypted Data.pdf","","","",""
"Journal article","Guo C,Wang L,Tang X,Feng B,Zhang G","","Two-party interactive secure deduplication with efficient data ownership management in cloud storage","Journal of Information Security and Applications","","","","","","","2023","73","","103426","All","Ownership Management","","","","","","","","2023-03-01","","","","","2214-2126","","https://www.sciencedirect.com/science/article/pii/S221421262300011X;http://dx.doi.org/10.1016/j.jisa.2023.103426;https://www.sciencedirect.com/science/article/pii/S221421262300011X?casa_token=h7AqOHPnu80AAAAA:hRDAZYsYPt1v3yjZm4m9OxPP-KQx0yr1aRrr6vkiD4bJ6ovPk58O1NJqfWrJSpNgUd_fiAOKgM4","10.1016/j.jisa.2023.103426","","","","Deduplication, which stores only one copy of duplicate data, is used extensively in cloud storage to reduce the overhead associated with storage. Unfortunately, client-side encryption prevents cloud storage from performing deduplication due to the randomness of traditional encryption. Some existing schemes can balance encryption and deduplication, but their interaction with third-party servers or online users adds additional overhead to the system. Also, the ownership of outsourced data will change frequently due to users’ requests to upload/delete/modify the data. But many existing schemes that can achieve dynamic ownership management have security flaws or require users to manage multiple keys. By focusing on these troubles, we have developed a secure deduplication scheme that does not rely on any third-party entities and supports data ownership management. More specifically, we take advantage of elliptic curve cryptography to design a key-sharing method so that different owners of the same data can share a random key only by interacting with the cloud service provider. And the broadcast encryption is used to manage the ownership of data, and this allows the cloud service provider to control users’ access to outsourced data by updating the broadcast key. In addition, the security analysis shows that the proposed scheme can meet the required security and that it outperforms other related schemes. The detailed simulation comparisons with other related schemes demonstrate that the proposed scheme has good performance.","Cloud storage; Secure deduplication; Two-party interaction; Ownership management; Key-sharing; Re-encryption","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Guo et al. 2023 - Two-party interactive secure deduplication with efficient data ownership management in cloud storage.pdf","","","",""
"Journal article","Zhou Y,Yu Z,Gu L,Feng D","","An efficient encrypted deduplication scheme with security-enhanced proof of ownership in edge computing","BenchCouncil Transactions on Benchmarks, Standards and Evaluations","","","","","","","2022","2","2","100062","All","PoW","","","","","","","","2022-04-01","","","","","2772-4859","","https://www.sciencedirect.com/science/article/pii/S2772485922000497;http://dx.doi.org/10.1016/j.tbench.2022.100062","10.1016/j.tbench.2022.100062","","","","With the rapid expansion of Internet of Things (IoT), relevant files are stored and transmitted at the network edge by employing data deduplication to eliminate redundant data for the best accessibility. Although deduplication improves storage and network efficiency, it decreases security strength and performance. Existing schemes usually adopt message-locked encryption (MLE) to encrypt data, which is vulnerable to brute-force attacks. Meanwhile, these schemes utilize proof-of-ownership (PoW) to prevent duplicate-faking attacks, while they suffer from replay attacks or incur large computation overheads. This paper proposes SE-PoW, an efficient and location-aware hybrid encrypted deduplication scheme with a dual-level security-enhanced Proof-of-Ownership in edge computing. Specifically, SE-PoW firstly encrypts files with an inter-edge server-aided randomized convergent encryption (RCE) method and then protects blocks with an intra-edge edge-aided MLE method to balance security and system efficiency. To resist duplicate-faking attacks and replay attacks, SE-PoW performs the dual-level PoW algorithm. Then it combines the verification of a cuckoo filter and the homomorphism of algebraic signatures in sequence to enhance security and improve ownership checking efficiency. Security analysis demonstrates that SE-PoW ensures data security and resists the mentioned attacks. Evaluation results show that SE-PoW reduces up to 61.9% upload time overheads compared with the state-of-the-art schemes.","Deduplication; Message-locked encryption; Proof of ownership; Edge computing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2022 - An efficient encrypted deduplication scheme with security-enhanced proof of ownership in edge computing.pdf","","","",""
"Journal article","Yuan H,Chen X,Li J,Jiang T,Wang J,Deng RH","","Secure Cloud Data Deduplication with Efficient Re-Encryption","IEEE Trans. Serv. Comput.","IEEE Transactions on Services Computing","","","","","","2022","15","1","442-456","All","","","","IEEE","","","","","01 Jan.-Feb 2022","","","","","1939-1374","2372-0204","http://dx.doi.org/10.1109/TSC.2019.2948007","10.1109/TSC.2019.2948007","","","","Data deduplication technique has been widely adopted by commercial cloud storage providers, which is both important and necessary in coping with the explosive growth of data. To further protect the security of users’ sensitive data in the outsourced storage mode, many secure data deduplication schemes have been designed and applied in various scenarios. Among these schemes, secure and efficient re-encryption for encrypted data deduplication attracted the attention of many scholars, and many solutions have been designed to support dynamic ownership management. In this paper, we focus on the re-encryption deduplication storage system and show that the recently designed lightweight rekeying-aware encrypted deduplication scheme (REED) is vulnerable to an attack which we call it stub-reserved attack. Furthermore, we propose a secure data deduplication scheme with efficient re-encryption based on the convergent all-or-nothing transform (CAONT) and randomly sampled bits from the Bloom filter. Due to the intrinsic property of one-way hash function, our scheme can resist the stub-reserved attack and guarantee the data privacy of data owners’ sensitive data. Moreover, instead of re-encrypting the entire package, data owners are only required to re-encrypt a small part of it through the CAONT, thereby effectively reducing the computation overhead of the system. Finally, security analysis and experimental results show that our scheme is secure and efficient in re-encryption.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yuan et al. 2022 - Secure Cloud Data Deduplication with Efficient Re-Encryption.pdf","","","",""
"Journal article","Wu X,Wang H,Yuan Y,Li F","","A lightweight encrypted deduplication scheme supporting backup","Int. J. High Perform. Syst. Archit.","International Journal of High Performance Systems Architecture","","","","","","2023","138","","102858","All","","","","","","","","","2023-05-01","","","","","1383-7621","","https://www.sciencedirect.com/science/article/pii/S1383762123000371;http://dx.doi.org/10.1016/j.sysarc.2023.102858;https://www.sciencedirect.com/science/article/pii/S1383762123000371/pdf?crasolve=1&r=8628fe641d3c981f&ts=1710133361302&rtype=https&vrr=UKN&redir=UKN&redir_fr=UKN&redir_arc=UKN&vhash=UKN&host=d3d3LnNjaWVuY2VkaXJlY3QuY29t&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&rh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&re=X2JsYW5rXw%3D%3D&ns_h=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ns_e=X2JsYW5rXw%3D%3D&rh_fd=rrr)n%5Ed%60i%5E%60_dm%60%5Eo)%5Ejh&tsoh_fd=rrr)n%5Ed%60i%5E%60_dm%60%5Eo)%5Ejh&iv=86e674eced663babb8b426e09ef9a05a&token=65636161643631386233326534646531636530303763626637623965333665396236616537353531666235366134373163383230333261373664343232376639356533333238363532313439613463646164393763633164616136383763653262303962613363363937343138313633626465336165663730383637643264333a313361316461636361303439616535323332666238336334&text=f5beeb2da6f6e8a5f4a66b9f3a8ccb73607acd9535297c2dd444586c1f1f616dd2428465daf834f7be2f23b52231f8e49d31fa7e2ed3ca63551dc1d0bfef1732e8beacdc47877b32f67b9c6707aa307f7bb304147ccfcedfe38c008dc6ee8618200c1645f451c3cfb7dad135ae35f7fe00446f706d944664072db2d5858ca227081c9c744006a8c279c791a79dfc10dbebc3324063d89ffb9bb42628d28a9bd7eaa73e85ef1876b3ec69f37a8eaf24f0d59a812f28902a70341dcbfb64e95a400ff0178e47af9c419f5614c243cfa5dd75902610f31a389d0890b6496317a6fbab2c14c2db1b19dffd27a4a045bb0806402267a91886c80fbaa35f699eea9f8d95c88d939fa8d3ed3d5ce2d1860260aec92d0cf9322f33fcc239342116c2aa85095fea8962f33b493acca5af0a3bb361&original=3f;https://pdf.sciencedirectassets.com/271017/1-s2.0-S1383762123X00034/1-s2.0-S1383762123000371/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEBwaCXVzLWVhc3QtMSJHMEUCIBfEpthwMSqApTuUESFHpD1LCWA1x1TJOm39ieQl%2F3PaAiEAwOIEQbP7XqWdLi80%2BHSOGkBnEAJ5gw6mFX4e3ovhR1EqswUIJRAFGgwwNTkwMDM1NDY4NjUiDErbGQr25A15pcrA7yqQBXB2mo%2F%2Fg7v3zc7s3SyC0K%2BPO5xUyGA6kUejZLD%2FGISWl7Z88ROw9HOosrLOVR%2FzbbzkFc9WdrUkf9UNpm5zu2twF6UuTu4JAEdlYJ2bhqg9r8mIjAidmsCNhYNahSZmsVqY6Mkpz7hppSv0nUIPTqxTb0R2fwejQSrDFRW8n%2FdQ7Ink0fgOk9V6gzflhWcwZ4vNyIJPs2K16LyVhTpMD7AeuRbvF8%2F4deiheGLFEv2PJQOsZMNJ5KewfYdwzUn0tUSbC2rWtiroJ1kO4tciy2jeVh9RfNelhXzliJostwugWbkkaLbNNt48xjTiB5UwGT3QSeAIT2rWSAOAmxYKtLISnGwhLBjBRqMIiVfUrNuui2O2qtvknGKvjImHMfNwLQ4dbDXm%2BVlwTcJKv%2B2po5oVDjBg5%2BEzr%2FTGbEcOeyXHeTymuqTxTH8BoioScYbZa%2BqdzOTSZYZMxvWsqI8U%2FiIoJtt6hH2ASIEGYfS8jsPlXuqadx710fO2MunK02pGqhKFuIvQc9axLClGYrrdM5%2BkN0yD4O8MbdX8Ic1pRR0q2ZilVn2GcCR8VivG2FD6vTs0nM3yqY%2F7XTDa%2BPk5mhwvGp0SR%2BlUD58HQ3SyPFdw78Qwo8bIyflKCtI%2FJJqiSe5yMojlQKzNvjr9sh0kXZKVfEf2o2QC9183pP5vkhw0%2Bvix9SoXUkNVIss37aYGZTudtGAPeC77OujtspKbYMZgskbVWc75UZFcI0D1SZg3sJMgzqU0ssuiZqCqMR9tIgd9v2fxgdObzK97ojpzUTtYAy0iYjqNO6XO9Nnh%2ByCGrzCDTdYAOHzYI%2F6F2F%2BOBXGldTRyWheLFw8copIXFffHy2jdcDHJHBIbGNHCxrsYMOT9ua8GOrEBv6k%2F4%2Fu20GtDsE2%2FncLGohAVwqRVD0qtTSf3irvCZu%2Fc2TAmWjONJxMoVM5D%2Flxx%2B7rTMwPz1qf9jPTx04LziZ4GsmKpBD9Jid2KfWWLTfEXlXc6upf1sTH8EUuJ%2Fiyn2bebkAEZuN9iqrivexdnhUbkMKz8OZBbmLmcB004AHVllKfyWl4KNqbB14e29%2BmiHvuE32er9UtWZ0T9dcuYLHgTnOdDVpOSaOruz9knFGmf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240311T050257Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUIKLZBMN%2F20240311%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2f5200e098e5240561bc5f0b44b174cde5c193aa955e3cc49255e39594a0f2b7&hash=72986ff2d9fcdd0094cfa710c900d483e3166661de4c81785cf0e1b48bc7ddb5&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1383762123000371&tid=spdf-5282ed99-8a37-4991-b67c-ddb28a280525&sid=623a495570991848ec9b4ca628fe23c258a1gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=15115c585358000203&rr=8628fec73bf59837&cc=us","10.1016/j.sysarc.2023.102858","","","","Data deduplication technology is frequently used by many cloud service providers to handle the exponential growth in data. To protect their data privacy, users frequently encrypt their data before uploading it to a cloud storage provider. Therefore, efficient encrypted deduplication technology has been widely studied. Although cloud storage provides convenience for users, it also has the problem of high data concentration. Once an accident occurs in the data center, the impact is enormous. Therefore, it is necessary to back up the ciphertext data after deduplication. In this paper, we propose an efficient encrypted data deduplication technology that supports backup. Our scheme uses a one-way hash chain to verify the legitimacy of a user’s identity. It updates the user’s ownership of the data using the user’s index table. In this paper, we use the XOR operation to re-encrypt data to obtain backup ciphertext, reducing computational complexity. In LEDB, the user can extract the key from the data label. Therefore, the user does not need to manage the key separately. In addition, LEDB supports a two-level deduplication mechanism, which can not only ensure transmission security but also reduce network transmission bandwidth. Finally, LEDB allows the user to determine the integrity of the data before decrypting the ciphertext. Security analysis and experimental results show that our scheme is secure and efficient for data encryption and decryption.","Ciphertext backup; Convergent encryption; Ownership modification; User revocation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2023 - A lightweight encrypted deduplication scheme supporting backup.pdf","","","",""
"Conference paper","Gharaibeh A,Al-Kiswany S,Gopalakrishnan S,Ripeanu M","","A GPU accelerated storage system","","","","","Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing","","","2010","","","167-178","Thesis;All","GPU","","","Association for Computing Machinery","New York, NY, USA","","Chicago, Illinois","","2010-06-21","","2024-03-10","9781605589428","","","","https://doi.org/10.1145/1851476.1851497;http://dx.doi.org/10.1145/1851476.1851497;https://dl.acm.org/doi/pdf/10.1145/1851476.1851497?casa_token=dVAvoEsWaWQAAAAA:3EHoZXyGunKwt--eCO9XjSQ5gloQcTFnqoBPsPFZt17C88_L46pICF1x4YZtNw42GJhYiqyZvnPQ4w","10.1145/1851476.1851497","","","","Massively multicore processors, like, for example, Graphics Processing Units (GPUs), provide, at a comparable price, a one order of magnitude higher peak performance than traditional CPUs. This drop in the cost of computation, as any order-of-magnitude drop in the cost per unit of performance for a class of system components, triggers the opportunity to redesign systems and to explore new ways to engineer them to recalibrate the cost-to-performance relation.In this context, we focus on data storage: We explore the feasibility of harnessing the GPUs' computational power to improve the performance, reliability, or security of distributed storage systems. In this context we present the design of a storage system prototype that uses GPU offloading to accelerate a number of computationally intensive primitives based on hashing.We evaluate the performance of this prototype under two configurations: as a content addressable storage system that facilitates online similarity detection between successive versions of the same file and as a traditional system that uses hashing to preserve data integrity. Further, we evaluate the impact of offloading to the GPU on competing applications' performance. Our results show that this technique can bring tangible performance gains without negatively impacting the performance of concurrently running applications.Further, this work sheds light on the use of heterogeneous multicore processors for enhancing low-level system primitives, and introduces techniques to efficiently leverage the processing power of GPUs.","storage system design, massively-parallel processors, graphics processing units (GPUs), content addressable storage","","","","","","HPDC '10","","","","","","","","","","","","","","","","","","All Papers/G/Gharaibeh et al. 2010 - A GPU accelerated storage system.pdf","","","",""
"Conference paper","Anand A,Muthukrishnan C,Akella A,Ramjee R","","Redundancy in network traffic: findings and implications","","","","","Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems","","","2009","","","37-48","Thesis;All","","","","Association for Computing Machinery","New York, NY, USA","","Seattle, WA, USA","","2009-06-15","","2024-03-10","9781605585116","","","","https://doi.org/10.1145/1555349.1555355;http://dx.doi.org/10.1145/1555349.1555355;https://dl.acm.org/doi/abs/10.1145/1555349.1555355?casa_token=WreF-993hCkAAAAA:M3k5RWJ2LRrRKK243NkU9xLL8jA20KbdwHPuh2QX3uCgYSt9_IFWLkIufV_cGPm_4Anm4Ec0SknaBQ;https://dl.acm.org/doi/pdf/10.1145/1555349.1555355?casa_token=EOU6YBX996YAAAAA:ozVc6aFjN33MbOs_RvyPWGyQR41zfDHTYgFVvihLrHll--PvLTUnTBYl6zLArFg4YnGt_MOLVS2Z0g;https://dl.acm.org/doi/pdf/10.1145/1555349.1555355","10.1145/1555349.1555355","","","","A large amount of popular content is transferred repeatedly across network links in the Internet. In recent years, protocol-independent redundancy elimination, which can remove duplicate strings from within arbitrary network flows, has emerged as a powerful technique to improve the efficiency of network links in the face of repeated data. Many vendors offer such redundancy elimination middleboxes to improve the effective bandwidth of enterprise, data center and ISP links alike.In this paper, we conduct a large scale trace-driven study of protocol independent redundancy elimination mechanisms, driven by several terabytes of packet payload traces collected at 12 distinct network locations, including the access link of a large US-based university and of 11 enterprise networks of different sizes. Based on extensive analysis, we present a number of findings on the benefits and fundamental design issues in redundancy elimination systems. Two of our key findings are (1) A new redundancy elimination algorithm based on Winnowing that outperforms the widely-used Rabin fingerprint-based algorithm by 5-10% on most traces and by as much as 35% in some traces. (2) A surprising finding that 75-90% of middlebox's bandwidth savings in our enterprise traces is due to redundant byte-strings from within each client's traffic, implying that pushing redundancy elimination capability to the end hosts, i.e. an end-to-end redundancy elimination solution, could obtain most of the middlebox's bandwidth savings.","traffic engineering, traffic redundancy","","","","","","SIGMETRICS '09","","","","","","","","","","","","","","","","","","All Papers/A/Anand et al. 2009 - Redundancy in network traffic - findings and implications.pdf","","","",""
"Journal article","Ha G,Chen H,Jia C,Li M","","Threat Model and Defense Scheme for Side-Channel Attacks in Client-Side Deduplication","Tsinghua Sci. Technol.","Tsinghua science and technology","","","","","","2023","28","1","1-12","Thesis;p-scailbib;All","","","","TUP","","","","","2023-02","","","","","1007-0214","","http://dx.doi.org/10.26599/TST.2021.9010071;https://ieeexplore.ieee.org/abstract/document/9837022/;https://ieeexplore.ieee.org/iel7/5971803/9836992/09837022.pdf","10.26599/TST.2021.9010071","","","","In cloud storage, client-side deduplication is widely used to reduce storage and communication costs. In client-side deduplication, if the cloud server detects that the user's outsourced data have been stored, then clients will not need to reupload the data. However, the information on whether data need to be uploaded can be used as a side-channel, which can consequently be exploited by adversaries to compromise data privacy. In this paper, we propose a new threat model against side-channel attacks. Different from existing schemes, the adversary could learn the approximate ratio of stored chunks to unstored chunks in outsourced files, and this ratio will affect the probability that the adversary compromises the data privacy through side-channel attacks. Under this threat model, we design two defense schemes to minimize privacy leakage, both of which design interaction protocols between clients and the server during deduplication checks to reduce the probability that the adversary compromises data privacy. We analyze the security of our schemes, and evaluate their performances based on a real-world dataset. Compared with existing schemes, our schemes can better mitigate data privacy leakage and have a slightly lower communication cost.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Ha et al. 2023 - Threat Model and Defense Scheme for Side-Channel Attacks in Client-Side Deduplication.pdf","","","",""
"Conference paper","Lucani DE,Nielsen L,Orlandi C,Pagnin E,Vestergaard R","","Secure Generalized Deduplication via Multi-Key Revealing Encryption","","","","","Security and Cryptography for Networks","","","2020","","","298-318","All","","","","Springer International Publishing","","","","","2020","","","","","","","http://dx.doi.org/10.1007/978-3-030-57990-6_15;https://link.springer.com/chapter/10.1007/978-3-030-57990-6_15;https://eprint.iacr.org/2020/799.pdf","10.1007/978-3-030-57990-6_15","","","","Cloud Storage Providers (CSPs) offer solutions to relieve users from locally storing vast amounts of data, including personal and sensitive ones. While users may desire to retain some privacy on the data they outsource, CSPs are interested in reducing the total storage space by employing compression techniques such as deduplication. We propose a new cryptographic primitive that simultaneously realizes both requirements: Multi-Key Revealing Encryption (MKRE). The goal of MKRE is to disclose the result of a pre-defined function over multiple ciphertexts, even if the ciphertexts were generated using different keys, while revealing nothing else about the data. We present a formal model and a security definition for MKRE and provide a construction of MKRE for generalized deduplication that only uses symmetric key primitives in a black-box way. Our construction allows (a) cloud providers to reduce the storage space by using generalized deduplication to compress encrypted data across users, and (b) each user to maintain a certain privacy level for the outsourced information. Our scheme can be proven secure in the random oracle model (and we argue that this is a necessary evil). We develop a proof-of-concept implementation of our solution. For a test data set, our MKRE construction achieves secure generalized deduplication with a compression ratio of 87% for 1 KB file chunks and 82.2% for 8 KB chunks. Finally, our experiments show that, compared to generalized deduplication setup with un-encrypted files, adding privacy via MKRE introduces a compression overhead of less than $$3\%$$and reduces the storage throughput by at most $$6.9\%$$.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lucani et al. 2020 - Secure Generalized Deduplication via Multi-Key Revealing Encryption.pdf","","","",""
"Book chapter","Ateniese G,de Medeiros B","","On the key exposure problem in chameleon hashes","","","Security in Communication Networks","","","","","2005","","","165-179","Thesis;All","PoW","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","2005","","2024-03-01","9783540243014","9783540305989","0302-9743","1611-3349","https://eprint.iacr.org/2004/243.pdf;https://link.springer.com/10.1007/978-3-540-30598-9_12;http://dx.doi.org/10.1007/978-3-540-30598-9_12","10.1007/978-3-540-30598-9_12","","","","","","","https://www.springernature.com/gp/researchers/text-and-data-mining","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/A/Ateniese and de Medeiros 2005 - On the key exposure problem in chameleon hashes.pdf","","","",""
"Conference paper","Ateniese G,de Medeiros B","","Identity-Based Chameleon Hash and Applications","","","","","Financial Cryptography","","","2004","","","164-180","Thesis;All","PoW","","","Springer Berlin Heidelberg","","","","","2004","","2024-03-01","","","","","http://dx.doi.org/10.1007/978-3-540-27809-2_19;https://link.springer.com/chapter/10.1007/978-3-540-27809-2_19;https://eprint.iacr.org/2003/167.pdf","10.1007/978-3-540-27809-2_19","","","","Chameleon signatures are non-interactive signatures based on a hash-and-sign paradigm, and similar in efficiency to regular signatures. The distinguishing characteristic of chameleon signatures is that their are non-transferable, with only the designated recipient capable of asserting its validity. In this paper, we introduce the first identity-based chameleon hash function. The general advantages of identity-based cryptography over conventional schemes relative to key distribution are even more pronounced in a chameleon hashing scheme, because the owner of a public key does not necessarily need to retrieve the associated secret key. We use the identity-based chameleon hashing scheme to build the id-based chameleon signature and a novel sealed-bid auction scheme that is robust, communication efficient (bidders send a single message), and secure under a particular trust model.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Ateniese and de Medeiros 2004 - Identity-Based Chameleon Hash and Applications.pdf","","","",""
"Conference paper","Merkle RC","","A Certified Digital Signature","","","","","Advances in Cryptology — CRYPTO’ 89 Proceedings","","","1990","","","218-238","Thesis;All","PoW","","","Springer New York","","","","","1990","","","","","","","http://dx.doi.org/10.1007/0-387-34805-0_21;https://link.springer.com/chapter/10.1007/0-387-34805-0_21;http://130.18.208.80/~ramkumar/merkle1.pdf;https://link.springer.com/content/pdf/10.1007/0-387-34805-0_21.pdf","10.1007/0-387-34805-0_21","","","","A practical digital signature system based on a conventional encryption function which is as secure as the conventional encryption function is described. Since certified conventional systems are available it can be implemented quickly, without the several years delay required for certification of an untested system.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Merkle 1990 - A Certified Digital Signature.pdf","","","",""
"Conference paper","Halevi S,Harnik D,Pinkas B,Shulman-Peleg A","","Proofs of ownership in remote storage systems","","","","","Proceedings of the 18th ACM conference on Computer and communications security","","","2011","","","491-500","Thesis;All","PoW","","","Association for Computing Machinery","New York, NY, USA","","Chicago, Illinois, USA","","2011-10-17","","2024-03-01","9781450309486","","","","https://doi.org/10.1145/2046707.2046765;http://dx.doi.org/10.1145/2046707.2046765;https://dl.acm.org/doi/pdf/10.1145/2046707.2046765?casa_token=hnzaH5Z5UlsAAAAA:8nl2XTQfIK2EKv9uVayRuigbotVXl9FnTrOrEFvsGQXtHpSOef1ozPXJL1fyW5JhACgc3VdHOUMPKg","10.1145/2046707.2046765","","","","Cloud storage systems are becoming increasingly popular. A promising technology that keeps their cost down is deduplication, which stores only a single copy of repeating data. Client-side deduplication attempts to identify deduplication opportunities already at the client and save the bandwidth of uploading copies of existing files to the server. In this work we identify attacks that exploit client-side deduplication, allowing an attacker to gain access to arbitrary-size files of other users based on a very small hash signatures of these files. More specifically, an attacker who knows the hash signature of a file can convince the storage service that it owns that file, hence the server lets the attacker download the entire file. (In parallel to our work, a subset of these attacks were recently introduced in the wild with respect to the Dropbox file synchronization service.) To overcome such attacks, we introduce the notion of proofs-of-ownership (PoWs), which lets a client efficiently prove to a server that that the client holds a file, rather than just some short information about it. We formalize the concept of proof-of-ownership, under rigorous security definitions, and rigorous efficiency requirements of Petabyte scale storage systems. We then present solutions based on Merkle trees and specific encodings, and analyze their security. We implemented one variant of the scheme. Our performance measurements indicate that the scheme incurs only a small overhead compared to naive client-side deduplication.","cloud storage, deduplication, merkle trees, proofs of ownership","","","","","","CCS '11","","","","","","","","","","","","","","","","","","All Papers/H/Halevi et al. 2011 - Proofs of ownership in remote storage systems.pdf","","","",""
"Conference paper","He Y,Xiang L,Xia W,Zou X","","Dsync: a Lightweight Delta Synchronization Approach for Cloud Storage Services","","","","","The 36th International Conference on Massive Storage Systems and Technology (MSST'20)","","","2020","","","","All","Incremental Backup/Sync","","","unknown","","","","","2020-10-29","","2024-03-01","","","","","http://dx.doi.org/;https://www.researchgate.net/publication/349454491_Dsync_a_Lightweight_Delta_Synchronization_Approach_for_Cloud_Storage_Services;https://www.researchgate.net/profile/Xiangyu-Zou-4/publication/349454491_Dsync_a_Lightweight_Delta_Synchronization_Approach_for_Cloud_Storage_Services/links/603086a692851c4ed5837504/Dsync-a-Lightweight-Delta-Synchronization-Approach-for-Cloud-Storage-Services.pdf;https://www.researchgate.net/publication/349454491","","","","","Delta synchronization (sync) is a key bandwidth-saving technique for cloud storage services. The representative delta sync utility, rsync, matches data chunks by sliding a search window byte by byte, to maximize the redundancy detection for bandwidth efficiency. This process, however, is difficult to cater to the demand of forthcoming high-bandwidth cloud storage services , which require lightweight delta sync that can well support large files. Inspired by the Content-Defined Chunking (CDC) technique used in data deduplication, we propose Dsync, a CDC-based lightweight delta sync approach that has essentially less computation and protocol (metadata) overheads than the state-of-the-art delta sync approaches. The key idea of Dsync is to simplify the process of chunk matching by (1) proposing a novel and fast weak hash called FastFp that is piggybacked on the rolling hashes from CDC; and (2) redesigning the delta sync protocol by exploiting deduplication locality and weak/strong hash properties. Our evaluation results driven by both benchmark and real-world datasets suggest Dsync performs 2×-8× faster and supports 30%-50% more clients than the state-of-the-art rsync-based WebR2sync+ and deduplication-based approach.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/He et al. 2020 - Dsync - a Lightweight Delta Synchronization Approach for Cloud Storage Services.pdf","","","",""
"Journal article","Wu S,Tu Z,Zhou Y,Wang Z,Shen Z,Chen W,Wang W,Wang W,Mao B","","FASTSync: A FAST Delta Sync Scheme for Encrypted Cloud Storage in High-bandwidth Network Environments","ACM Trans. Storage","","","","","","","2023","19","4","1-22","Grouped by Publication/ACM TOS;All","Incremental Backup/Sync","","","Association for Computing Machinery","New York, NY, USA","","","","2023-10-03","","","","","1553-3077","","https://doi.org/10.1145/3607536;http://dx.doi.org/10.1145/3607536;https://dl.acm.org/doi/abs/10.1145/3607536?casa_token=N_honKgNRBsAAAAA:DfqxHd-d8e_Egb6V1ro9XhbOwFUmntWsTWwgHbUtSw9WzXusK9kTA908dsB7NUmtTwGdaMZoMWYu3Q;https://dl.acm.org/doi/pdf/10.1145/3607536?casa_token=VsZDCj6-g8oAAAAA:CUCol-6_-5WVzES2RQA4wv_ffjs4bWVehzYEvCu1yOETyBH3g3yUleV_4r0h_ENcHh9KAE0uk_K_DA","10.1145/3607536","","","","More and more data are stored in cloud storage, which brings two major challenges. First, the modified files in the cloud should be quickly synchronized to ensure data consistency, e.g., delta synchronization (sync) achieves efficient cloud sync by synchronizing only the updated part of the file. Second, the huge data in the cloud needs to be deduplicated and encrypted, e.g., Message-Locked Encryption (MLE) implements data deduplication by encrypting the content among different users. However, when combined, a few updates in the content can cause large sync traffic amplification for both keys and ciphertext in the MLE-based cloud storage, significantly degrading the cloud sync efficiency. A feature-based encryption sync scheme, FeatureSync, is proposed to address the delta amplification problem. However, with further improvement of the network bandwidth, the performance of FeatureSync stagnates. In our preliminary experimental evaluations, we find that the bottleneck of the computational overhead in the high-bandwidth network environments is the main bottleneck in FeatureSync. In this article, we propose an enhanced feature-based encryption sync scheme FASTSync to optimize the performance of FeatureSync in high-bandwidth network environments. The performance evaluations on a lightweight prototype implementation of FASTSync show that FASTSync reduces the cloud sync time by 70.3% and the encryption time by 37.3%, on average, compared with FeatureSync.","Cloud storage, delta synchronization, message-locked encryption, ciphertext synchronization, high-bandwidth network","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2023 - FASTSync - A FAST Delta Sync Scheme for Encrypted Cloud Storage in High-bandwidth Network Environments.pdf","","","37","November 2023"
"Journal article","Ha G,Jia C,Huang Y,Chen H,Li R,Jia Q","","Scalable and Popularity-Based Secure Deduplication Schemes With Fully Random Tags","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","","PP","99","1-17","All","","","","IEEE","","","","","","","","","","1545-5971","1941-0018","http://dx.doi.org/10.1109/TDSC.2023.3285173;https://ieeexplore.ieee.org/abstract/document/10149415/;https://ieeexplore.ieee.org/iel7/8858/4358699/10149415.pdf;https://ieeexplore.ieee.org/ielx7/8858/4358699/10149415.pdf?tp=&arnumber=10149415&isnumber=4358699&ref=","10.1109/TDSC.2023.3285173","","","","It is non-trivial to provide semantic security for user data while achieving deduplication in cloud storage. Some studies deploy a trusted party to store deterministic tags for recording data popularity, then provide different levels of security for data according to popularity. However, deterministic tags are vulnerable to offline brute-force attacks. In this paper, we first propose a popularity-based secure deduplication scheme with fully random tags, which avoids the storage of deterministic tags. Our scheme uses homomorphic encryption (HE) to generate comparable random tags to record data popularity and then uses the binary search in the AVL tree to accelerate the tag comparisons. Besides, we find the popularity tamper attacks in existing schemes and design a proof of ownership (PoW) protocol against it. To achieve scalability and updatability, we introduce the multi-key homomorphic proxy re-encryption (MKH-PRE) to design a multi-tenant scheme. Users in different tenants generate tags using different key pairs, and the cross-tenant tags can be compared for equality. Meanwhile, our multi-tenant scheme supports efficient key updates. We give comprehensive security analysis and conduct performance evaluations based on both synthetic and real-world datasets. The results show that our schemes achieve efficient data encryption and key update, and have high storage efficiency.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Ha et al. - Scalable and Popularity-Based Secure Deduplication Schemes With Fully Random Tags.pdf","","","",""
"Journal article","Song M,Hua Z,Zheng Y,Huang H,Jia X","","LSDedup: Layered Secure Deduplication for Cloud Storage","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2024","73","2","422-435","All","","","","IEEE","","","","","2024-02","","","","","0018-9340","1557-9956","http://dx.doi.org/10.1109/TC.2023.3331953;https://ieeexplore.ieee.org/abstract/document/10316332/?casa_token=wCxnNl2w7QAAAAAA:Uve5TenOZp9jyMcf5iIx8gEJ5cgjvVOdO3I4ZubpKc9wBbp4L3dxdnPlL6gsncnqiJlHXa4v7OklQg;https://ieeexplore.ieee.org/iel7/12/4358213/10316332.pdf?casa_token=w9GwNY6NsqAAAAAA:eCSktXhB4qA7WJGlCGGX-qxyxQHBNw307aKaLapWSY5F3kg-xErVPuuV5zmKSU6W0032vxXbek_2iQ","10.1109/TC.2023.3331953","","","","To implement encrypted data deduplication in a cloud storage system, users must encrypt files using special encryption algorithms (e.g., convergent encryption (CE)), which cannot provide strong protection. The confidential level of an outsourced file is determined by the user himself/herself subjectively or by the owner number of the file objectively. These files owned by a few users are considered strictly confidential and require strong protection. In this paper, we design, analyze and implement LSDedup, which attains a high storage efficiency while providing strictly confidential files (SCFiles) with strong protection. LSDedup allows cloud users to securely interact with cloud servers to check the confidential level of an outsourced file. Users encrypt the SCFiles using standard symmetric encryption algorithms to achieve a high security level, whereas encrypting the less confidential files (LSFiles) using CE such that cloud servers can perform deduplication. LSDedup is designed to prevent cloud servers reporting fake confidential level and a fake file user claiming the ownership of the file. Formal analysis is provided to justify its security. Besides, we implement an LSDedup prototype using Alibaba Cloud as backend storage. Our evaluations demonstrate that LSDedup can work with existing cloud service providers’ APIs and achieves modest performance overhead.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Song et al. 2024 - LSDedup - Layered Secure Deduplication for Cloud Storage.pdf","","","",""
"Journal article","Ha G,Jia C,Chen Y,Chen H,Li M","","A Secure Client-Side Deduplication Scheme Based on Updatable Server-Aided Encryption","IEEE Transactions on Cloud Computing","","","","","","","2023","11","4","3672-3684","All","","","","IEEE","","","","","Oct.-Dec 2023","","","","","2168-7161","2372-0018","http://dx.doi.org/10.1109/TCC.2023.3311760;https://ieeexplore.ieee.org/abstract/document/10239319/?casa_token=8lE8RUV2teUAAAAA:xIDf8yhbf-IzjBcdUFKyx7U8QlSThiVvI8xBqhdFVpb7cTR76AQ8DCuNbh-d4jjVTuuJTjsdVhGjEg;https://ieeexplore.ieee.org/iel7/6245519/6562694/10239319.pdf?casa_token=Qi5YMHvZgKQAAAAA:nLLsYOxAczaeeUFP3I3spcqmvQpm9oWEkuqQlIoo9uaLnFGZbG9MC9YRZxA7zVDd9nNmtgi5QW3G0A","10.1109/TCC.2023.3311760","","","","The server-aided encryption is widely used in encrypted deduplication systems to protect against brute-force attacks. However, it is non-trivial to update the master key managed by the key server in existing schemes. Once the master key is leaked, all user data are vulnerable to offline brute-force attacks. In this article, we extend the server-aided encryption with the updatable encryption (UE) and a dynamic proof of ownership (PoW) protocol to make it support efficient key updates and can be used in the client-side deduplication. Specifically, we design an updatable server-aided encryption scheme based on UE, which achieves efficient encryption and the user-transparent key update for a system-level master key. Besides, to further enable our updatable server-aided encryption to be applicable to the client-side deduplication, we propose a dynamic PoW protocol based on the Merkle tree. Compared to the state-of-the-art PoW scheme, our PoW protects data privacy and allows multi-time leakages for the target file. Finally, we analyze the security of our scheme and present the performance evaluation. The results show that our scheme provides comprehensive security protection for user data and achieves efficient encryption, PoW, and key update.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Ha et al. 2023 - A Secure Client-Side Deduplication Scheme Based on Updatable Server-Aided Encryption.pdf","","","",""
"Thesis","Wu H","","Towards integrating learning algorithms into computer system design","","","","","","","","2019","","","","All","MLL","","","","","","","","2019","","","","","","","https://unsworks.unsw.edu.au/entities/publication/1de92a09-dc6d-4972-b338-372dee15d6e3;https://unsworks.unsw.edu.au/bitstreams/34099f5c-9627-4a7f-88bd-2e1678c0b4eb/download","","","","","","","","","","","","","","","","","","University New South Wales","","","","","","","","","","","","All Papers/W/Wu 2019 - Towards integrating learning algorithms into computer system design.pdf;All Papers/W/Wu 2019 - Towards integrating learning algorithms into computer system design.pdf","","","",""
"Journal article","Douglis F,Bhardwaj D,Qian H,Shilane P","","Content-aware load balancing for distributed backup","USENIX Large Install Syst Adm Conf","LiSA","","","","","","2011","","","13-13","Thesis;p-scailbib;All","","","","","","","","","2011-12-04","","2024-03-01","","","1340-8836","","https://www.usenix.org/event/lisa11/tech/full_papers/Douglis.pdf","","","","","When backing up a large number of computer systems to many different storage devices, an administrator has to balance the workload to ensure the successful completion of all backups within a particular period of time. When these devices were magnetic tapes, this assignment was trivial: find an idle tape drive, write what fits on a tape, and replace tapes as needed. Backing up data onto deduplicating disk storage adds both complexity and opportunity. Since one cannot swap out a filled disk-based file system the way one switches tapes, each separate backup appliance needs an appropriate workload that fits into both the available storage capacity and the throughput available during the backup window. Repeating a given client's backups on the same appliance not only reduces capacity requirements but it can improve performance by eliminating duplicates from network traffic. Conversely, any reconfiguration of the mappings of backup clients to appliances suffers the overhead of repopulating the new appliance with a full copy of a client's data. Reassigning clients to new servers should only be done when the need for load balancing exceeds the overhead of the move. In addition, deduplication offers the opportunity for content-aware load balancing that groups clients together for improved deduplication that can further improve both capacity and performance; we have seen a system with as much as 75% of its data overlapping other systems, though overlap around 10% is more common. We describe an approach for clustering backup clients based on content, assigning them to backup appliances, and adapting future configurations based on changing requirements while minimizing client migration. We define a cost function and compare several algorithms for minimizing this cost. This assignment tool resides in a tier between backup software such as EMC NetWorker and deduplicating storage systems such as EMC Data Domain.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Douglis et al. 2011 - Content-aware load balancing for distributed backup.pdf","","","",""
"Conference paper","Storer MW,Greenan K,Long DD,Miller EL","","Secure data deduplication","","","","","Proceedings of the 4th ACM international workshop on Storage security and survivability","","","2008","","","1-10","Thesis;p-scailbib;All","CE/MLE","","","Association for Computing Machinery","New York, NY, USA","","Alexandria, Virginia, USA","","2008-10-31","","2024-03-01","9781605582993","","","","https://doi.org/10.1145/1456469.1456471;http://dx.doi.org/10.1145/1456469.1456471;https://dl.acm.org/doi/abs/10.1145/1456469.1456471?casa_token=pqnFCVykntoAAAAA:tP4PSwqmtUQ-4e249psJem2-ntXWKxSn6mfe0uTIBljD7lv4S3lm6mTbyT1DuWntfSzJIGwL_QCUrg;https://dl.acm.org/doi/pdf/10.1145/1456469.1456471?casa_token=QvVWXdSHZ4kAAAAA:DGFW_vwTpiw0a3CsMRj-iGjCH424rrtnVFncBXUx283eaLOsQIZnOfUGMHnbtlcrmYfkihFcQvoygA","10.1145/1456469.1456471","","","","As the world moves to digital storage for archival purposes, there is an increasing demand for systems that can provide secure data storage in a cost-effective manner. By identifying common chunks of data both within and between files and storing them only once, deduplication can yield cost savings by increasing the utility of a given amount of storage. Unfortunately, deduplication exploits identical content, while encryption attempts to make all content appear random; the same content encrypted with two different keys results in very different ciphertext. Thus, combining the space efficiency of deduplication with the secrecy aspects of encryption is problematic.We have developed a solution that provides both data security and space efficiency in single-server storage and distributed storage systems. Encryption keys are generated in a consistent manner from the chunk data; thus, identical chunks will always encrypt to the same ciphertext. Furthermore, the keys cannot be deduced from the encrypted chunk data. Since the information each user needs to access and decrypt the chunks that make up a file is encrypted using a key known only to the user, even a full compromise of the system cannot reveal which chunks are used by which users.","single-instance storage, secure storage, optimization, encryption, deduplication, cryptography, capacity","","","","","","StorageSS '08","","","","","","","","","","","","","","","","","","All Papers/S/Storer et al. 2008 - Secure data deduplication.pdf","","","",""
"Book chapter","Sinha GR,Bajaj V","Thwel TT,Sinha GR","18 - Data deduplication applications in cognitive science and computer vision research","","","Data Deduplication Approaches","","","","","2021","","","357-368","All","MLL","","","Academic Press","","","","","2021-01-01","","","9780128233955","","","","https://www.sciencedirect.com/science/article/pii/B978012823395500001X;http://dx.doi.org/10.1016/B978-0-12-823395-5.00001-X;https://www.researchgate.net/profile/Sudhansu-Patra-2/publication/349224609_DedupCloud_an_optimized_efficient_virtual_machine_deduplication_algorithm_in_cloud_computing_environment/links/607061ff92851c8a7bb5dd88/DedupCloud-an-optimized-efficient-virtual-machine-deduplication-algorithm-in-cloud-computing-environment.pdf#page=382","10.1016/B978-0-12-823395-5.00001-X","","","","Computer vision (CV) makes computers mimic as human and thus utilizes the concept of human vision. The human vision capabilities are brought in computers and huge number of operations can be performed by the computers. Image processing is an integral part of any CV system that involves acquisition of images and their processing for specific applications. Medical image processing is researched in modern times and brain research is ongoing all across the world. The study of cognitive science (CS) involves huge amount of data especially images and videos; and in various stages of CS implementations, lot of redundant data are also processed. This limits the computational speed of the system and also affects the utilization of data storage. This chapter highlights the difficulties in CS- and CV-based research, especially related to dimensionality problem and data duplicates. The applications of data deduplication methods will be discussed in context with CS and CV research.","Cognitive science; computer vision; data deduplication; data dimensionality; data storage; computational speed","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sinha and Bajaj 2021 - 18 - Data deduplication applications in cognitive science and computer vision research.pdf","","","",""
"Conference paper","Feldman SD,Bhat A,LaBorde P,Yi Q,Dechev D","","Effective use of non-blocking data structures in a deduplication application","","","","","Proceedings of the 2013 companion publication for conference on Systems, programming, & applications: software for humanity","","","2013","","","133-142","Thesis;p-scailbib;All","","","","Association for Computing Machinery","New York, NY, USA","","Indianapolis, Indiana, USA","","2013-10-26","","2024-03-01","9781450319959","","","","https://doi.org/10.1145/2508075.2508431;http://dx.doi.org/10.1145/2508075.2508431;https://dl.acm.org/doi/pdf/10.1145/2508075.2508431","10.1145/2508075.2508431","","","","Efficient multicore programming demands fundamental data structures that support a high degree of concurrency. Existing research on non-blocking data structures promises to satisfy such demands by providing progress guarantees that allow a significant increase in parallelism while avoiding the safety hazards of lock-based synchronizations. It is well-acknowledged that the use of non-blocking containers can bring significant performance benefits to applications where the shared data experience heavy contention. However, the practical implications of integrating these data structures in real-world applications are not well-understood. In this paper, we study the effective use of non-blocking data structures in a data deduplication application which performs a large number of concurrent compression operations on a data stream using the pipeline parallel processing model. We present our experience of manually refactoring the application from using conventional lock-based synchronization mechanisms to using a wait-free hash map and a set of lock-free queues to boost the degree of concurrency of the application. Our experimental study explores the performance trade-offs of parallelization mechanisms that rely on a) traditional blocking techniques, b) fine-grained mutual exclusion, and c) lock-free and wait-free synchronization.","parallel data structures, multiprocessor software design, lock-free synchronization, concurrent data deduplication, C/C++ multithreading","","","","","","SPLASH '13","","","","","","","","","","","","","","","","","","All Papers/F/Feldman et al. 2013 - Effective use of non-blocking data structures in a deduplication application.pdf","","","",""
"Journal article","Cox LP,Murray CD,Noble BD","","Pastiche: making backup cheap and easy","Oper. Syst. Rev.","ACM SIGOPS Operating Systems Review","","","","","","2003","36","SI","285-298","Thesis;p-scailbib;All","Chunking","","","Association for Computing Machinery","New York, NY, USA","","","","2003-12-31","","","","","0163-5980","","https://doi.org/10.1145/844128.844155;http://dx.doi.org/10.1145/844128.844155;https://dl.acm.org/doi/abs/10.1145/844128.844155?casa_token=tPflqt3OehQAAAAA:qbXhtzjVDLxzBrFH59fI-AtQk_8OuV9quGNahhvxTeWb7dCBEoHFq0hVtZjPLsdgj7HQDvHjP0VOJQ;https://dl.acm.org/doi/pdf/10.1145/844128.844155?casa_token=bNJ7oGUSF0gAAAAA:q0ltcsnDSWUf4x_n8sPxX2bSqLs18BOwgGd0i6RuUCXf_sM_v1D0ywrM2l9FfQdZVjJcPoU4p4fcJA;https://dl.acm.org/doi/pdf/10.1145/844128.844155","10.1145/844128.844155","","","","Backup is cumbersome and expensive. Individual users almost never back up their data, and backup is a significant cost in large organizations. This paper presents Pastiche, a simple and inexpensive backup system. Pastiche exploits excess disk capacity to perform peer-to-peer backup with no administrative costs. Each node minimizes storage overhead by selecting peers that share a significant amount of data. It is easy for common installations to find suitable peers, and peers with high overlap can be identified with only hundreds of bytes. Pastiche provides mechanisms for confidentiality, integrity, and detection of failed or malicious peers. A Pastiche prototype suffers only 7.4% overhead for a modified Andrew Benchmark, and restore performance is comparable to cross-machine copy.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cox et al. 2003 - Pastiche - making backup cheap and easy.pdf","","","","Winter 2002"
"Book","Menezes AJ,van Oorschot PC,Vanstone SA","","Handbook of Applied Cryptography","","","","","","","","2018","","","","All","Frequency","","","CRC Press","","","","","2018-12-07","","","9780429881329","","","","https://play.google.com/store/books/details?id=YyCyDwAAQBAJ","","","","","Cryptography, in particular public-key cryptography, has emerged in the last 20 years as an important discipline that is not only the subject of an enormous amount of research, but provides the foundation for information security in many applications. Standards are emerging to meet the demands for cryptographic protection in most areas of data communications. Public-key cryptographic techniques are now in widespread use, especially in the financial services industry, in the public sector, and by individuals for their personal privacy, such as in electronic mail. This Handbook will serve as a valuable reference for the novice as well as for the expert who needs a wider scope of coverage within the area of cryptography. It is a necessary and timely guide for professionals who practice the art of cryptography. The Handbook of Applied Cryptography provides a treatment that is multifunctional:It serves as an introduction to the more practical aspects of both conventional and public-key cryptography It is a valuable source of the latest techniques and algorithms for the serious practitioner It provides an integrated treatment of the field, while still presenting each major topic as a self-contained unit It provides a mathematical treatment to accompany practical discussions It contains enough abstraction to be a valuable reference for theoreticians while containing enough detail to actually allow implementation of the algorithms discussedNow in its third printing, this is the definitive cryptography reference that the novice as well as experienced developers, designers, researchers, engineers, computer scientists, and mathematicians alike will use.","","","","","en","","","","","","","810","","","","","","","","","","","","","","","","",""
"Conference paper","Jia Q,Ha G,Wang H,Ma H,Chen H","","An enhanced MinHash encryption scheme for encrypted deduplication","","","","","2022 IEEE International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom)","","","2022","","","565-573","All","Frequency","","","IEEE","","","","","2022-12","","","9781665494250","9781665494267","2324-9013","2324-898X","http://dx.doi.org/10.1109/TrustCom56396.2022.00083;https://ieeexplore.ieee.org/document/10063598","10.1109/TrustCom56396.2022.00083","","","","The encrypted deduplication can provide both storage savings and data confidentiality for cloud storage systems. Convergent encryption (CE) is a well-known solution for encrypted deduplication, but it brings huge computation and storage overheads for key management. MinHash encryption is an effective solution to this issue. It reduces the number of keys by grouping multiple consecutive chunks into segments and generating one key for each segment. However, MinHash encryption does not take full advantage of the segment similarity, which can be used to further reduce the overhead for key management. To this end, we augment MinHash encryption with Bloom filter and Locality Sensitivity Hash (LSH) to design an enhanced MinHash encryption scheme. Firstly, our scheme generates a sketch for each segment based on the Bloom filter, and projects sketches to the points in a hash table through LSH functions. Secondly, we detect the segment similarity by the distance between points, and similar segments are grouped into super-segments. Finally, we combine MinHash encryption and server-aided message-locked encryption to encrypt the super-segments for achieving encrypted deduplication. We conduct trace-driven experiments using a realworld dataset. Compared with MinHash encryption, our scheme has higher storage efficiency and better encryption performance.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jia et al. 2022 - An enhanced MinHash encryption scheme for encrypted deduplication.pdf","","","",""
"Journal article","Hu W,Yang T,Matthews JN","","The good, the bad and the ugly of consumer cloud storage","Oper. Syst. Rev.","ACM SIGOPS Operating Systems Review","","","","","","2010","44","3","110-115","Thesis;All","","","","Association for Computing Machinery","New York, NY, USA","","","","2010-08-17","","","","","0163-5980","","https://doi.org/10.1145/1842733.1842751;http://dx.doi.org/10.1145/1842733.1842751;https://dl.acm.org/doi/pdf/10.1145/1842733.1842751?casa_token=dtA4hvj0BAUAAAAA:gS5bpPs5dCISMwRNlcE-qGF8ODBHRd7noWOZDNIiQMF5T623vFTLiPykJfhDsqD8KaBc7g07mdT3Fw","10.1145/1842733.1842751","","","","… storage offerings - Mozy, Carbonite , Dropbox, and CrashPlan - to determine if they live up to the benefits users expect. We document wide variations in backup and restore performance, …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hu et al. 2010 - The good, the bad and the ugly of consumer cloud storage.pdf","","","","July 2010"
"Journal article","Quick D,Choo KK","","Google Drive: Forensic analysis of data remnants","Journal of Network and Computer Applications","","","","","","","2014","40","","179-193","Thesis;All","","","","Elsevier","","","","","2014-04-01","","","","","1084-8045","","https://www.sciencedirect.com/science/article/pii/S1084804513002051;http://dx.doi.org/10.1016/j.jnca.2013.09.016;https://www.sciencedirect.com/science/article/pii/S1084804513002051?casa_token=o9fnzAxY6bAAAAAA:mJHBduTQvGkru2CIv0EWPY8h6pyxkhiqxQe9-XE_pgEtMDpM_gZFj9P_hKp9wSPsM4gYmyxJYIc","10.1016/j.jnca.2013.09.016","","","","Cloud storage is an emerging challenge to digital forensic examiners. The services are increasingly used by consumers, business, and government, and can potentially store large amounts of data. The retrieval of digital evidence from cloud storage services (particularly from offshore providers) can be a challenge in a digital forensic investigation, due to virtualisation, lack of knowledge on location of digital evidence, privacy issues, and legal or jurisdictional boundaries. Google Drive is a popular service, providing users a cost-effective, and in some cases free, ability to access, store, collaborate, and disseminate data. Using Google Drive as a case study, artefacts were identified that are likely to remain after the use of cloud storage, in the context of the experiments, on a computer hard drive and Apple iPhone3G, and the potential access point(s) for digital forensics examiners to secure evidence.","Cloud storage; Cloud storage forensics; Cloud forensics; Digital forensic analysis; Google Drive","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Q/Quick and Choo 2014 - Google Drive - Forensic analysis of data remnants.pdf","","","",""
"Journal article","Gallaway TO,Starkey J","","Google Drive","The Charleston Advisor","","","","","","","2013","14","3","16-19","All","","","","ingentaconnect.com","","","","","2013","","","","","1525-4011","","https://annurev.publisher.ingentaconnect.com/content/annurev/tca/2013/00000014/00000003/art00008;http://dx.doi.org/10.5260/chara.14.3.16;https://www.ingentaconnect.com/content/charleston/chadv/2013/00000014/00000003/art00008","10.5260/chara.14.3.16","","","","Google Drive is a platform that offers two distinct functions. Just like its predecessor Google Docs, Drive continues to offer Web-based office applications with storage in the cloud, which features sharing and collaboration options. The new addition to the platform is a file storage system that synchronizes to a local folder that the user installs, enabling files of many types to be stored and accessed through a Google account. The authors examine the major functions of Google Drive and consider it in light of competitors. In addition, the authors discuss Google Drives utility for library staff and as a collaborative tool that can support students academic work and learning.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Zheng H,Zeng S,Li H,Li Z","","Secure Batch Deduplication without Dual Servers in Backup System","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","","PP","99","1-13","All","","","","IEEE","","","","","","","","","","1545-5971","1941-0018","http://dx.doi.org/10.1109/TDSC.2024.3362796","10.1109/TDSC.2024.3362796","","","","Cloud storage provides highly available and low cost resources to users. However, as massive amounts of outsourced data grow rapidly, an effective data deduplication scheme is necessary. This is a hot and challenging field, in which there are quite a few researches. However, most of previous works require dual-server fashion to be against brute-force attacks and do not support batch checking. It is not practicable for the massive data stored in the cloud. In this paper, we present a secure batch deduplication scheme for backup system. Besides, our scheme resists the brute-force attacks without the aid of other servers. The core idea of the batch deduplication is to separate users into different groups by using short hashes. Within each group, we leverage group key agreement and symmetric encryption to achieve secure batch checking and semantically secure storage. We also extensively evaluate its performance and overhead based on different datasets. We show that our scheme saves the data storage by up to 89.84%. These results show that our scheme is efficient and scalable for cloud backup system and can also ensure data confidentiality.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zheng et al. - Secure Batch Deduplication without Dual Servers in Backup System.pdf","","","",""
"Conference paper","Broder AZ,Charikar M,Frieze AM,Mitzenmacher M","","Min-wise independent permutations (extended abstract)","","","","","Proceedings of the thirtieth annual ACM symposium on Theory of computing","","","1998","","","327-336","All","","","","Association for Computing Machinery","New York, NY, USA","","Dallas, Texas, USA","","1998-05-23","","2024-01-10","9780897919623","","","","https://doi.org/10.1145/276698.276781;http://dx.doi.org/10.1145/276698.276781;https://dl.acm.org/doi/pdf/10.1145/276698.276781","10.1145/276698.276781","","","","","","","","","","","STOC '98","","","","","","","","","","","","","","","","","","All Papers/B/Broder et al. 1998 - Min-wise independent permutations (extended abstract).pdf","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7847069","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Unknown Title - Unknown Title.pdf","","","",""
"Conference paper","Bhagwat D,Eshghi K,Mehra P","","Content-based document routing and index partitioning for scalable similarity-based searches in a large corpus","","","","","Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining","","","2007","","","105-112","All","","","","Association for Computing Machinery","New York, NY, USA","","San Jose, California, USA","","2007-08-12","","","9781595936097","","","","https://doi.org/10.1145/1281192.1281207;http://dx.doi.org/10.1145/1281192.1281207;https://dl.acm.org/doi/abs/10.1145/1281192.1281207;https://www.crss.ucsc.edu/media/pubs/c47b53a4e80f1d8f9709a17b9836db53594a307a.pdf","10.1145/1281192.1281207","","","","We present a document routing and index partitioning scheme for scalable similarity-based search of documents in a large corpus. We consider the case when similarity-based search is performed by finding documents that have features in common with the query document. While it is possible to store all the features of all the documents in one index, this suffers from obvious scalability problems. Our approach is to partition the feature index into multiple smaller partitions that can be hosted on separate servers, enabling scalable and parallel search execution. When a document is ingested into the repository, a small number of partitions are chosen to store the features of the document. To perform similarity-based search, also, only a small number of partitions are queried. Our approach is stateless and incremental. The decision as to which partitions the features of the document should be routed to (for storing at ingestion time, and for similarity based search at query time) is solely based on the features of the document.Our approach scales very well. We show that executing similarity-based searches over such a partitioned search space has minimal impact on the precision and recall of search results, even though every search consults less than 3% of the total number of partitions.","scalability, document routing, distributed indexing, index partitioning, similarity-based search","","","","","","KDD '07","","","","","","","","","","","","","","","","","","All Papers/B/Bhagwat et al. 2007 - Content-based document routing and index partitioning for scalable similarity-based searches in a large corpus.pdf","","","",""
"Website","Collet Y","","xxHash: Extremely fast non-cryptographic hash algorithm","","","","xxHash: Extremely fast non-cryptographic hash algorithm","","","","2023","","","","Thesis;All","","","","","","","","","2023-11-01","","2023-09-01","","","","","https://xxhash.com/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Dabek F,Kaashoek MF,Karger D,Morris R,Stoica I","","Wide-area cooperative storage with CFS","","","","","Proceedings of the eighteenth ACM symposium on Operating systems principles - SOSP '01","","","2001","","","","All","","","","ACM Press","New York, New York, USA","the eighteenth ACM symposium","Banff, Alberta, Canada","2001/10/21-2001/10/24","2001","","2023-10-12","9781581133899","","","","http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15712-s05/www/readings/Dabek01-cfs.pdf;https://doi.org/10.1145/502034.502054?locatt=mode:legacy;http://dx.doi.org/10.1145/502051.502054","10.1145/502051.502054","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dabek et al. 2001 - Wide-area cooperative storage with CFS.pdf","","","",""
"Conference paper","Bolosky W,Corbin S,Goebel D,Douceur JR","J. Bradley Chen RD","Single instance storage in Windows® 2000","","","","","4th USENIX Windows Systems Symposium","","","2000","","","1-12","Thesis;All","","","","","","","","","2000-08-03","","2023-10-11","","","","","https://www.usenix.org/legacy/publications/library/proceedings/usenix-win2000/full_papers/bolosky/bolosky.pdf","","","","","Certain applications, such as Windows 2000's Remote Install service, can result in a set of files in which many different files have the same content. Using a traditional file system to store these files separately results in excessive use of disk and main memory file cache space. Using hard or symbolic links would eliminate the excess resource requirements, but changes the semantics of having separate files, in that updates to one ""copy"" of a file would be visible to users of another ""copy."" We describe the Single Instance Store (SIS), a component within Windows® 2000 that implements links with the semantics of copies for files stored on a Windows 2000 NTFS volume. SIS uses copy-on-close to implements the copy semantics of its links. SIS is structured as a file system filter driver that implements links and a user level service that detects duplicate files and reports them to the filter for conversion into links. Because SIS links are semantically identical to separate files, SIS creates them automatically when it detects files with duplicate contents. This paper describes the design and implementation of SIS in detail, briefly presents measurements of a remote install server showing a 58% disk space savings by using SIS, and discusses other possible uses of SIS.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bolosky et al. 2000 - Single instance storage in Windows® 2000.pdf","","","",""
"Conference paper","Spring NT,Wetherall D","","A protocol-independent technique for eliminating redundant network traffic","","","","","Proceedings of the conference on Applications, Technologies, Architectures, and Protocols for Computer Communication","","","2000","","","87-95","All","","","","Association for Computing Machinery","New York, NY, USA","","Stockholm, Sweden","","2000-08-28","","2023-10-10","9781581132236","","","","https://doi.org/10.1145/347059.347408;http://dx.doi.org/10.1145/347059.347408;https://dl.acm.org/doi/pdf/10.1145/347059.347408","10.1145/347059.347408","","","","We present a technique for identifying repetitive information transfers and use it to analyze the redundancy of network traffic. Our insight is that dynamic content, streaming media and other traffic that is not caught by today's Web caches is nonetheless likely to derive from similar information. We have therefore adapted similarity detection techniques to the problem of designing a system to eliminate redundant transfers. We identify repeated byte ranges between packets to avoid retransmitting the redundant data.We find a high level of redundancy and are able to detect repetition that Web proxy caches are not. In our traces, after Web proxy caching has been applied, an additional 39% of the original volume of Web traffic is found to be redundant. Moreover, because our technique makes no assumptions about HTTP protocol syntax or caching semantics, it provides immediate benefits for other types of content, such as streaming media, FTP traffic, news and mail.","","","","","","","SIGCOMM '00","","","","","","","","","","","","","","","","","","All Papers/S/Spring and Wetherall 2000 - A protocol-independent technique for eliminating redundant network traffic.pdf","","","",""
"Conference paper","Brin S,Davis J,García-Molina H","","Copy detection mechanisms for digital documents","","","","","Proceedings of the 1995 ACM SIGMOD international conference on Management of data","","","1995","","","398-409","All","","","","Association for Computing Machinery","New York, NY, USA","","San Jose, California, USA","","1995-05-22","","2023-10-09","9780897917315","","","","https://doi.org/10.1145/223784.223855;http://dx.doi.org/10.1145/223784.223855;https://dl.acm.org/doi/pdf/10.1145/223784.223855","10.1145/223784.223855","","","","In a digital library system, documents are available in digital form and therefore are more easily copied and their copyrights are more easily violated. This is a very serious problem, as it discourages owners of valuable information from sharing it with authorized users. There are two main philosophies for addressing this problem: prevention and detection. The former actually makes unauthorized use of documents difficult or impossible while the latter makes it easier to discover such activity.In this paper we propose a system for registering documents and then detecting copies, either complete copies or partial copies. We describe algorithms for such detection, and metrics required for evaluating detection mechanisms (covering accuracy, efficiency, and security). We also describe a working prototype, called COPS, describe implementation issues, and present experimental results that suggest the proper settings for copy detection parameters.","","","","","","","SIGMOD '95","","","","","","","","","","","","","","","","","","All Papers/B/Brin et al. 1995 - Copy detection mechanisms for digital documents.pdf","","","",""
"Journal article","Jiang T,Yuan X,Chen Y,Cheng K,Wang L,Chen X,Ma J","","FuzzyDedup: Secure fuzzy deduplication for cloud storage","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","2023","20","3","2466-2483","All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2023-05-01","","","","","1545-5971","1941-0018","http://dx.doi.org/10.1109/TDSC.2022.3185313;https://ieeexplore.ieee.org/document/9804218/;http://dx.doi.org/10.1109/tdsc.2022.3185313","10.1109/tdsc.2022.3185313","","","","","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jiang et al. 2023 - FuzzyDedup - Secure fuzzy deduplication for cloud storage.pdf","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9804218","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Quinlan S,Dorward S","","Venti: A new approach to archival data storage","","","","","Conference on File and Storage Technologies (FAST 02)","","","2002","","","","Thesis;All","client-side deduplication;Fixed Size Blocks","","","usenix.org","","","","","2002","","2023-10-07","","","","","https://www.usenix.org/conference/fast-02/venti-new-approach-archival-data-storage;http://www.usenix.org/publications/library/proceedings/fast02/quinlan/quinlan.pdf;https://www.usenix.org/publications/library/proceedings/fast02/quinlan/quinlan.pdf","","","","","This paper describes a network storage system, called Venti , intended for archival data. In this system, a unique hash of a block's contents acts as the block identifier for read and write …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Q/Quinlan and Dorward 2002 - Venti - A new approach to archival data storage.pdf","","","",""
"Conference paper","Xu G,Tang B,Lu H,Yu Q,Sung CW","","Lipa: A learning-based indexing and prefetching approach for data deduplication","","","","","2019 35th Symposium on mass storage systems and technologies (MSST)","","","2019","","","299-310","All","","IEEE","","","","","","","2019","","","","","","","https://ieeexplore.ieee.org/abstract/document/8890070/;http://www.ee.cityu.edu.hk/~cwsung/paper/conf/2019_MSST_LIPA.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xu et al. 2019 - Lipa - A learning-based indexing and prefetching approach for data deduplication.pdf","","","",""
"Conference paper","Juels A,Kaliski BS","","Pors: proofs of retrievability for large files","","","","","Proceedings of the 14th ACM conference on Computer and communications security","","","2007","","","584-597","Thesis;All","","","","Association for Computing Machinery","New York, NY, USA","","Alexandria, Virginia, USA","","2007-10-28","","2023-09-30","9781595937032","","","","https://doi.org/10.1145/1315245.1315317;http://dx.doi.org/10.1145/1315245.1315317;https://dl.acm.org/doi/abs/10.1145/1315245.1315317;https://www.arijuels.com/wp-content/uploads/2013/09/JK07.pdf","10.1145/1315245.1315317","","","","In this paper, we define and explore proofs of retrievability (PORs). A POR scheme enables an archive or back-up service (prover) to produce a concise proof that a user (verifier) can retrieve a target file F, that is, that the archive retains and reliably transmits file data sufficient for the user to recover F in its entirety.A POR may be viewed as a kind of cryptographic proof of knowledge (POK), but one specially designed to handle a large file (or bitstring) F. We explore POR protocols here in which the communication costs, number of memory accesses for the prover, and storage requirements of the user (verifier) are small parameters essentially independent of the length of F. In addition to proposing new, practical POR constructions, we explore implementation considerations and optimizations that bear on previously explored, related schemes.In a POR, unlike a POK, neither the prover nor the verifier need actually have knowledge of F. PORs give rise to a new and unusual security definition whose formulation is another contribution of our work.We view PORs as an important tool for semi-trusted online archives. Existing cryptographic techniques help users ensure the privacy and integrity of files they retrieve. It is also natural, however, for users to want to verify that archives do not delete or modify files prior to retrieval. The goal of a POR is to accomplish these checks without users having to download the files themselves. A POR can also provide quality-of-service guarantees, i.e., show that a file is retrievable within a certain time bound.","proofs of knowledge, proofs of retrievability, storage security, storage systems","","","","","","CCS '07","","","","","","","","","","","","","","","","","","All Papers/J/Juels and Kaliski 2007 - Pors - proofs of retrievability for large files.pdf","","","",""
"Conference paper","Cidon A,Rumble S,Stutsman R,Katti S,Ousterhout J,Rosenblum M","","Copysets: Reducing the frequency of data loss in cloud storage","","","","","2013 USENIX Annual Technical Conference (USENIX ATC 13)","","","2013","","","37-48","Thesis;All","","","","","","","","","2013","","2023-09-30","9781931971010","","","","https://www.usenix.org/conference/atc13/technical-sessions/presentation/cidon;https://www.usenix.org/system/files/conference/atc13/atc13-cidon.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cidon et al. 2013 - Copysets - Reducing the frequency of data loss in cloud storage.pdf","","","",""
"Website","De Bruin L,Hoepman JH","","Analyzing the Tahoe for privacy friendly replication and file sharing","","","","","","","","","","","","All","","","","","","","","","","","2023-09-08","","","","","https://www.ru.nl/publish/pages/769526/z5_master_thesis_luuk_de_bruin.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/De Bruin and Hoepman - Analyzing the Tahoe for privacy friendly replication and file sharing.pdf","","","",""
"Conference paper","Wilcox-O'Hearn Z,Warner B","","Tahoe: the least-authority filesystem","","","","","Proceedings of the 4th ACM international workshop on Storage security and survivability","","","2008","","","21-26","All;Thesis;p-scailbib","CE/MLE","","","Association for Computing Machinery","New York, NY, USA","","Alexandria, Virginia, USA","","2008-10-31","","2023-09-08","9781605582993","","","","https://doi.org/10.1145/1456469.1456474;http://dx.doi.org/10.1145/1456469.1456474;https://dl.acm.org/doi/abs/10.1145/1456469.1456474;https://eprint.iacr.org/2012/524.pdf","10.1145/1456469.1456474","","","","Tahoe is a system for secure, distributed storage. It uses capabilities for access control, cryptography for confidentiality and integrity, and erasure coding for fault-tolerance. It has been deployed in a commercial backup service and is currently operational. The implementation is Open Source.","open source, capabilities, peer-to-peer, fault-tolerance","","","","","","StorageSS '08","","","","","","","","","","","","","","","","","","","","","",""
"Website","Ammon J,Fenner T,Weston D","","Super-chunking in Deduplication Storage Systems","","","","","","","","2020","","","","All","","","","","","","","","2020-09","","2023-09-06","","","","","https://www.dcs.bbk.ac.uk/site/assets/files/1023/ammons_2020_poster.pdf","","","","","Our research is motivated by problems facing deduplication storage systems (DSS). We hope to devise mechanisms that reduce storage requirements. One technique we are investigating is a mechanism to reduce metadata storage requirements, especially for a dataset which is backed up periodically, an evolving dataset.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Ammons J,Fenner T,Weston D","","SCAIL: Encrypted Deduplication With Segment Chunks and Index Locality","","","","","2022 IEEE International Conference on Networking, Architecture and Storage (NAS)","","","2022","","","1-9","Thesis;All;p-scailbib","","","","IEEE","","","","","2022","","","","","","","http://dx.doi.org/10.1109/NAS55553.2022.9925408","10.1109/NAS55553.2022.9925408","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Ammons et al. 2022 - SCAIL - Encrypted Deduplication With Segment Chunks and Index Locality.pdf;All Papers/A/Ammons et al. 2022 - NAS_2022_SCAIL_CORRECTED.pdf","","","",""
"Miscellaneous","","","ACM_P_SCAIL_RC.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/ACM_P_SCAIL_RC.pdf - ACM_P_SCAIL_RC.pdf","","","",""
"Conference paper","Maron CA,Vogel A,Griebler D,Fernandes LG","","Should PARSEC Benchmarks be More Parametric? A Case Study with Dedup","","","","","2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)","","","2019","","","217-221","All","","","","","","","","","2019-02","","","","","2377-5750","","http://dx.doi.org/10.1109/EMPDP.2019.8671592;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8671592","10.1109/EMPDP.2019.8671592","","","","Parallel applications of the same domain can present similar patterns of behavior and characteristics. Characterizing common application behaviors can help for understanding performance aspects in the real-world scenario. One way to better understand and evaluate applications' characteristics is by using customizable/parametric benchmarks that enable users to represent important characteristics at run-time. We observed that parameterization techniques should be better exploited in the available benchmarks, especially on stream processing domain. For instance, although widely used, the stream processing benchmarks available in PARSEC do not support the simulation and evaluation of relevant and modern characteristics. Therefore, our goal is to identify the stream parallelism characteristics present in PARSEC. We also implemented a ready to use parameterization support and evaluated the application behaviors considering relevant performance metrics for stream parallelism (service time, throughput, latency). We choose Dedup to be our case study. The experimental results have shown performance improvements in our parameterization support for Dedup. Moreover, this support increased the customization space for benchmark users, which is simple to use. In the future, our solution can be potentially explored on different parallel architectures and parallel programming frameworks.","Benchmark testing;Parallel processing;Microsoft Windows;Streaming media;Computer architecture;Measurement;Throughput","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Maron et al. 2019 - Should PARSEC Benchmarks be More Parametric - A Case Study with Dedup.pdf","","","",""
"Journal article","Cheng TC,Kahlbacher HG","","A proof for the longest-job-first policy in one-machine scheduling","Nav. Res. Logist.","Naval Research Logistics","","","","","","1991","38","5","715-720","p-scailbib;Thesis;All","","","","Wiley","","","","","1991-10","","","","","0894-069X","1520-6750","https://onlinelibrary.wiley.com/doi/10.1002/1520-6750(199110)38:5%3C715::AID-NAV3220380506%3E3.0.CO;2-6;http://dx.doi.org/10.1002/1520-6750(199110)38:5<715::aid-nav3220380506>3.0.co;2-6;https://onlinelibrary.wiley.com/doi/abs/10.1002/1520-6750%28199110%2938%3A5%3C715%3A%3AAID-NAV3220380506%3E3.0.CO%3B2-6;http://dx.doi.org/10.1002/1520-6750(199110)38:5<715::AID-NAV3220380506>3.0.CO;2-6","10.1002/1520-6750(199110)38:5<715::aid-nav3220380506>3.0.co;2-6","","","","We consider a one‐machine scheduling problem with earliness and tardiness penalties. All jobs are assigned a common due date and the objective is to minimize the total penalty due to job earliness and tardiness. We are interested in finding the optimal combination of the common due‐date value and the job sequence. Despite the fact that this problem in general is very hard to solve, we prove that there exists at least a common property for all optimal solutions: The first job in an optimal sequence is one of the longest jobs. We also prove that this property holds for a general class of unimodal penalty functions.","","","","","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Yang TM,Feng D,Niu ZY,Wan YP","","Scalable high performance de-duplication backup via hash join","Journal of Zhejiang University SCIENCE C","","","","","","","2010","11","5","315-327","All","","","","","","","","","2010-05-01","","","","","1869-196X","","https://doi.org/10.1631/jzus.C0910445;http://dx.doi.org/10.1631/jzus.C0910445;https://link.springer.com/article/10.1631/jzus.C0910445","10.1631/jzus.C0910445","","","","Apart from high space efficiency, other demanding requirements for enterprise de-duplication backup are high performance, high scalability, and availability for large-scale distributed environments. The main challenge is reducing the significant disk input/output (I/O) overhead as a result of constantly accessing the disk to identify duplicate chunks. Existing inline de-duplication approaches mainly rely on duplicate locality to avoid disk bottleneck, thus suffering from degradation under poor duplicate locality workload. This paper presents Chunkfarm, a post-processing de-duplication backup system designed to improve capacity, throughput, and scalability for de-duplication. Chunkfarm performs de-duplication backup using the hash join algorithm, which turns the notoriously random and small disk I/Os of fingerprint lookups and updates into large sequential disk I/Os, hence achieving high write throughput not influenced by workload locality. More importantly, by decentralizing fingerprint lookup and update, Chunkfarm supports a cluster of servers to perform de-duplication backup in parallel; it hence is conducive to distributed implementation and thus applicable to large-scale and distributed storage systems.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2010 - Scalable high performance de-duplication backup via hash join.pdf","","","",""
"Journal article","Mseddi A,Salahuddin MA,Zhani MF,Elbiaze H,Glitho RH","","Efficient replica migration scheme for distributed cloud storage systems","IEEE Trans. Cloud Comput.","IEEE transactions on cloud computing","","","","","","2021","9","1","155-167","All","","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2021-01-01","","","","","2168-7161","2372-0018","https://ieeexplore.ieee.org/abstract/document/8417951/;https://ieeexplore.ieee.org/document/8417951/;http://dx.doi.org/10.1109/tcc.2018.2858792","10.1109/tcc.2018.2858792","","","","","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Mseddi et al. 2021 - Efficient replica migration scheme for distributed cloud storage systems.pdf","","","",""
"Journal article","Luo S,Zhang G,Wu C,Khan SU,Li K","","Boafft: Distributed Deduplication for Big Data Storage in the Cloud","IEEE Transactions on Cloud Computing","","","","","","","2020","8","4","1199-1211","Parallel Schemes;All;Thesis;p-scailbib","Similarity/Resemblance","","","IEEE","","","","","01 Oct.-Dec 2020","","","","","2168-7161","2372-0018","http://dx.doi.org/10.1109/TCC.2015.2511752;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7364228","10.1109/TCC.2015.2511752","","","","As data progressively grows within data centers, the cloud storage systems continuously facechallenges in saving storage capacity and providing capabilities necessary to move big data within an acceptable time frame. In this paper, we present the Boafft, a cloud storage system with distributed deduplication. The Boafft achieves scalable throughput and capacity usingmultiple data servers to deduplicate data in parallel, with a minimal loss of deduplication ratio. Firstly, the Boafft uses an efficient data routing algorithm based on data similarity that reduces the network overhead by quickly identifying the storage location. Secondly, the Boafft maintains an in-memory similarity indexing in each data server that helps avoid a large number of random disk reads and writes, which in turn accelerates local data deduplication. Thirdly, the Boafft constructs hot fingerprint cache in each data server based on access frequency, so as to improve the data deduplication ratio. Our comparative analysis with EMC's stateful routing algorithm reveals that the Boafft can provide a comparatively high deduplication ratio with a low network bandwidth overhead. Moreover, the Boafft makes better usage of the storage space, with higher read/write bandwidth and good load balance.","Cloud computing;Routing;Big Data;Storage management;Distributed databases;Throughput;Bandwidth;File systems;Big data;cloud storage;data deduplication;data routing;file system","Superblock and hot fingerprint","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Luo et al. 2020 - Boafft - Distributed Deduplication for Big Data Storage in the Cloud.pdf","","","",""
"Conference paper","Ma X,Yang W,Zhu Y,Bai Z","","A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing","","","","","2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)","","","2022","","","194-201","All","","","","","","","","","2022-11","","","","","2374-9628","","http://dx.doi.org/10.1109/IPCCC55026.2022.9894331","10.1109/IPCCC55026.2022.9894331","","","","Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.","Access control;Performance evaluation;Cloud computing;Storage management;Resists;Computer architecture;Bandwidth;Data Deduplication;Cloud Computing;Access Control;Storage Management;hybrid cloud","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Ma et al. 2022 - A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing.pdf","","","",""
"Journal article","Liu J,Chai Y,Yan C,Wang X","","A Delayed Container Organization Approach to Improve Restore Speed for Deduplication Systems","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2016","27","9","2477-2491","All","","","","","","","","","2016-09","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2015.2509060;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7358159","10.1109/TPDS.2015.2509060","","","","Data deduplication has become necessary to improve the space-efficiency of large-scale distributed storage systems, as the global data have accumulated at an exponential rate and they have significant redundancy. However, the negative impact on restore performance is a main challenge for deduplication systems. One of the key reasons is that when restoring data, the low average useful data ratio (UDR) of containers wastes a considerable part of disk bandwidth to read useless data. This is mainly attributed to the uncontrollable compositions of containers. To solve this problem, we propose a new approach called Delayed Container Organization (DCO) to delay the construction of containers after accumulating some redundant data chunks in fast Non-Volatile Memory (NVM) devices to organize high-UDR containers. For example, data chunks in the intersection of some data segments can be organized together in one container to achieve both high deduplication ratio and high UDRs when restoring these related data segments. DCO is implemented in a prototype deduplication system. The experimental results indicate that compared with Capping, DCO promotes the average UDR of containers by 38.30 percent, improves the restore performance by a factor of 2.2, and achieves better space-efficiency and higher cost performance.","Containers;Nonvolatile memory;Organizations;Acceleration;Distributed databases;Redundancy;Bandwidth;Deduplication;NVM;SSD;restore;UDR","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Liu et al. 2016 - A Delayed Container Organization Approach to Improve Restore Speed for Deduplication Systems.pdf","","","",""
"Preprint","Eddelbuettel D","","A Brief Introduction to Redis","","","","","","arXiv [stat.CO]","","2022","","","","PR-SCAIL;p-scailbib;Thesis;All","","","","","","","","","2022-03-13","","","","","","","http://arxiv.org/abs/2203.06559;http://dx.doi.org/10.1080/00031305.2017.1375990","10.1080/00031305.2017.1375990","","2203.06559","","This note provides a brief introduction to Redis highlighting its usefulness in multi-lingual statistical computing.","","","","","","","","","arXiv","2203.06559","stat.CO","","","","","","","","","","","","","","All Papers/E/Eddelbuettel 2022 - A Brief Introduction to Redis.pdf","","","",""
"Preprint","Moritz P,Nishihara R,Wang S,Tumanov A,Liaw R,Liang E,Elibol M,Yang Z,Paul W,Jordan MI,Stoica I","","Ray: A Distributed Framework for Emerging AI Applications","","","","","","arXiv [cs.DC]","","2017","","","","PR-SCAIL;p-scailbib;Thesis;All","","","","","","","","","2017-12-16","","","","","","","http://arxiv.org/abs/1712.05889","","","1712.05889","","The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray---a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system's control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.","","","","","","","","","arXiv","1712.05889","cs.DC","","","","","","","","","","","","","","All Papers/M/Moritz et al. 2017 - Ray - A Distributed Framework for Emerging AI Applications.pdf","","","",""
"Conference paper","Das S,Saraf M,Jagadeesh V,Amardeep MJ,Phalachandra HL","","Deduplication of Docker Image Registry","","","","","2021 IEEE Madras Section Conference (MASCON)","","","2021","","","1-8","All","","","","","","","","","2021-08","","","","","","","http://dx.doi.org/10.1109/MASCON51689.2021.9563465;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9563465","10.1109/MASCON51689.2021.9563465","","","","Containers are prominently being used today, as a mechanism for delivering application packages bundled with the requisite libraries, configuration information and everything needed to run and seamlessly deploy applications. Docker has evolved as the de-facto standard for building and sharing these containerized applications. Images, the bundled application templates, are executed with configuration information for creation of these application containers. These images stored in Docker registries, could be in DockerHub which is a public Docker registry or in private registries, have an increased need for storage with the increase in proliferation and adoption of containers. This increased need for scaling leads to the need for optimization approaches to manage and effectively use this storage. Analysis shows that there is an abundance of interrelationship, interdependence, common pieces and redundant components with these applications. This paper proposes a solution to reduce this redundancy using data deduplication and also proposed techniques to offset the potential increase in latencies for reconstructing images and thus a solution for optimal management of storage scaling.","IEEE Sections;Conferences;Redundancy;Buildings;Containers;Libraries;Image storage;File Level Deduplication;Docker Images;Docker Registry;Two Tier Storage;Reconstruction;Deduplication Ratio","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Anwar A,Mohamed M,Tarasov V,Littley M,Rupprecht L,Cheng Y,Zhao N,Skourtis D,Warke AS,Ludwig H,Others","","Improving docker registry design based on production workload analysis","","","","","16th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 18)","","","2018","","","265-278","All","","","","","","","","","2018","","","","","","","https://people.cs.vt.edu/~butta/docs/fast2018-dockerAnalysis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Anwar et al. 2018 - Improving docker registry design based on production workload analysis.pdf","","","",""
"Conference paper","Zhang Z,Bhagwat D,Litwin W,Long D,Schwarz SJ","","Improved deduplication through parallel Binning","","","","","2012 IEEE 31st International Performance Computing and Communications Conference (IPCCC)","","","2012","","","","All","","","","IEEE","","2012 IEEE 31st International Performance Computing and Communications Conference (IPCCC)","Austin, TX, USA","2012/12/1-2012/12/3","2012-12","","2023-05-02","9781467348836","9781467348812","","","https://d1wqtxts1xzle7.cloudfront.net/84321163/zhang-ipccc12-libre.pdf?1650200795=&response-content-disposition=inline%3B+filename%3DImproved_deduplication_through_parallel.pdf&Expires=1683019228&Signature=CbRCVAH53fHoj0gnJwfGSCHMlOx6JA1jZskU8BVj5HIdMfaBNk-FaOkolHXsn1Pwq8fGgI2QqMgyPfzO7zm8d-lZd3UCWtNl49hzDMiW6qr-bgDIR7EoSsw-OEDuGt7rGueIT4vhrqGtH1l5bJ7TmyZEx0Z1YD2hztZ9jt0hpphCBXAWBVT5vgajs57XAryASfz35L4QtoSvvm6501EeeaAv71UKY0f5dcXJQDagu5DlF92wHu305-KCac4x5VOmcp7Qpzqn5iAkzzl3fTw8Vz3OTsEe3~Q1M~5y5krESVAyPKwuYjhMleiJG6BB8xacnmVjDvbZbIbI9SGTVtA3OQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA;http://ieeexplore.ieee.org/document/6407746/;http://dx.doi.org/10.1109/pccc.2012.6407746","10.1109/pccc.2012.6407746","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2012 - Improved deduplication through parallel Binning.pdf","","","",""
"Journal article","Saharan S,Somani G,Gupta G,Verma R,Gaur MS,Buyya R","","QuickDedup: Efficient VM deduplication in cloud computing environments","J. Parallel Distrib. Comput.","Journal of parallel and distributed computing","","","","","","2020","139","","18-31","All","","","","Elsevier","","","","","2020-05-01","","","","","0743-7315","","https://www.sciencedirect.com/science/article/pii/S0743731519303442;http://dx.doi.org/10.1016/j.jpdc.2020.01.002","10.1016/j.jpdc.2020.01.002","","","","Deduplication is one of the major storage optimisation techniques for Virtual Machines (VMs) in cloud environment. Usually, hashing of blocks helps in identifying duplicate data blocks. This paper proposes a novel deduplication approach, QuickDedup that reduces the overall deduplication time, metadata overhead and the number of hash computations, and subsequent comparisons for the VM disk images. In addition to minimising the deduplication related metadata, which is a necessary by-product useful in checking deduplication, QuickDedup, follows novel byte comparison scheme to prepare various block classes. This way, QuickDedup eliminates or minimises the need for hash calculation and subsequent comparisons. QuickDedup performs the calculation and comparisons of hashes within the respective categories only. QuickDedup saves the space required for hash storage during deduplication and makes deduplication of VM disk images much faster. We conducted a detailed evaluation of QuickDedup on various metrics with different kinds and sizes of VM images taken from publicly available datasets. The evaluation results show a substantial improvement of up to 96% in the overall deduplication time required to deduplicate VM images apart from significant savings in metadata and storage overhead.","Deduplication; VM disk image; Storage; Hashing performance","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Saharan et al. 2020 - QuickDedup - Efficient VM deduplication in cloud computing environments.pdf","","","",""
"Book chapter","Feng D","Feng D","Post-deduplication Delta Compression Schemes","","","Data Deduplication for High Performance Storage System","","","","","2022","","","111-134","All","","","","Springer Nature Singapore","Singapore","","","","2022","","","9789811901126","","","","https://doi.org/10.1007/978-981-19-0112-6_7;http://dx.doi.org/10.1007/978-981-19-0112-6_7;https://link.springer.com/chapter/10.1007/978-981-19-0112-6_7","10.1007/978-981-19-0112-6_7","","","","Delta compression has been gaining increasing attention in recent years for its ability to remove redundancy among non-duplicate but very similar data files and chunks, for which the data deduplication technology often fails to identify and eliminate. Given a new chunk B and an existing chunk A, delta compression encodes B relative to A and generates their differences. We call the differences a “delta” and the chunk A its “base.” We then only need to transfer or store the delta, rather than the entire chunk B, thus obtaining bandwidth- or space-savings. Due to significant data reduction efficiency, some applications adopt delta compression as a complement for chunk-level deduplication to further reduce the storage space (or bandwidth) requirement. In this chapter, we discuss the problems facing the post-deduplication delta compression and the solutions to address the problems. The rest of this chapter is organized as follows: Section 7.1 presents the state-of-the-art solutions for post-deduplication delta compression techniques. Section 7.2 describes the design and implementations of our proposed deduplication-inspired delta compression approach, called Ddelta. Section 7.3 describes our proposed delta compressed and deduplicated technique with low overhead.","","Offers Birkbeck access but fails to properly authenticate.","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Zou X,Deng C,Xia W,Shilane P,Tan H,Zhang H,Wang X","","Odess: Speeding up Resemblance Detection for Redundancy Elimination by Fast Content-Defined Sampling","","","","","2021 IEEE 37th International Conference on Data Engineering (ICDE)","","","2021","","","480-491","All;Thesis;p-scailbib","Similarity/Resemblance","","","ieeexplore.ieee.org","","","","","2021-04","","","","","2375-026X","","http://dx.doi.org/10.1109/ICDE51399.2021.00048;https://ieeexplore.ieee.org/abstract/document/9458911/;https://www.researchgate.net/profile/Xiangyu-Zou-4/publication/351037342_Odess_Speeding_up_Resemblance_Detection_for_Redundancy_Elimination_by_Fast_Content-Defined_Sampling/links/60806985881fa114b41b6604/Odess-Speeding-up-Resemblance-Detection-for-Redundancy-Elimination-by-Fast-Content-Defined-Sampling.pdf","10.1109/ICDE51399.2021.00048","","","","Multiple data reduction techniques have been investigated to lower storage costs for a wide variety of customers. In this work, we focus on similarity-based delta compression, which calculates and stores the difference of very similar, but non-duplicate, chunks in storage systems. Delta compression is often implemented along with deduplication and has been shown to achieve a much higher compression ratio.Currently, the N-Transform method is the most popular and widely-used approach to generate features for data content (e.g. chunks) to detect similar candidates (and then apply delta compression). For delta compression systems, though, the throughput of N-Transform is often the bottleneck. Finesse is a high throughput variant of N-Transform, but it suffers from lower detection accuracy and compression ratio. The computation overhead of N-Transform consists of two parts: calculating the rolling hash across data and applying time-consuming transforms on each hash. In this work, we propose Odess, a fast resemblance detection approach, that uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set and then applies transforms on this small hash set. This reduces the calculations in the transform step from being the bottleneck. Meanwhile, Odess also leverages the faster Gear hash to generate rolling hashes. Thus, Odess greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and high compression ratio.Our evaluation results show that Odess is 5.4× (Finesse) and 26.9× (N-Transform) faster (on average) at generating features for resemblance detection. When considering an end-to-end data reduction storage system, Odess increases throughput by 1.36× (Finesse) and 2.76× (N-Transform) while maintaining the compression ratio of N-Transform and increasing the compression ratio 1.22× over Finesse.","Gears;Conferences;Redundancy;Transforms;Throughput;Feature extraction;Sampling methods;Resemblance Detection;Sampling;Delta Compression;Deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zou et al. 2021 - Odess - Speeding up Resemblance Detection for Redundancy Elimination by Fast Content-Defined Sampling.pdf;All Papers/Z/Zou et al. 2021 - Odess - Speeding up Resemblance Detection for Redundancy Elimination by Fast Content-Defined Sampling.pdf","","","",""
"Conference paper","Xue Z,Qian H,Shen L,Wu X","","A Comprehensive Study of Present Data Deduplication","","","","","2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)","","","2021","","","1748-1754","All","","","","ieeexplore.ieee.org","","","","","2021-12","","","","","","","http://dx.doi.org/10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00257;https://ieeexplore.ieee.org/abstract/document/9781103/","10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00257","","","","With the proliferation of the Internet of Things (IoT), various computing paradigms have been proposed and are developing rapidly these years, which led to the explosive increase of the data amount. However, the significant increase in data imposed a significant burden on contemporary server storage. Many approaches have been designed to tackle the challenge. Among them, deduplication is a quite effective data reduction technique, which received considerable attention from both academia and industry in the large-scale storage systems field. Data deduplication identifies redundant data at chunk level by using secure fingerprints, which not only removes replicated data, decreases the bandwidth, but also minimizes the storage usage and cost. This paper aims at describing the general framework of deduplication compression systems with duplicate and resemblance detection in detail. First, we use a flow chart to present the overall framework and process of the deduplication compression system. Then we summarize the existing algorithm applied to duplicate detection and resemblance detection. Additionally, we make a detailed evaluation of different resemblance detection algorithms. Finally, we make a conclusion of delta compressed prototype system, outline the open problems and shed a light on the future research directions in data deduplication systems.","Industries;Costs;Smart cities;Prototypes;Bandwidth;Fingerprint recognition;Explosives;Data deduplication;resemblance detection;delta compression","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xue et al. 2021 - A Comprehensive Study of Present Data Deduplication.pdf","","","",""
"Conference paper","Li B,Tian W,Li R,Xiao W,Fu Z,Ye X,Duan R,Li Y,Xu Z","","Cross-domain Resemblance Detection based on Meta-learning for Cloud Storage","","","","","2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)","","","2022","","","374-379","All","","","","ieeexplore.ieee.org","","","","","2022-11","","","","","2374-9628","","http://dx.doi.org/10.1109/IPCCC55026.2022.9894349;https://ieeexplore.ieee.org/abstract/document/9894349/","10.1109/IPCCC55026.2022.9894349","","","","Recently, cloud storage has been widely used in our daily life. And there are lots of redundancy among these outsourced data. Conventional deduplication technology efficiently splits these data at the chunk level and removes the duplicate chunks to save the network bandwidth and improve the cloud storage utility. But it ignores the redundancy among similar chunks. Resemblance detection has recently become a hot issue with detecting these redundant parts among similar data. CARD, the state-of-the-art work, can efficiently and effectively remove these redundancies by introducing the neural network with resemblance detection. However, the source domain of the CARD model may have an explicitly different input distribution. The cloud cannot deal with the possible future domain data based on CARD design. This cross-domain setting may serials degrades the performance of CARD. To overcome this problem, we propose a cross-domain resemblance detection scheme called MetaContext. Integrating the chunk-context aware model and the learn-to-learn idea can produce a more robust chunk feature than CARD. As a byproduct, it also outperforms the CARD in speed. Finally, we implement the MetaContext and conduct serial experiments on real workloads. The results show that our method can efficiently and effectively detect and remove the redundancy among similar data.","Cloud computing;Redundancy;Neural networks;Bandwidth;Cloud Storage;Data Deduplication;Meta-Learning;Resemblance Detection","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2022 - Cross-domain Resemblance Detection based on Meta-learning for Cloud Storage.pdf","","","",""
"Journal article","Xia W,Pu L,Zou X,Shilane P,Li S,Zhang H,Wang X","","The Design of Fast and Lightweight Resemblance Detection for Efficient Post-Deduplication Delta Compression","ACM Trans. Storage","","","","","","","2023","","","","All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2023-02-16","","","","","1553-3077","","https://doi.org/10.1145/3584663;http://dx.doi.org/10.1145/3584663;https://dl.acm.org/doi/abs/10.1145/3584663;https://dl.acm.org/doi/pdf/10.1145/3584663","10.1145/3584663","","","","Post-deduplication delta compression is a data reduction technique that calculates and stores the differences of the very similar but non-duplicate chunks in storage systems, which is able to achieve a very high compression ratio. However, the low throughput of widely-used resemblance detection approaches (e.g., N-Transform) usually becomes the bottleneck of delta compression systems due to introducing high computational overhead. Generally, this overhead mainly consists of two parts: ① calculating the rolling hash byte-by-byte across data chunks and ② applying multiple transforms on all the calculated rolling hash values. In this paper, we propose Odess, a fast and lightweight resemblance detection approach, that greatly reduces the computational overhead for resemblance detection while achieving high detection accuracy and high compression ratio. Specifically, Odess first utilizes a novel Subwindow-based Parallel Rolling hash method (called SWPR) using SIMD (i.e., Single Instruction Multiple Data [1]) to accelerate calculation of rolling hashes (corresponding to the first part of the overhead). Moreover, Odess uses a novel Content-Defined Sampling method to generate a much smaller proxy hash set from the whole rolling hash set, and then quickly applies transforms on this small hash set for resemblance detection (corresponding to the second part of the overhead). Evaluation results show that during the stage of resemblance detection, the Odess approach is ∼ 31.4 × and ∼ 7.9 × faster than the state-of-the-art N-Transform and Finesse (i.e., a recent variant of N-Transform [39]), respectively. When considering an end-to-end data reduction storage system, Odess-based system’s throughput is about 3.20 × and 1.41 × higher than N-Transform and Finesse-based systems’ throughput, respectively, while maintaining the high compression ratio of N-Transform and achieving ∼ 1.22 × higher compression ratio over Finesse.","parallel rolling hash, content-defined sampling, SIMD, post-deduplication delta compression, resemblance detection","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2023 - The Design of Fast and Lightweight Resemblance Detection for Efficient Post-Deduplication Delta Compression.pdf","","","",""
"Conference paper","Geng Y,Tian W,Li R,Xiao W,Ouyang C,Liu Y,Liu Q,Li J,Ye X,Xu Z","","Context-aware Resemblance Detection based Deduplication Ratio Prediction for Cloud Storage","","","","","2022 IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT)","","","2022","","","21-29","All","","","","ieeexplore.ieee.org","","","","","2022-12","","","","","","","http://dx.doi.org/10.1109/BDCAT56447.2022.00011;https://ieeexplore.ieee.org/abstract/document/10062265/","10.1109/BDCAT56447.2022.00011","","","","With the prevalence of cloud storage, people prefer to outsource their data to the cloud for flexibility and reliability. Undoubtedly, there are lots of redundancy among these data. However, high-end storage with deduplication costs heavy computation and increases the data management complexity. Potential customers need the redundancy proportion information of their outsourced data to decide whether high-end storage with deduplication is worthwhile. Thus, many researchers have previously attempted to predict the redundant ratio. However, existing mechanisms ignore the redundancy proportion among similar chunks containing many duplicate data. Although resemblance detection, detecting the duplicate parts among similar data, has become a hot issue, it is hardly applied to the conventional deduplication ratio estimation because of unacceptable calculation cost. Therefore, we analyze the limitations and challenges of deduplication ratio prediction in prediction scope and response time and further propose a novel prediction scheme. By leveraging the context-aware resemblance detection, and confidence interval theory, our method can achieve faster estimation speed with higher accuracy in deduplication ratio compared with the state-of-the-art work. Finally, the results show that our method can efficiently and effectively estimate the proportion of duplicate chunks and redundant data among similar chunks by conducting experiments on real workloads.","Cloud computing;Costs;Redundancy;Estimation;Prototypes;Prediction methods;Machine learning;Cloud Storage;Resemblance Detection;Context-Aware;Deduplication Ratio Prediction","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Ye X,Xue X,Tian W,Xu Z,Xiao W,Li R","","Chunk Content is not Enough: Chunk-Context Aware Resemblance Detection for Deduplication Delta Compression","","","","","","arXiv [cs.DC]","","2021","","","","All","Similarity/Resemblance","","","","","","","","2021-06-02","","","","","","","http://arxiv.org/abs/2106.01273","","","2106.01273","","With the growing popularity of cloud storage, removing duplicated data across users is getting more critical for service providers to reduce costs. Recently, Data resemblance detection is a novel technology to detect redundancy among similarity. It extracts feature from each chunk content and treat chunks with high similarity as candidates for removing redundancy. However, popular resemblance methods such as ""N-transform"" and ""Finesse"" use only the chunk data for feature extraction. A minor modification on the data chunk could seriously deteriorate its capability for resemblance detection. In this paper, we proposes a novel chunk-context aware resemblance detection algorithm, called CARD, to mitigate this issue. CARD introduces a BP-Neural network-based chunk-context aware model, and uses N-sub-chunk shingles-based initial feature extraction strategy. It effectively integrates each data chunk content's internal structure with the context information for feature extraction, the impact of small changes in data chunks is significantly reduced. To evaluate its performance, we implement a CARD prototype and conduct extensive experiments using real-world data sets. The results show that CARD can detect up to 75.03% more redundant data and accelerate the resemblance detection operations by 5.6 to 17.8 times faster compared with the state-of-the-art resemblance detection approaches.","","","","","","","","","arXiv","2106.01273","cs.DC","","","","","","","","","","","","","","All Papers/Y/Ye et al. 2021 - Chunk Content is not Enough - Chunk-Context Aware Resemblance Detection for Deduplication Delta Compression.pdf","","","",""
"Journal article","Wang C,Fu Y,Yan J,Wu X,Zhang Y,Xia H,Yuan Y","","A cost‐efficient resemblance detection scheme for post‐deduplication delta compression in backup systems","Concurr. Comput.","Concurrency and computation: practice & experience","","","","","","2022","34","3","","All","","","","Wiley","","","","","2022-02","","","","","1532-0626","1532-0634","https://onlinelibrary.wiley.com/doi/10.1002/cpe.6558;http://dx.doi.org/10.1002/cpe.6558;https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6558","10.1002/cpe.6558","","","","Summary Delta compression, which is efficient in removing repeated string among similar chunks, can be used as a complement to data deduplication in backup storage for extra space savings. The process of detecting similar candidates to use as the base for delta compression is called resemblance detection. Several indexes are required for resemblance detection. Maintaining them in RAM would limit the system scalability and increase system cost. Storing them on the disk suffers from low throughput due to poor random I/O performance of the disk. In this article, we present the history-aware resemblance detection (HARD), a cost-efficient resemblance detection approach that captures most of the similar chunks with a limited memory footprint. HARD is based on the observation that, for chunks in a backup, most of their similar chunks can be found in the most recent backups. HARD thus only indexes super-features in the most recent backups for resemblance detection to reduce the memory footprint of resemblance indexes while captures most of the potential similar chunks for delta compression. Experimental results based on three real-world datasets show that HARD achieves higher compression than the state-of-the-art approach.","","","http://onlinelibrary.wiley.com/termsAndConditions#vor","School of Computer Science Hubei University of Technology Wuhan China; Hunan Provincial Maternal and Child Health Care Hospital Changsha China","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Douglis F,Iyengar A","","Application-specific Delta-encoding via Resemblance Detection","","","","","USENIX annual technical conference, general track","","","2003","","","113-126","All","Similarity/Resemblance","San Antonio, TX, USA","","","","","","","2003","","","","","","","https://www.usenix.org/events/usenix03/tech/full_papers/douglis/douglis_html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Douglis and Iyengar 2003 - Application-specific Delta-encoding via Resemblance Detection.pdf","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore.ieee.org/document/9166692","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Long S,Li Z,Liu Z,Deng Q,Oh S,Komuro N","","A similarity clustering-based deduplication strategy in cloud storage systems","","","","","2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)","","","2020","","","35-43","All;FSL Traces;Thesis","Similarity/Resemblance","","","","","","","","2020-12","","","","","2690-5965","","http://dx.doi.org/10.1109/ICPADS51040.2020.00015;https://ieeexplore.ieee.org/abstract/document/9359136/","10.1109/ICPADS51040.2020.00015","","","","Deduplication is a data redundancy elimination technique, designed to save system storage resources by reducing redundant data in cloud storage systems. With the development of cloud computing technology, deduplication has been increasingly applied to cloud data centers. However, traditional technologies face great challenges in big data deduplication to properly weigh the two conflicting goals of deduplication throughput and high duplicate elimination ratio. This paper proposes a similarity clustering-based deduplication strategy (named SCDS), which aims to delete more duplicate data without significantly increasing system overhead. The main idea of SCDS is to narrow the query range of fingerprint index by data partitioning and similarity clustering algorithms. In the data preprocessing stage, SCDS uses data partitioning algorithm to classify similar data together. In the data deletion stage, the similarity clustering algorithm is used to divide the similar data fingerprint superblock into the same cluster. Repetitive fingerprints are detected in the same cluster to speed up the retrieval of duplicate fingerprints. Experiments show that the deduplication ratio of SCDS is better than some existing similarity deduplication algorithms, but the overhead is only slightly higher than some high throughput but low deduplication ratio methods.","Cloud computing;Clustering algorithms;Estimation;Fingerprint recognition;Throughput;Partitioning algorithms;Classification algorithms;deduplication;cloud storage system;similarity clustering;data partitioning;block fingerprint","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Long et al. 2020 - A similarity clustering-based deduplication strategy in cloud storage systems.pdf","","","",""
"Journal article","Godavari A,Sudhakar C,Ramesh T","","Hybrid Deduplication System—A Block-Level Similarity-Based Approach","IEEE Syst. J.","IEEE Systems Journal","","","","","","2021","15","3","3860-3870","All","Similarity/Resemblance","","","","","","","","2021-09","","","","","1937-9234","","http://dx.doi.org/10.1109/JSYST.2020.3012702;https://ieeexplore.ieee.org/abstract/document/9166692/","10.1109/JSYST.2020.3012702","","","","Deploying deduplication for primary storage is a challenging task in view of random access patterns of I/O requests and the requirement of quick response time. Existing deduplication approaches designed for primary workloads are locality based with the assumption that I/O requests follow the locality principle. However, primary workloads in cloud systems need not necessarily follow the locality principle, and hence, the existing methods for deduplication are likely to exhibit poor response time. To provide a solution to this challenging problem, we propose and implement a hybrid deduplication system (HDS), a block-based partial deduplication system with similarity-based indexing. The proposed system applies deduplication in the background to decrease the latency and also aims at decreasing the data fragmentation. It applies similarity-based indexing to reduce high number of metadata lookups arising out of random access patterns of the requests. HDS for primary workloads is simulated in the Linux environment using three different types of FIU traces, and the effectiveness of the system is compared with full deduplication based on the parameters-metadata access overhead, average segment length, and response time. The experimental results show that the system has performed consistently better in reducing the metadata overhead and increasing the average segment length for all three sets of I/O trace data.","Indexing;Metadata;Time factors;Cloud computing;Redundancy;Optimization;Data fragmentation;deduplication;disk bottleneck;similarity-based indexing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Godavari et al. 2021 - Hybrid Deduplication System—A Block-Level Similarity-Based Approach.pdf","","","",""
"Journal article","Rasina Begum B,Chitra P","","SEEDDUP: A Three-Tier SEcurE Data DedUPlication Architecture-Based Storage and Retrieval for Cross-Domains Over Cloud","IETE J. Res.","IETE journal of research","","","","","","2023","69","4","2224-2241","All","","","","Taylor & Francis","","","","","2023-05-19","","","","","0377-2063","","https://doi.org/10.1080/03772063.2021.1886882;http://dx.doi.org/10.1080/03772063.2021.1886882;https://www.tandfonline.com/doi/abs/10.1080/03772063.2021.1886882","10.1080/03772063.2021.1886882","","","","Data deduplication has become a promising technique that significantly reduces space utilization of cloud storage servers and optimizes the network bandwidth. A variety of deduplication techniques have been proposed to enhance the cloud server?s storage efficiency. Most of the deduplication schemes do not address global indexing and user privacy issues. This paper elucidates a novel three-tier secure data deduplication architecture (referred to as SEEDDUP) for data storage and retrieval in the cloud. SEEDDUP provides data deduplication with proper indexing and user privacy. In this work, the optimal chunk size, for a given file size and format, is calculated using Cuckoo Search Algorithm. The chunk existence in the cloud is verified by the hash value generated via SHA3-512. To improve data deduplication speed, Merkle Hash Tree based MapReduce (MHTMR) is used for fast index lookup, which improves the performance at data storage and retrieval. In addition, data are encrypted based on its sensitivity level (low and high) using Niederreiter Public Key Cryptosystem. With this new SEEDDUP architecture, only legitimate users can request data successfully for limited times (k) and spurious data requests cannot be processed or handled in the cloud. Experimental results show that the proposed SEEDDUP can reduce latency, achieved good throughput and deduplication efficiency (chunk size variance and security measures) in data deduplication.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Shynu,Nadesh,Menon VG,Venu,Abbasi M,Khosravi MR","","A secure data deduplication system for integrated cloud-edge networks","J. Cloud Comput. Adv. Syst. Appl.","Journal of Cloud Computing Advances Systems and Applications","","","","","","2020","9","1","","All","","","","Springer Science and Business Media LLC","","","","","2020-12","","","","","2192-113X","","https://journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-020-00214-6;http://dx.doi.org/10.1186/s13677-020-00214-6","10.1186/s13677-020-00214-6","","","","AbstractData redundancy is a significant issue that wastes plenty of storage space in the cloud-fog storage integrated environments. Most of the current techniques, which mainly center around the static scenes, for example, the backup and archive systems, are not appropriate because of the dynamic nature of data in the cloud or integrated cloud environments. This problem can be effectively reduced and successfully managed by data deduplication techniques, eliminating duplicate data in cloud storage systems. Implementation of data deduplication (DD) over encrypted data is always a significant challenge in an integrated cloud-fog storage and computing environment to optimize the storage efficiently in a highly secured manner. This paper develops a new method using Convergent and Modified Elliptic Curve Cryptography (MECC) algorithms over the cloud and fog environment to construct secure deduplication systems. The proposed method focuses on the two most important goals of such systems. On one side, the redundancy of data needs to be reduced to its minimum, and on the other hand, a robust encryption approach must be developed to ensure the security of the data. The proposed technique is well suited for operations such as uploading new files by a user to the fog or cloud storage. The file is first encrypted using the Convergent Encryption (CE) technique and then re-encrypted using the Modified Elliptic Curve Cryptography (MECC) algorithm. The proposed method can recognize data redundancy at the block level, reducing the redundancy of data more effectively. Testing results show that the proposed approach can outperform a few state-of-the-art methods of computational efficiency and security levels.","","","https://creativecommons.org/licenses/by/4.0","","en","","","","","","","","","","","","","","","","","","","","All Papers/S/Shynu et al. 2020 - A secure data deduplication system for integrated cloud-edge networks.pdf","","","",""
"Journal article","","","ycsb-overview.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://helenchw.github.io/files/ycsb-overview.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/ycsb-overview.pdf - ycsb-overview.pdf","","","",""
"Journal article","Zhang P,Huang P,He X,Wang H,Zhou K","","Resemblance and mergence based indexing for high performance data deduplication","J. Syst. Softw.","The Journal of systems and software","","","","","","2017","128","","11-24","All;FSL Traces;Thesis;p-scailbib","Similarity/Resemblance","","","Elsevier","","","","","2017-06-01","","","","","0164-1212","","https://www.sciencedirect.com/science/article/pii/S0164121217300389;http://dx.doi.org/10.1016/j.jss.2017.02.039;https://www.sciencedirect.com/science/article/am/pii/S0164121217300389","10.1016/j.jss.2017.02.039","","","","Data deduplication, a data redundancy elimination technique, has been widely employed in many application environments to reduce data storage space. However, it is challenging to provide a fast and scalable key-value fingerprint index particularly for large datasets, while the index performance is critical to the overall deduplication performance. This paper proposes RMD, a resemblance and mergence based deduplication scheme, which aims to provide quick responses to fingerprint queries. The key idea of RMD is to leverage a bloom filter array and a data resemblance algorithm to dramatically reduce the query range. At data ingesting time, RMD uses a resemblance algorithm to detect resemble data segments and put resemblance segments in the same bin. As a result, at querying time, it only needs to search in the corresponding bin to detect duplicate content, which significantly speeds up the query process. Moreover, RMD uses a mergence strategy to accumulate resemblance segments to relevant bins, and exploits frequency-based fingerprint retention policy to cap the bin capacity to improve query throughput and data deduplication ratio. Extensive experimental results with real-world datasets have shown that RMD is able to achieve high query performance and outperforms several well-known deduplication schemes.","Fast index; Deduplication; Resemblance mergence; Fingerprint retrieval; Key value index","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2017 - Resemblance and mergence based indexing for high performance data deduplication.pdf","","","",""
"Conference paper","Cooper BF,Silberstein A,Tam E,Ramakrishnan R,Sears R","","Benchmarking cloud serving systems with YCSB","","","","","Proceedings of the 1st ACM symposium on Cloud computing","","","2010","","","143-154","All","","","","Association for Computing Machinery","New York, NY, USA","","Indianapolis, Indiana, USA","","2010-06-10","","2023-04-25","9781450300360","","","","https://doi.org/10.1145/1807128.1807152;http://dx.doi.org/10.1145/1807128.1807152;https://dl.acm.org/doi/abs/10.1145/1807128.1807152;https://people.cs.pitt.edu/~chang/231/y13/papers/benchmarkcloud.pdf","10.1145/1807128.1807152","","","","While the use of MapReduce systems (such as Hadoop) for large scale data analysis has been widely recognized and studied, we have recently seen an explosion in the number of systems developed for cloud data serving. These newer systems address ""cloud OLTP"" applications, though they typically do not support ACID transactions. Examples of systems proposed for cloud serving use include BigTable, PNUTS, Cassandra, HBase, Azure, CouchDB, SimpleDB, Voldemort, and many others. Further, they are being applied to a diverse range of applications that differ considerably from traditional (e.g., TPC-C like) serving workloads. The number of emerging cloud serving systems and the wide range of proposed applications, coupled with a lack of apples-to-apples performance comparisons, makes it difficult to understand the tradeoffs between systems and the workloads for which they are suited. We present the ""Yahoo! Cloud Serving Benchmark"" (YCSB) framework, with the goal of facilitating performance comparisons of the new generation of cloud data serving systems. We define a core set of benchmarks and report results for four widely used systems: Cassandra, HBase, Yahoo!'s PNUTS, and a simple sharded MySQL implementation. We also hope to foster the development of additional cloud benchmark suites that represent other classes of applications by making our benchmark tool available via open source. In this regard, a key feature of the YCSB framework/tool is that it is extensible--it supports easy definition of new workloads, in addition to making it easy to benchmark new systems.","benchmarking, cloud serving database","","","","","","SoCC '10","","","","","","","","","","","","","","","","","","All Papers/C/Cooper et al. 2010 - Benchmarking cloud serving systems with YCSB.pdf","","","",""
"Conference paper","Zhang P,Zhou K,Wang H","","LERD, a locality enhanced and resemblance based deduplication scheme for large data sets","","","","","2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","","","2015","","","1-4","All","Similarity/Resemblance","","","","","","","","2015-09","","","","","","","http://dx.doi.org/10.1109/ICSPCC.2015.7338962;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7338962","10.1109/ICSPCC.2015.7338962","","","","As one kind of storage technology, deduplicaition is widely deployed in all kinds of storage systems. However, the key problems of duplication, such as data throughput and usage of RAM, have not been perfectly addressed. Especially, with the emergence of cloud storage, traditional deduplication methods are not able to adapt to the velocity characteristic of the large data sets. This paper proposes LERD, a temporal locality enhanced resemblance based Duplication scheme, aiming at rapidly querying duplicated data for large scale data sets. LERD takes advantage of data resemblance and temporal locality of data stream to narrow query range, which not only rise throughput, but also decline usage of RAM. Theoretical analysis and experimental results show that LERD's performance is much better than other state-of-the-art schemes.","Throughput;Fingerprint recognition;Indexes;Random access memory;Radio frequency;Bars;Electronic mail","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2015 - LERD, a locality enhanced and resemblance based deduplication scheme for large data sets.pdf","","","",""
"Journal article","Geer D","","Reducing the Storage Burden via Data Deduplication","Computer ","Computer","","","","","","2008","41","12","15-17","All","","","","","","","","","2008-12","","","","","0018-9162","1558-0814","http://dx.doi.org/10.1109/MC.2008.538","10.1109/MC.2008.538","","","","Companies are collecting and storing huge amounts of data, much of it redundant. Many organizations are turning to data deduplication to reduce these huge information volumes, as well as the equipment and operational costs they entail.","Energy storage;Material storage;Government;Memory;Companies;Energy management;Information management;Resource management;Costs;Data engineering;data deduplication;data storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Geer 2008 - Reducing the Storage Burden via Data Deduplication.pdf","","","",""
"Conference paper","Mohana Chelvan P,Perumal K","","A comparative analysis of feature selection stability measures","","","","","2017 International Conference on Trends in Electronics and Informatics (ICEI)","","","2017","","","","All","","","","IEEE","","2017 International Conference on Trends in Electronics and Informatics (ICOEI)","Tirunelveli","2017/5/11-2017/5/12","2017-05","","","9781509042579","","","","https://www.researchgate.net/publication/261286019;https://en.wikipedia.org/wiki/Rabin-;http://ieeexplore.ieee.org/document/8300901/;http://dx.doi.org/10.1109/icoei.2017.8300901","10.1109/icoei.2017.8300901","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Mohana Chelvan and Perumal 2017 - A comparative analysis of feature selection stability measures.pdf","","","",""
"Conference paper","Drago I,Mellia M,M. Munafo M,Sperotto A,Sadre R,Pras A","","Inside dropbox","","","","","Proceedings of the 2012 Internet Measurement Conference","","","2012","","","","All","","","","ACM","New York, NY, USA","IMC '12: Internet Measurement Conference","Boston Massachusetts USA","2012/11/14-2012/11/16","2012-11-14","","","9781450317054","","","","http://dx.doi.org/10.1145/2398776.2398827;https://research.utwente.nl/en/publications/inside-dropbox-understanding-personal-cloud-storage-services;https://dl.acm.org/doi/10.1145/2398776.2398827","10.1145/2398776.2398827","","","","","","","","University of Twente, Enschede, Netherlands; Politecnico di Torino, Torino, Italy","","","","","","","","","","","","","","","","","","","","","All Papers/D/Drago et al. 2012 - Inside dropbox.pdf","","","",""
"Journal article","Yu CM","","Counteracting Side Channels in Cross-user Client-side Deduplicated Cloud Storage","IEEE Internet of Things Journal","","","","","","","2023","","","1-1","All","","","","","","","","","2023","","","","","2327-4662","","http://dx.doi.org/10.1109/JIOT.2023.3264793;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10092777","10.1109/JIOT.2023.3264793","","","","Client-side data deduplication enables cloud storage services to reduce storage-space and bandwidth consumption, resulting in reduced operating cost and a high level of user satisfaction. However, duplicate checks (i.e., the corresponding message exchange) generate a side channel, exposing the file existence status to attackers. In particular, the binary response from a duplicate check reveals file existence information. This can be exploited to launch further attacks, such as learning sensitive file content and establishing a covert channel. As current solutions provide only weaker privacy or rely on unreasonable assumptions, we propose RAndom REsponse (RARE) to achieve stronger privacy. The underlying principle is that the uploading user simultaneously sends a duplication request for two chunks. The cloud receiving the request returns a carefully designed randomized duplication response to preserve the deduplication gain and minimize privacy leakage. By proposing multiple chunk uploading and chunk re-arrangement strategies, we optimize the design of RARE to significantly reduce the communication burden without compromising privacy. We also study the impact of different implementations of multiple chunk uploading on the deduplication gain, further reducing communication cost. The analytical results confirm that formal privacy is ensured, whereas experiment results demonstrate that RARE preserves both privacy and the deduplication benefit.","Cloud computing;Privacy;Data privacy;Servers;Security;Costs;Passwords;Cloud Storage;Side Channel;Data Deduplication;Privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu 2023 - Counteracting Side Channels in Cross-user Client-side Deduplicated Cloud Storage.pdf","","","",""
"Journal article","Vrable M,Savage S,Voelker GM","","Cumulus: Filesystem backup to the cloud","ACM Trans. Storage","","","","","","","2009","5","4","1-28","All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2009-12-14","","","","","1553-3077","","https://doi.org/10.1145/1629080.1629084;http://dx.doi.org/10.1145/1629080.1629084;https://dl.acm.org/doi/pdf/10.1145/1629080.1629084;https://dl-acm-org.ezproxy.lib.bbk.ac.uk/doi/pdf/10.1145/1629080.1629084","10.1145/1629080.1629084","","","","Cumulus is a system for efficiently implementing filesystem backups over the Internet, specifically designed under a thin cloud assumption—that the remote datacenter storing the backups does not provide any special backup services, but only a least-common-denominator storage interface. Cumulus aggregates data from small files for storage and uses LFS-inspired segment cleaning to maintain storage efficiency. While Cumulus can use virtually any storage service, we show its efficiency is comparable to integrated approaches.","Backup, cloud storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Vrable et al. 2009 - Cumulus - Filesystem backup to the cloud.pdf","","","14","December 2009"
"Conference paper","Kwon M,Zhang J,Park G,Choi W,Donofrio D,Shalf J,Kandemir M,Jung M","","TraceTracker: Hardware/software co-evaluation for large-scale I/O workload reconstruction","","","","","2017 IEEE International Symposium on Workload Characterization (IISWC)","","","2017","","","87-96","All","","","","","","","","","2017-10","","","","","","","http://dx.doi.org/10.1109/IISWC.2017.8167759;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8167759","10.1109/IISWC.2017.8167759","","","","Block traces are widely used for system studies, model verifications, and design analyses in both industry and academia. While such traces include detailed block access patterns, existing trace-driven research unfortunately often fails to find true-north due to a lack of runtime contexts such as user idle periods and system delays, which are fundamentally linked to the characteristics of target storage hardware. In this work, we propose TraceTracker, a novel hardware/software co-evaluation method that allows users to reuse a broad range of the existing block traces by keeping most their execution contexts and user scenarios while adjusting them with new system information. Specifically, our TraceTracker's software evaluation model can infer CPU burst times and user idle periods from old storage traces, whereas its hardware evaluation method remasters the storage traces by interoperating the inferred time information, and updates all inter-arrival times by making them aware of the target storage system. We apply the proposed co-evaluation model to 577 traces, which were collected by servers from different institutions and locations a decade ago, and revive the traces on a high-performance flash-based storage array. The evaluation results reveal that the accuracy of the execution contexts reconstructed by TraceTracker is on average 99% and 96% with regard to the frequency of idle operations and the total idle periods, respectively.","Acceleration;Delays;Servers;Hardware;Software;Analytical models","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kwon et al. 2017 - TraceTracker - Hardware - software co-evaluation for large-scale I - O workload reconstruction.pdf","","","",""
"Website","Kotlarska I,Jackowski A,Lichota K,Welnicki M,Dubnicki C","","InftyDedup: Scalable and Cost-Effective Cloud Tiering with Deduplication","","","","","","","","","","","","All","Accelerate","","","","","","","","","","2023-03-28","","","","","https://www.usenix.org/system/files/fast23-kotlarska.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kotlarska et al. - InftyDedup - Scalable and Cost-Effective Cloud Tiering with Deduplication.pdf","","","",""
"Conference paper","Fu Y,Jiang H,Xiao N,Tian L,Liu F","","AA-Dedupe: An Application-Aware Source Deduplication Approach for Cloud Backup Services in the Personal Computing Environment","","","","","2011 IEEE International Conference on Cluster Computing","","","2011","","","112-120","All","","","","","","","","","2011-09","","","","","2168-9253","","http://dx.doi.org/10.1109/CLUSTER.2011.20;https://ieeexplore.ieee.org/abstract/document/6061046/;https://www.researchgate.net/profile/Yinjin-Fu/publication/221201945_AA-Dedupe_An_Application-Aware_Source_Deduplication_Approach_for_Cloud_Backup_Services_in_the_Personal_Computing_Environment/links/0deec51afcbccf1fe4000000/AA-Dedupe-An-Application-Aware-Source-Deduplication-Approach-for-Cloud-Backup-Services-in-the-Personal-Computing-Environment.pdf","10.1109/CLUSTER.2011.20","","","","The market for cloud backup services in the personal computing environment is growing due to large volumes of valuable personal and corporate data being stored on desktops, laptops and smart phones. Source deduplication has become a mainstay of cloud backup that saves network bandwidth and reduces storage space. However, there are two challenges facing deduplication for cloud backup service clients: (1) low deduplication efficiency due to a combination of the resource-intensive nature of deduplication and the limited system resources on the PC-based client site, and (2) low data transfer efficiency since post-deduplication data transfers from source to backup servers are typically very small but must often cross a WAN. In this paper, we present AA-Dedupe, an application-aware source deduplication scheme, to significantly reduce the computational overhead, increase the deduplication throughput and improve the data transfer efficiency. The AA-Dedupe approach is motivated by our key observations of the substantial differences among applications in data redundancy and deduplication characteristics, and thus is based on an application-aware index structure that effectively exploits this application awareness. Our experimental evaluations, based on an AA-Dedupe prototype implementation, show that our scheme can improve deduplication efficiency over the state-of-art source-deduplication methods by a factor of 2-7, resulting in shortened backup window, increased power-efficiency and reduced cost for cloud backup services.","Indexes;Redundancy;Cloud computing;Fingerprint recognition;Throughput;Bandwidth;Wide area networks;Cloud backup;source deduplication;intelligent data chunking;application-aware index;deduplication efficiency","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2011 - AA-Dedupe - An Application-Aware Source Deduplication Approach for Cloud Backup Services in the Personal Computing Environment.pdf","","","",""
"Journal article","Fu Y,Jiang H,Xiao N,Tian L,Liu F,Xu L","","Application-Aware Local-Global Source Deduplication for Cloud Backup Services of Personal Storage","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2014","25","5","1155-1165","All","","","","","","","","","2014-05","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2013.167;https://ieeexplore.ieee.org/abstract/document/6550860/","10.1109/TPDS.2013.167","","","","In personal computing devices that rely on a cloud storage environment for data backup, an imminent challenge facing source deduplication for cloud backup services is the low deduplication efficiency due to a combination of the resource-intensive nature of deduplication and the limited system resources. In this paper, we present ALG-Dedupe, an Application-aware Local-Global source deduplication scheme that improves data deduplication efficiency by exploiting application awareness, and further combines local and global duplicate detection to strike a good balance between cloud storage capacity saving and deduplication time reduction. We perform experiments via prototype implementation to demonstrate that our scheme can significantly improve deduplication efficiency over the state-of-the-art methods with low system overhead, resulting in shortened backup window, increased power efficiency and reduced cost for cloud backup services of personal storage.","Redundancy;Cloud computing;Fingerprint recognition;Indexes;Image coding;Throughput;Bandwidth;Cloud backup;personal storage;source deduplication;deduplication efficiency;application awareness","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2014 - Application-Aware Local-Global Source Deduplication for Cloud Backup Services of Personal Storage.pdf","","","",""
"Conference paper","Sehat H,Pagnin E,Lucani DE","","Yggdrasil: Privacy-Aware Dual Deduplication in Multi Client Settings","","","","","ICC 2021 - IEEE International Conference on Communications","","","2021","","","1-6","All","","","","IEEE","","","","","2021-06","","","9781728171227","9781728171234","1938-1883","1550-3607","http://dx.doi.org/10.1109/ICC42927.2021.9500816;https://ieeexplore.ieee.org/abstract/document/9500816/;https://arxiv.org/pdf/2007.11403;https://arxiv.org/pdf/2007.11403?t=","10.1109/ICC42927.2021.9500816","","","","This paper proposes Yggdrasil, a protocol for privacy-aware dual data deduplication in multi-client settings. Yggdrasil is designed to reduce cloud storage space while safeguarding the privacy of clients’ data. This is achieved by exploiting a ‘dual’ setting, where both the cloud and the clients store a fraction of the data. Yggdrasil combines two innovative techniques to achieve this goal. First, generalized deduplication, an emerging solution to reduce data footprint; second, non- deterministic lightweight transformations that ensure a high level of privacy while improving the degree of cross-user data compression in the cloud. Our client preprocessing guarantees that an honest-but-curious cloud storage provider faces a high degree of uncertainty in determining the original clients’ data. We introduce an uncertainty metric to measure the privacy of the client’s outsourced data and three compression metrics to investigate the performance of Yggdrasil. Our experiments with a dataset of DVI files show that Yggdrasil achieves an overall compression rate of 43%, which means that Yggdrasil can represent the same database using less than half of the original space. Moreover, for the same experiment clients only store 17% of the original data, the cloud hosts the remaining 26%, and the client preprocessing ensures each outsourced fragment has 10 293 possible original strings. Higher uncertainty is possible, but reduces the cloud’s compression capability.","Cloud computing;Data privacy;Uncertainty;Protocols;Databases;Conferences;Measurement uncertainty;Data Compression;Data Privacy;Deduplication;Generalized Deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sehat et al. 2021 - Yggdrasil - Privacy-Aware Dual Deduplication in Multi Client Settings.pdf","","","",""
"Preprint","Sehat H,Pagnin E,Lucani DE","","Bifrost: Secure, Scalable and Efficient File Sharing System Using Dual Deduplication","","","","","","arXiv [cs.CR]","","2022","","","","All","","","","","","","","","2022-01-26","","","","","","","http://arxiv.org/abs/2201.10839","","","2201.10839","","We consider the problem of sharing sensitive or valuable files across users while partially relying on a common, untrusted third-party, e.g., a Cloud Storage Provider (CSP). Although users can rely on a secure peer-to-peer (P2P) channel for file sharing, this introduces potential delay on the data transfer and requires the sender to remain active and connected while the transfer process occurs. Instead of using the P2P channel for the entire file, users can upload information about the file on a common CSP and share only the essential information that enables the receiver to download and recover the original file. This paper introduces Bifrost, an innovative file sharing system inspired by recent results on dual deduplication. Bifrost achieves the desired functionality and simultaneously guarantees that (1) the CSP can efficiently compress outsourced data; (2) the secure P2P channel is used only to transmit short, but crucial information; (3) users can check for data integrity, i.e., detect if the CSP alters the outsourced data; and (4) only the sender (data owner) and the intended receiver can access the file after sharing, i.e., the cloud or no malicious adversary can infer useful information about the shared file. We analyze compression and bandwidth performance using a proof-of-concept implementation. Our experiments show that secure file sharing can be achieved by sending only 650 bits on the P2P channel, irrespective of file size, while the CSP that aids the sharing can enjoy a compression rate of 86.9 %.","","","","","","","","","arXiv","2201.10839","cs.CR","","","","","","","","","","","","","","All Papers/S/Sehat et al. 2022 - Bifrost - Secure, Scalable and Efficient File Sharing System Using Dual Deduplication.pdf","","","",""
"Conference paper","Vestergaard R,Zhang Q,Lucani DE","","Generalized Deduplication: Bounds, Convergence, and Asymptotic Properties","","","","","2019 IEEE Global Communications Conference (GLOBECOM)","","","2019","","","1-6","All","","","","","","","","","2019-12","","","","","2576-6813","","http://dx.doi.org/10.1109/GLOBECOM38437.2019.9014012;https://ieeexplore.ieee.org/abstract/document/9014012/;https://arxiv.org/pdf/1901.02720","10.1109/GLOBECOM38437.2019.9014012","","","","We study a generalization of deduplication, which enables lossless deduplication of highly similar data and show that classic deduplication with fixed chunk length is a special case. We provide bounds on the expected length of coded sequences for generalized deduplication and show that the coding has asymptotic near-entropy cost under the proposed source model. More importantly, we show that generalized deduplication allows for multiple orders of magnitude faster convergence than classic deduplication. This means that generalized deduplication can provide compression benefits much earlier than classic deduplication, which is key in practical systems. Numerical examples demonstrate our results, showing that our lower bounds are achievable, and illustrating the potential gain of using the generalization over classic deduplication. In fact, we show that even for a simple case of generalized deduplication, the gain in convergence speed is linear with the size of the data chunks.","Dictionaries;Decoding;Convergence;Source coding;Zinc;Internet of Things","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Vestergaard et al. 2019 - Generalized Deduplication - Bounds, Convergence, and Asymptotic Properties.pdf","","","",""
"Journal article","Jeyaraj JR,Kambaraj S,Dharmarajan V","","High-speed data deduplication using parallelized cuckoo hashing","Turkish Journal of Electrical Engineering and Computer Sciences","Turkish Journal of Electrical Engineering and Computer Sciences","","","","","","2018","26","3","1417-1429","Parallel Schemes;All","","","","","","","","","2018","","2023-03-26","","","1300-0632","","https://journals.tubitak.gov.tr/elektrik/vol26/iss3/24/;http://dx.doi.org/10.3906/elk-1708-336;https://journals.tubitak.gov.tr/cgi/viewcontent.cgi?article=1944&context=elektrik","10.3906/elk-1708-336","","","","Data deduplication is a capacity optimization technology used in backup systems for identifying and storing the nonredundant data blocks. The CPU intensive tasks involved in a hash-based deduplication system remain as challenges in improving the performance of the system. In this paper, we propose a parallel variant of the standard cuckoo hashing that enables the hashing technique to be performed in parallel. The CPU intensive tasks of fingerprint insertion and lookup operations are performed in parallel and distributed among the nodes of the deduplication cluster. Furthermore, the uniform handling of the blocks by the cluster nodes involved in the process of duplicate identification provides good load balance. Experimental evaluations using real-world backup and Linux kernel data sets reveal that the proposed deduplication system achieves up to 100{\%} higher backup speed, up to 28{\%} reduced lookup latency, and up to 24{\%} reduced backup time than the other deduplication systems.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jeyaraj et al. 2018 - High-speed data deduplication using parallelized cuckoo hashing.pdf","","","",""
"Journal article","Wu X,Wang H,Yuan Y,Li F","","A lightweight encrypted deduplication scheme supporting backup","Int. J. High Perform. Syst. Archit.","International Journal of High Performance Systems Architecture","","","","","","2023","138","","102858","All","","","","","","","","","2023-05-01","","","","","1383-7621","","https://www.sciencedirect.com/science/article/pii/S1383762123000371;http://dx.doi.org/10.1016/j.sysarc.2023.102858","10.1016/j.sysarc.2023.102858","","","","Data deduplication technology is frequently used by many cloud service providers to handle the exponential growth in data. To protect their data privacy, users frequently encrypt their data before uploading it to a cloud storage provider. Therefore, efficient encrypted deduplication technology has been widely studied. Although cloud storage provides convenience for users, it also has the problem of high data concentration. Once an accident occurs in the data center, the impact is enormous. Therefore, it is necessary to back up the ciphertext data after deduplication. In this paper, we propose an efficient encrypted data deduplication technology that supports backup. Our scheme uses a one-way hash chain to verify the legitimacy of a user’s identity. It updates the user’s ownership of the data using the user’s index table. In this paper, we use the XOR operation to re-encrypt data to obtain backup ciphertext, reducing computational complexity. In LEDB, the user can extract the key from the data label. Therefore, the user does not need to manage the key separately. In addition, LEDB supports a two-level deduplication mechanism, which can not only ensure transmission security but also reduce network transmission bandwidth. Finally, LEDB allows the user to determine the integrity of the data before decrypting the ciphertext. Security analysis and experimental results show that our scheme is secure and efficient for data encryption and decryption.","Ciphertext backup; Convergent encryption; Ownership modification; User revocation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2023 - A lightweight encrypted deduplication scheme supporting backup.pdf","","","",""
"Conference paper","Santos W,Teixeira T,Machado C,Meira Jr W,Ferreira R,Guedes D,Da Silva AS","","A Scalable Parallel Deduplication Algorithm","","","","","19th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'07)","","","2007","","","79-86","All","","","","","","","","","2007-10","","","","","1550-6533","","http://dx.doi.org/10.1109/SBAC-PAD.2007.32;https://ieeexplore.ieee.org/abstract/document/4384045/;https://www.academia.edu/download/48460049/A_Scalable_Parallel_Deduplication_Algori20160831-13737-d9zht3.pdf","10.1109/SBAC-PAD.2007.32","","","","The identification of replicas in a database is fundamental to improve the quality of the information. Deduplication is the task of identifying replicas in a database that refer to the same real world entity. This process is not always trivial, because data may be corrupted during their gathering, storing or even manipulation. Problems such as misspelled names, data truncation, data input in a wrong format, lack of conventions (like how to abbreviate a name), missing data or even fraud may lead to the insertion of replicas in a database. The deduplication process may be very hard, if not impossible, to be performed manually, since actual databases may have hundreds of millions of records. In this paper, we present our parallel deduplication algorithm, called FER- APARDA. By using probabilistic record linkage, we were able to successfully detect replicas in synthetic datasets with more than 1 million records in about 7 minutes using a 20- computer cluster, achieving an almost linear speedup. We believe that our results do not have similar in the literature when it comes to the size of the data set and the processing time.","Couplings;Computer science;Scalability;Computer architecture;High performance computing;Demography;Clustering algorithms;Deductive databases;Software libraries;Erbium","This is database record (multifield) deduplication and is not applicable to data deduplication.","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Santos et al. 2007 - A Scalable Parallel Deduplication Algorithm.pdf","","","",""
"Conference paper","Hao S,Du Z,Bader D,Ye Y","","A Partition-Merge Based Cache-Conscious Parallel Sorting Algorithm for CMP with Shared Cache","","","","","2009 International Conference on Parallel Processing","","","2009","","","396-403","All","","","","","","","","","2009-09","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPP.2009.26;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5362416","10.1109/ICPP.2009.26","","","","To explore chip-level parallelism, the PSC (Parallel Shared Cache) model is provided in this paper to describe high performance shared cache of Chip Multi-Processors (CMP). Then for a specific application, parallel sorting, a cache-conscious parallel algorithm, PMCC (Partition-Merge based Cache-Conscious) is designed based on the PSC model. The PMCC algorithm consists of two steps: the partition-based in-cache sorting and merge-based k-way merge sorting. In the first stage, PMCC first divides the input dataset into multiple blocks so that each block can fit into the shared L2 cache, and then employs multiple cores to perform parallel cache sorting to generate sorted blocks. In the second stage, PMCC first selects an optimized parameter k which can not only improve the parallelism but also reduce the cache missing rate, then performs a k-way merge sorting to merge all the sorted blocks. The I/O complexity of the in-cache sorting step and k-way merge step are analyzed in detail. The simulation results show that the PSC based PMCC algorithm can out-performance the latest PEM based cache-conscious algorithm and the scalability of PMCC is also discussed. The low I/O complexity, high parallelism and the high scalability of PMCC can take advantage of CMP to improve its performance significantly and deal with large scale problem efficiently.","Sorting;Partitioning algorithms;Parallel processing;Algorithm design and analysis;Parallel algorithms;Scalability;Computer architecture;Information science;Computer science;Concurrent computing;Parallel Sorting;Cache-conscious Algorithm;Chip Multi-Processors (CMP)","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hao et al. 2009 - A Partition-Merge Based Cache-Conscious Parallel Sorting Algorithm for CMP with Shared Cache.pdf","","","",""
"Conference paper","Graefe G","","Sort-merge-join: an idea whose time has(h) passed?","","","","","Proceedings of 1994 IEEE 10th International Conference on Data Engineering","","","1994","","","406-417","All","","","","","","","","","1994-02","","","","","","","http://dx.doi.org/10.1109/ICDE.1994.283062;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=283062","10.1109/ICDE.1994.283062","","","","Matching two sets of data items is a fundamental operation required in relational, extensible, and object-oriented database systems alike. However, the pros and cons of sort- and hash-based query evaluation techniques in modern query processing systems are still not fully understood. After our earlier research clarified strengths and weaknesses of sort- and hash-based query processing techniques and suggested remedies for the shortcomings of hash-based algorithms, the present paper outlines a number of further differences between sort-merge-join and hybrid hash join that traditionally have been ignored in such comparisons and render sort-merge-join mostly obsolete. We consolidate old and raise new issues pertinent to the comparison of sort- and hash-based query evaluation techniques and stir some thought and discussion among both academic and industrial database system builders.","Query processing;Database systems;Performance analysis;Algorithm design and analysis;Sorting;Relational databases","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Graefe 1994 - Sort-merge-join - an idea whose time has(h) passed.pdf","","","",""
"Conference paper","Davidson A,Tarjan D,Garland M,Owens JD","","Efficient parallel merge sort for fixed and variable length keys","","","","","2012 Innovative Parallel Computing (InPar)","","","2012","","","1-9","All","","","","","","","","","2012-05","","","","","","","http://dx.doi.org/10.1109/InPar.2012.6339592;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6339592","10.1109/InPar.2012.6339592","","","","We design a high-performance parallel merge sort for highly parallel systems. Our merge sort is designed to use more register communication (not shared memory), and does not suffer from over-segmentation as opposed to previous comparison based sorts. Using these techniques we are able to achieve a sorting rate of 250 MKeys/sec, which is about 2.5 times faster than Thrust merge sort performance, and 70% faster than non-stable state-of-the-art GPU merge sorts. Building on this sorting algorithm, we develop a scheme for sorting variable-length key/value pairs, with a special emphasis on string keys. Sorting non-uniform, unaligned data such as strings is a fundamental step in a variety of algorithms, yet it has received comparatively little attention. To our knowledge, our system is the first published description of an efficient string sort for GPUs. We are able to sort strings at a rate of 70 MStrings/s on an NVidia GTX 580 on one dataset, and up to 1.25 GB/s on another dataset.","Abstracts;Registers","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Davidson et al. 2012 - Efficient parallel merge sort for fixed and variable length keys.pdf","","","",""
"Miscellaneous","Maron CA,Vogel A,Griebler D,Fernandes LG","","Should PARSEC Benchmarks be More Parametric? A Case Study with Dedup","2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)","","","","","","","2019","","","","All","","","","","","","","","2019","","","","","","","http://dx.doi.org/10.1109/empdp.2019.8671592","10.1109/empdp.2019.8671592","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Maron et al. 2019 - Should PARSEC Benchmarks be More Parametric - A Case Study with Dedup.pdf","","","",""
"Conference paper","Lin C,Cao Q,Huang J,Yao J,Li X,Xie C","","HPDV:A Highly Parallel Deduplication Cluster for Virtual Machine Images","","","","","2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","","","2018","","","472-481","All","","","","","","","","","2018-05","","","","","","","http://dx.doi.org/10.1109/CCGRID.2018.00074;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8411063","10.1109/CCGRID.2018.00074","","","","Data deduplication has been widely introduced to effectively reduce storage requirement of virtual machine (VM) images running on VM servers in the virtualized cloud platforms. Nevertheless, the existing state-of-the-art deduplication for VM images approaches can not sufficiently exploit the potential of underlying hardware with consideration of the interference of deduplication on the foreground VM services, which could affect the quality of VM services. In this paper, we present HPDV, a highly parallel deduplication cluster for VM images, which well utilizes the parallelism to achieve high throughput with minimum interference on the foreground VM services. The main idea behind HPDV is to exploit idle CPU resource of VM servers to parallelize the compute-intensive chunking and fingerprinting, and to parallelize the I/O-intensive fingerprint indexing in the deduplication servers by dividing the globally shared fingerprint index into multiple independent sub-indexes according to the operating systems of VM images. To ensure the quality of VM services, a resource-aware scheduler is proposed to dynamically adjust the number of parallel chunking and fingerprinting threads according to the CPU utilization of VM servers. Our evaluation results demonstrate that compared to a state-of-the-art deduplication system for VM images called Light, HPDV achieves up to 67% deduplication throughput improvement.","Servers;Indexing;Throughput;Random access memory;Cloud computing;Metadata;deduplication;deduplication cluster;virtual machine image;highly parallel;resource aware scheduler","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2018 - HPDV - A Highly Parallel Deduplication Cluster for Virtual Machine Images.pdf","","","",""
"Journal article","","","A Review on Secure Data Deduplication: Cloud Storage Security Issue","Journal of King Saud University - Computer and Information Sciences","Journal of King Saud University - Computer and Information Sciences","","","","","","2022","34","7","3996-4007","All","","","","Elsevier","","","","","2022-07-01","","2023-02-20","","","1319-1578","","http://www.sciencedirect.com/science/article/pii/S1319157820305140;http://dx.doi.org/10.1016/j.jksuci.2020.10.021;https://reader.elsevier.com/reader/sd/pii/S1319157820305140?token=181DBEB210AF30D4F5A31B9C275C62305EF02CC2D33379ECFCA6DB333B428A44E19DBEA4CD3F24F68B48BF7595D9A2C0&originRegion=eu-west-1&originCreation=20230220201744","10.1016/j.jksuci.2020.10.021","","","","Cloud storage service providers caters to the need of organizations and individuals by allowing them to store, transfer and backup their ever-increasi…","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/A Review on Secure Data Deduplication... 2022 - A Review on Secure Data Deduplication - Cloud Storage Security Issue.pdf","","","",""
"Conference paper","Xia W,Jiang H,Feng D,Tian L","","Accelerating data deduplication by exploiting pipelining and parallelism with multicore or manycore processors","","","","","Proc. 10th USENIX Conf. File Storage Technol","","","2012","","","1-2","PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","2012","","2023-02-20","","","","","https://www.usenix.org/legacy/events/fast/poster_descriptions/Xiadescription.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2012 - Accelerating data deduplication by exploiting pipelining and parallelism with multicore or manycore processors.pdf","","","",""
"Miscellaneous","Lin B,Liao X,Li S,Wang Y,Huang H,Wen L","","G-Paradex: GPU-Based Parallel Indexing for Fast Data Deduplication","Lecture Notes in Computer Science","","","","","","","2013","","","91-103","All","","","","","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-45293-2_7","10.1007/978-3-642-45293-2_7","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Mohamed SM,Wang Y","","A survey on novel classification of deduplication storage systems","Distributed and Parallel Databases","","","","","","","2021","39","1","201-230","All","","","","","","","","","2021-03-01","","","","","1573-7578","","https://doi.org/10.1007/s10619-020-07301-2;http://dx.doi.org/10.1007/s10619-020-07301-2;https://link.springer.com/article/10.1007/s10619-020-07301-2","10.1007/s10619-020-07301-2","","","","The huge blast of information caused a lot of dilemmas in both storage and retrieval procedures. The enlargement in a massive quantity of digital data requirements imposes more storage space, which in turn radically increases performance and cost of backup. Data deduplication is one of the techniques that vanishes replicated data, decreases the bandwidth, and minimizes the disk usage and cost. Since various researches have been broadly considered in the literature, this paper reviews the ideas, categories, and different storage approaches that use data deduplication. Apart from the well-known classification that uses Granularity, Side, Timing, and Implementation for classifying the deduplication approaches, a new classification principle is adopted using the storage location. This classification identifies and describes the diverse methods. Moreover, the deduplication systems are comprehensively described according to the storage location, including Local, Centralized, and Clustered storage systems. Furthermore, the describing objectives, used techniques, features, and drawbacks of the most advanced methods of each type are broadly tackled. Finally, the major deduplication systems' challenges are recognized and illustrated.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Zhang D,Liao C,Yan W,Tao R,Zheng W","","Data Deduplication Based on Hadoop","","","","","2017 Fifth International Conference on Advanced Cloud and Big Data (CBD)","","","2017","","","147-152","All","","","","","","","","","2017-08","","","","","","","http://dx.doi.org/10.1109/CBD.2017.33;https://ieeexplore.ieee.org/abstract/document/8026928/","10.1109/CBD.2017.33","","","","Efficient and scalable deduplication techniques are required to serve the need of removing duplicated data in big data processing platforms such as Hadoop. In this paper, an integrated deduplication approach is proposed by taking the features of Hadoop into acount and leveraging parallelism based on MapReduce and HBase so as to speed up the deduplication procedure. In our proposed approach, a new small-file aggregation scheme is proposed, and the new standard of Secure Hash Algorithm-3, Keccak is employed. The effectiveness of our proposed approach is demonstrated by extensive experiments on both artificial datasets with specific characteristics, as well as real world datasets.","Distributed databases;Indexes;Scalability;Servers;Redundancy;Big Data;Parallel processing;deduplication;distributed processing;Hadoop","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Ramya P,Sundar C","","SecDedoop: Secure Deduplication with Access Control of Big Data in the HDFS/Hadoop Environment","Big Data","Big data","","","","","","2020","8","2","147-163","All","","","","","","","","","2020-04","","","","","2167-647X","2167-6461","http://dx.doi.org/10.1089/big.2019.0120;https://www.ncbi.nlm.nih.gov/pubmed/32319799;https://www.liebertpub.com/doi/abs/10.1089/big.2019.0120","10.1089/big.2019.0120","32319799","","","With the rapid growth of storage providers, data deduplication is an essential storage optimization technique that greatly minimizes data storage costs by storing a unique copy of duplicate data. Nowadays, deduplication introduces various new challenges such as security and insufficient space issue. Hence, in this article, we propose a secure data deduplication with access control of big data over HDFS (Hadoop Distributed File System)/Hadoop environment, called SecDedoop. First, the system achieves security for data confidentiality by third party vendor using elliptic curve cryptography. There are two types of keys (public key and private key) generated for data retrieval. Second, we consider data deduplication. The user's original file is divided into a number of equal chunks. Then, each chunk (e.g., 1. txt) is tokenized into words and the weight of words is computed by using TF-IDF frequency. The SHA-3 hash computation is performed to the user's original file. If the hash value is not duplicate, then we store data in HDFS. The PSO (particle swarm optimization)-based MapReduce model is the proposed best data node selection. Initially, MapReduce process is finished for the user's original file and it results in the best set of data nodes; then, we apply PSO to compute the fitness value for best data node selection. Further, we consider MongoDB for fast indexing of the user's original files and also apply FCM (fuzzy-C-means clustering) for clustering the user's files. In this article, we consider the modified version of PSO and FCM to eliminate the open issues in conventional PSO and FCM. The performance of our proposed SecDedoop has been evaluated by using various performance metrics and also proved it outperforms better than previous approaches.","Hadoop Distributed File System; SHA-3; access control; data deduplication; elliptic curve cryptography; particle swarm optimization-based MapReduce","","","Department of Information Technology, Christian College of Engineering and Technology, Oddanchatram, Dindigul, India. Department of Computer Science and Engineering, Christian College of Engineering and Technology, Oddanchatram, Dindigul, India.","en","Research Article","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Kumar N,Rawat R,Jain SC","","Bucket based data deduplication technique for big data storage system","","","","","2016 5th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","","","2016","","","267-271","All","","","","","","","","","2016-09","","","","","","","http://dx.doi.org/10.1109/ICRITO.2016.7784963;https://ieeexplore.ieee.org/abstract/document/7784963/","10.1109/ICRITO.2016.7784963","","","","In this paper proposed bucket based data deduplication technique is presented. In proposed technique bigdata stream is given to the fixed size chunking algorithm to create fixed size chunks. When the chunks are obtained then these chunks are given to the MD5 algorithm module to generate hash values for the chunks. After that MapReduce model is applied to find whether hash values are duplicate or not. To detect the duplicate hash values MapReduce model compared these hash values with already stored hash values in bucket storage. If these hash values are already present in the bucket storage then these can be identified as duplicate. If the hash values are duplicated then do not store the data into the Hadoop Distributed File System (HDFS) else then store the data into the HDFS. The proposed technique is analyzed using real data set using Hadoop tool.","Conferences;Big data;Optimization;Systems architecture;Reliability;Market research;Algorithm design and analysis;Big Data;Hadoop;CDC Chunking;Bucket;Deduplication;Chunk","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Chu X,Ilyas IF,Koutris P","","Distributed data deduplication","Proceedings VLDB Endowment","Proceedings of the VLDB Endowment International Conference on Very Large Data Bases","","","","","","2016","9","11","864-875","Parallel Schemes;All","","","","VLDB Endowment","","","","","2016-07-01","","","","","2150-8097","","https://doi.org/10.14778/2983200.2983203;http://dx.doi.org/10.14778/2983200.2983203;https://dl.acm.org/doi/abs/10.14778/2983200.2983203;https://chu-data-lab.cc.gatech.edu/files/2020/10/distributed-dedup-paper.pdf","10.14778/2983200.2983203","","","","Data deduplication refers to the process of identifying tuples in a relation that refer to the same real world entity. The complexity of the problem is inherently quadratic with respect to the number of tuples, since a similarity value must be computed for every pair of tuples. To avoid comparing tuple pairs that are obviously non-duplicates, blocking techniques are used to divide the tuples into blocks and only tuples within the same block are compared. However, even with the use of blocking, data deduplication remains a costly problem for large datasets. In this paper, we show how to further speed up data deduplication by leveraging parallelism in a shared-nothing computing environment. Our main contribution is a distribution strategy, called Dis-Dedup, that minimizes the maximum workload across all worker nodes and provides strong theoretical guarantees. We demonstrate the effectiveness of our proposed strategy by performing extensive experiments on both synthetic datasets with varying block size distributions, as well as real world datasets.","","Joins, data lake, blocks. Not applicable to SCAIL.","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chu et al. 2016 - Distributed data deduplication.pdf","","","","July 2016"
"Journal article","Ma K,Dong F,Yang B","","Large-Scale Schema-Free Data Deduplication Approach with Adaptive Sliding Window Using MapReduce","Comput. J.","Computer Journal","","","","","","2015","58","11","3187-3201","All","","","","","","","","","2015-11","","","","","0010-4620","1460-2067","http://dx.doi.org/10.1093/comjnl/bxv052;https://ieeexplore.ieee.org/abstract/document/8213082/","10.1093/comjnl/bxv052","","","","Data deduplication is the task of identifying all groups of objects within one or several data sets, respectively. However, this task will become difficult in the context of big data. To address this limitation, we propose a new schema-free data deduplication approach in parallel in the aspect of breeding data deduplication related to food safety. Although MapReduce framework enables efficient parallel execution of data-intensive tasks, it cannot find duplicates in adjacent block. Furthermore, current deduplication approaches with MapReduce are restricted to fixed sliding window. Therefore, we investigate possible solutions to improve current deduplication approaches with MapReduce, to make sliding window size adaptive using adaptive multiple duplicate count strategy with alterable window step, and find duplicates by overlapping boundary objects in adjacent blocks. Moreover, we propose a multi-pass Partition-Sort-Map-Reduce approach with adaptive sliding window to speed up the deduplication process. Finally, our experimental evaluation based on the breeding data on large datasets shows the high effectiveness and efficiency of the proposed approaches.","deduplication;MapReduce;sorted neighborhood;BigData;NoSQL","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Kolb L,Thor A,Rahm E","","Dedoop: Efficient Deduplication with Hadoop","","","","","","","","","","","","All","","","","","","","","","","","2023-02-20","","","","","https://dbs.uni-leipzig.de/file/Dedoop.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kolb et al. - Dedoop - Efficient Deduplication with Hadoop.pdf","","","",""
"Conference paper","Dal Bianco G,Galante R,Heuser CA","","A fast approach for parallel deduplication on multicore processors","","","","","Proceedings of the 2011 ACM Symposium on Applied Computing","","","2011","","","1027-1032","Parallel Schemes;All","","","","Association for Computing Machinery","New York, NY, USA","","TaiChung, Taiwan","","2011-03-21","","2023-02-20","9781450301138","","","","https://doi.org/10.1145/1982185.1982411;http://dx.doi.org/10.1145/1982185.1982411;https://dl.acm.org/doi/abs/10.1145/1982185.1982411;https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=0261d528ec0e1a31e8a40053acd60513beebef2d","10.1145/1982185.1982411","","","","In this paper, we propose a fast approach that parallelizes the deduplication process on multicore processors. Our approach, named MD-Approach, combines an efficient blocking method with a robust data parallel programming model. The blocking phase is composed of two steps. The first step generates large blocks by grouping records with low degree of similarity. The second step segments large blocks, that may result in unbalanced load, in more precise sub-blocks. A parallel data programming model is used to implement our approach in a sequence of both map and reduce operations. An empirical evaluation has shown that our deduplication approach is almost twice faster than BTO-BK, that is a scalable parallel deduplication solution in distributed environment. To the best of our knowledge, MD-Approach is the first to focus on multicore processors for parallel dedu-plication.","data integration, parallel systems, deduplication","","","","","","SAC '11","","","","","","","","","","","","","","","","","","All Papers/D/Dal Bianco et al. 2011 - A fast approach for parallel deduplication on multicore processors.pdf","","","",""
"Conference paper","Santos W,Teixeira T,Machado C,Meira Jr W,Ferreira R,Guedes D,Da Silva AS","","A Scalable Parallel Deduplication Algorithm","","","","","19th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'07)","","","2007","","","79-86","All","","","","","","","","","2007-10","","","","","1550-6533","","http://dx.doi.org/10.1109/SBAC-PAD.2007.32;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4384045","10.1109/SBAC-PAD.2007.32","","","","The identification of replicas in a database is fundamental to improve the quality of the information. Deduplication is the task of identifying replicas in a database that refer to the same real world entity. This process is not always trivial, because data may be corrupted during their gathering, storing or even manipulation. Problems such as misspelled names, data truncation, data input in a wrong format, lack of conventions (like how to abbreviate a name), missing data or even fraud may lead to the insertion of replicas in a database. The deduplication process may be very hard, if not impossible, to be performed manually, since actual databases may have hundreds of millions of records. In this paper, we present our parallel deduplication algorithm, called FER- APARDA. By using probabilistic record linkage, we were able to successfully detect replicas in synthetic datasets with more than 1 million records in about 7 minutes using a 20- computer cluster, achieving an almost linear speedup. We believe that our results do not have similar in the literature when it comes to the size of the data set and the processing time.","Couplings;Computer science;Scalability;Computer architecture;High performance computing;Demography;Clustering algorithms;Deductive databases;Software libraries;Erbium","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Cogo V,Paulo J,Bessani A","","GenoDedup: Similarity-based deduplication and delta-encoding for genome sequencing data","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2021","70","5","669-681","All","Similarity/Resemblance","","","Institute of Electrical and Electronics Engineers (IEEE)","","","","","2021-05-01","","","","","0018-9340","1557-9956","http://repositorio.inesctec.pt/bitstream/123456789/11369/1/P-00S-B3X.pdf;https://ieeexplore.ieee.org/document/9094002/;http://dx.doi.org/10.1109/tc.2020.2994774","10.1109/tc.2020.2994774","","","","… However, the decompression speed becomes a bottleneck in cases where compressed data is read from remote storage systems and needs to be decompressed and read several …","","","https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cogo et al. 2021 - GenoDedup - Similarity-based deduplication and delta-encoding for genome sequencing data.pdf","","","",""
"Miscellaneous","Feng D","","Deduplication: Beginning from Data Backup System","Data Deduplication for High Performance Storage System","","","","","","","2022","","","1-8","All","","","","","","","","","2022","","","","","","","http://dx.doi.org/10.1007/978-981-19-0112-6_1","10.1007/978-981-19-0112-6_1","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Cheng G,Guo D,Luo L,Xia J,Gu S","","LOFS: A Lightweight Online File Storage Strategy for Effective Data Deduplication at Network Edge","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2022","33","10","2263-2276","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","","","","","","","","","2022-10","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2021.3133098;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9645269","10.1109/TPDS.2021.3133098","","","","Edge computing responds to users’ requests with low latency by storing the relevant files at the network edge. Various data deduplication technologies are currently employed at edge to eliminate redundant data chunks for space saving. However, the lookup for the global huge-volume fingerprint indexes imposed by detecting redundancies can significantly degrade the data processing performance. Besides, we envision a novel file storage strategy that realizes the following rationales simultaneously: 1) space efficiency, 2) access efficiency, and 3) load balance, while the existing methods fail to achieve them at one shot. To this end, we report LOFS, a Lightweight Online File Storage strategy, which aims at eliminating redundancies through maximizing the probability of successful data deduplication, while realizing the three design rationales simultaneously. LOFS leverages a lightweight three-layer hash mapping scheme to solve this problem with constant-time complexity. To be specific, LOFS employs the Bloom filter to generate a sketch for each file, and thereafter feeds the sketches to the Locality Sensitivity hash (LSH) such that similar files are likely to be projected nearby in LSH tablespace. At last, LOFS assigns the files to real-world edge servers with the joint consideration of the LSH load distribution and the edge server capacity. Trace-driven experiments show that LOFS closely tracks the global deduplication ratio and generates a relatively low load std compared with the comparison methods.","Servers;Indexes;Costs;Redundancy;Throughput;Image edge detection;Resource management;Data deduplication;locality sensitivity hash;edge computing;storage systems","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cheng et al. 2022 - LOFS - A Lightweight Online File Storage Strategy for Effective Data Deduplication at Network Edge.pdf","","","",""
"Journal article","Zhang C,Miao Y,Xie Q,Guo Y,Du H,Jia X","","Privacy-Preserving Deduplication of Sensor Compressed Data in Distributed Fog Computing","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2022","33","12","4176-4191","All","","","","","","","","","2022-12","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2022.3179992;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9787776","10.1109/TPDS.2022.3179992","","","","Distributed fog computing has received wide attention recently. It enables distributed computing and data management on the network nodes within the close vicinity of IoT devices. An important service of fog-cloud based systems is data deduplication. With the increasing concern of privacy, some privacy-preserving data deduplication schemes have been proposed. However, they cannot support lossless deduplication of encrypted similar data in the fog-cloud network. Meanwhile, no existing design can protect message equality information while resisting brute-force and frequency analysis attacks. In this paper, we propose a privacy-preserving and compression-based data deduplication system under the fog-cloud network, which supports lossless deduplication of similar data in the encrypted domain. Specifically, we first use the generalized deduplication technique and cryptographic primitives to implement secure deduplication over similar data. Then, we devise a two-level deduplication protocol that can perform secure and efficient deduplication at distributed fog nodes and the cloud. The proposed system can not only resist brute-force and frequency analysis attacks but also ensure that only the data operator can capture the message equality information. We formally analyze the security of our design. Performance evaluations demonstrate that our proposed design is efficient in computing, storage, and communication.","Cloud computing;Cryptography;Sensors;Internet of Things;Data models;Costs;Computer science;Brute-force attacks;fog-cloud network;frequency analysis;message equality information;similar data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2022 - Privacy-Preserving Deduplication of Sensor Compressed Data in Distributed Fog Computing.pdf","","","",""
"Journal article","Nachman A,Sheinvald S,Kolikant A,Yadgar G","","GoSeed: Optimal Seeding Plan for Deduplicated Storage","ACM Trans. Storage","","","","","","","2021","17","3","1-28","All;MS Traces;FSL Traces;Thesis;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2021-08-16","","","","","1553-3077","","https://doi.org/10.1145/3453301;http://dx.doi.org/10.1145/3453301;https://dl.acm.org/doi/10.1145/3453301","10.1145/3453301","","","","Deduplication decreases the physical occupancy of files in a storage volume by removing duplicate copies of data chunks, but creates data-sharing dependencies that complicate standard storage management tasks. Specifically, data migration plans must consider the dependencies between files that are remapped to new volumes and files that are not. Thus far, only greedy approaches have been suggested for constructing such plans, and it is unclear how they compare to one another and how much they can be improved. We set to bridge this gap for seeding—migration in which the target volume is initially empty. We prove that even this basic instance of data migration is NP-hard in the presence of deduplication. We then present GoSeed, a formulation of seeding as an integer linear programming (ILP) problem, and three acceleration methods for applying it to real-sized storage volumes. Our experimental evaluation shows that, while the greedy approaches perform well on “easy” problem instances, the cost of their solution can be significantly higher than that of GoSeed’s solution on “hard” instances, for which they are sometimes unable to find a solution at all.","capacity planning, data migration, Deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Nachman et al. 2021 - GoSeed - Optimal Seeding Plan for Deduplicated Storage.pdf","","","24","August 2021"
"Journal article","Lin L,Deng Y,Zhou Y,Zhu Y","","InDe: An inline data deduplication approach via adaptive detection of valid container utilization","ACM Trans. Storage","","","","","","","2022","","","","All;FSL Traces;Thesis;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2022-11-19","","","","","1553-3077","","https://doi.org/10.1145/3568426;http://dx.doi.org/10.1145/3568426;https://dl.acm.org/doi/10.1145/3568426","10.1145/3568426","","","","Inline deduplication removes redundant data in real-time as data is being sent to the storage system. However, it causes data fragmentation: logically consecutive chunks are physically scattered across various containers after data deduplication. Many rewrite algorithms aim to alleviate the performance degradation due to fragmentation by rewriting fragmented duplicate chunks as unique chunks into new containers. Unfortunately, these algorithms determine whether a chunk is fragmented based on a simple pre-set fixed value, ignoring the variance of data characteristics between data segments. Accordingly, when backups are restored, they often fail to select an appropriate set of old containers for rewrite, generating a substantial number of invalid chunks in retrieved containers. To address this issue, we propose an inline deduplication approach for storage systems, called InDe, which uses a greedy algorithm to detect valid container utilization and dynamically adjusts the number of old container references in each segment. InDe fully leverages the distribution of duplicated chunks to improve the restore performance while maintaining high backup performance. We define an effectiveness metric valid container referenced counts (VCRC), to identify appropriate containers for the rewrite. We design a rewrite algorithm F-greedy that detects valid container utilization to rewrite low-VCRC containers. According to the VCRC distribution of containers, F-greedy dynamically adjusts the number of old container references to only share duplicate chunks with high-utilization containers for each segment, thereby improving the restore speed. To take full advantage of the above features, we further propose another rewrite algorithm called F-greedy+ based on adaptive interval detection of valid container utilization. F-greedy+ makes a more accurate estimation of the valid utilization of old containers by detecting trends of VCRC’s change in two directions and selecting referenced containers in the global scope. We quantitatively evaluate InDe using three real-world backup workloads. The experimental results show that compared with two state-of-the-art algorithms (Capping and SMR), our scheme improves the restore speed by 1.3x - 2.4x while achieving almost the same backup performance.","Storage System, Data Deduplication, Restore Performance","191 FSL with specific users and dates","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2022 - InDe - An inline data deduplication approach via adaptive detection of valid container utilization.pdf","","","",""
"Journal article","Zhao N,Tarasov V,Albahar H,Anwar A,Rupprecht L,Skourtis D,Paul AK,Chen K,Butt AR","","Large-Scale Analysis of Docker Images and Performance Implications for Container Storage Systems","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2021","32","4","918-930","All","","","","","","","","","2021-04","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2020.3034517;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9242268","10.1109/TPDS.2020.3034517","","","","Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the application’s execution environment. Containers are created from images which are shared between users via a registry. The amount of data registries store is massive. For example, Docker Hub, a popular public registry, stores at least half a million public images. In this article, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3 percent of the files in images are unique while others are redundant file copies, which means file-level deduplication has a great potential to save storage space. Furthermore, we carry out a comprehensive analysis of both small I/O request performance and copy-on-write performance for multiple popular container storage drivers. Our findings can motivate and help improve the design of data reduction and caching methods for images, pulling optimizations for registries, and storage drivers.","Containers;Image coding;Cows;Crawlers;Measurement;Libraries;Ecosystems;Containers;Docker;container images;container registry;deduplication;Docker hub;container storage drivers","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhao et al. 2021 - Large-Scale Analysis of Docker Images and Performance Implications for Container Storage Systems.pdf","","","",""
"Conference paper","Li Z,Jin C,Xu T,Wilson C,Liu Y,Cheng L,Liu Y,Dai Y,Zhang ZL","","Towards Network-level Efficiency for Cloud Storage Services","","","","","Proceedings of the 2014 Conference on Internet Measurement Conference","","","2014","","","115-128","All","","","","Association for Computing Machinery","New York, NY, USA","","Vancouver, BC, Canada","","2014-11-05","","2022-11-01","9781450332132","","","","https://doi.org/10.1145/2663716.2663747;http://dx.doi.org/10.1145/2663716.2663747;https://dl.acm.org/doi/abs/10.1145/2663716.2663747?casa_token=RpAU16lX13gAAAAA:38AhTPEkUQiuNtr81VaSgkaVgKMRpDNL3JIZ535hZwr2xgFWY0wgQ69ZJi1gjwylD47pKaBtNo8F;https://dl.acm.org/doi/pdf/10.1145/2663716.2663747?casa_token=OMEQX9sadboAAAAA:xXp7GqdOEUeb7_4iexIRLe7hY84kmFcxHCb-MA0QYOj3ipW3QDb-MDg9EcgblbtL50sHK8-KIsZn","10.1145/2663716.2663747","","","","Cloud storage services such as Dropbox, Google Drive, and Microsoft OneDrive provide users with a convenient and reliable way to store and share data from anywhere, on any device, and at any time. The cornerstone of these services is the data synchronization (sync) operation which automatically maps the changes in users' local filesystems to the cloud via a series of network communications in a timely manner. If not designed properly, however, the tremendous amount of data sync traffic can potentially cause (financial) pains to both service providers and users.This paper addresses a simple yet critical question: Is the current data sync traffic of cloud storage services efficiently used? We first define a novel metric named TUE to quantify the Traffic Usage Efficiency} of data synchronization. Based on both real-world traces and comprehensive experiments, we study and characterize the TUE of six widely used cloud storage services. Our results demonstrate that a considerable portion of the data sync traffic is in a sense wasteful, and can be effectively avoided or significantly reduced via carefully designed data sync mechanisms. All in all, our study of TUE of cloud storage services not only provides guidance for service providers to develop more efficient, traffic-economic services, but also helps users pick appropriate services that best fit their needs and budgets.","traffic usage efficiency, network-level efficiency, cloud storage service, data synchronization","","","","","","IMC '14","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Towards Network-level Efficiency for Cloud Storage Services.pdf","","","",""
"Conference paper","Kulkarni P,Douglis F,LaVoie JD,Tracey JM","","Redundancy elimination within large collections of files","","","","","USENIX Annual Technical Conference, General Track","","","2004","","","59-72","All","","","","","","","","","2004","","","","","","","https://www.usenix.org/event/usenix04/tech/general/full_papers/kulkarni/kulkarni_html/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Shilane P,Huang M,Wallace G,Hsu W","","WAN-optimized replication of backup datasets using stream-informed delta compression","ACM Trans. Storage","","","","","","","2012","8","4","1-26","All;Thesis","","","","Association for Computing Machinery","New York, NY, USA","","","","2012-12-06","","","","","1553-3077","","https://doi.org/10.1145/2385603.2385606;http://dx.doi.org/10.1145/2385603.2385606;https://dl.acm.org/doi/abs/10.1145/2385603.2385606?casa_token=2jzVkhq751AAAAAA:j4TTB8diew4heC4uvNUhhyH4gZUJ2e5bmqB3YLPPdIpXHJ5If4OFodYlkEPr7pZPIsJtGgQDi4dj;https://dl.acm.org/doi/pdf/10.1145/2385603.2385606?casa_token=rj_LmOS_KIEAAAAA:jLYr03XpKCt8K_ynAjFZMql0b1cUUvaNDV8_JsjH3YvETh_37RZ71Iw8B5FHxluzMNpe0YozNPeP","10.1145/2385603.2385606","","","","Replicating data off site is critical for disaster recovery reasons, but the current approach of transferring tapes is cumbersome and error prone. Replicating across a wide area network (WAN) is a promising alternative, but fast network connections are expensive or impractical in many remote locations, so improved compression is needed to make WAN replication truly practical. We present a new technique for replicating backup datasets across a WAN that not only eliminates duplicate regions of files (deduplication) but also compresses similar regions of files with delta compression, which is available as a feature of EMC Data Domain systems.Our main contribution is an architecture that adds stream-informed delta compression to already existing deduplication systems and eliminates the need for new, persistent indexes. Unlike techniques based on knowing a file's version or that use a memory cache, our approach achieves delta compression across all data replicated to a server at any time in the past. From a detailed analysis of datasets and statistics from hundreds of customers using our product, we achieve an additional 2X compression from delta compression beyond deduplication and local compression, which enables customers to replicate data that would otherwise fail to complete within their backup window.","delta compression, deduplication, network replication, Backup storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shilane et al. 2012 - WAN-optimized replication of backup datasets using stream-informed delta compression.pdf","","","13","November 2012"
"Journal article","Zhang D,Deng Y,Zhou Y,Li J,Zhu W,Min G","","MGRM: A Multi-Segment Greedy Rewriting Method to Alleviate Data Fragmentation in Deduplication-Based Cloud Backup Systems","IEEE Transactions on Cloud Computing","","","","","","","2022","","","1-14","All","","","","","","","","","2022","","","","","2168-7161","","http://dx.doi.org/10.1109/TCC.2022.3214816;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9920180","10.1109/TCC.2022.3214816","","","","Data deduplication has been broadly used in Cloud due to its storage space saving ability. An issue of deduplication is the contiguous data chunks in a segment may be scattered in different containers. This phenomenon is called data fragmentation. Because of data fragmentation, a restore process must reference various containers across a wide variety of segments, thereby hurting the restore performance. Capping methods that rewrite the data chunks of low Container Reference Ratio (CRR) containers are developed to alleviate data fragmentation. We analyze and observe from real traces that a number of segments only point to low CRR containers, while some others only contain high CRR containers. This interesting observation is ignored by the existing capping methods which sort containers from a single segment, falling short in searching multiple segments collectively. Thus, the reference count of selected containers in the existing capping methods is still high. To address this problem, we propose a multi-segment greedy rewriting method named MGRM. MGRM sorts containers of segments in a sequential way. More specifically, given the i-th segment currently being processed, MGRM will sort all the containers in the top i-th segments. This salient searching feature enables MGRM to select and rewrite the true low-reference container set. Moreover, to achieve a good balance between deduplication ratio and restore performance, MGRM has two working modes: an optimal rewriting mode and a radical rewriting mode. When working in the optimal rewriting mode, MGRM aims to improve the deduplication ratio; when the radical rewriting mode, MGRM strives to improve the restore performance. MGRM adaptively switches the working mode according to workload. Furthermore, unlike the existing capping methods that improve restore performance at the cost of the deduplication ratio, MGRM pays attention to both aspects. Our extensive experimental results show that MGRM achieves high restore performance, coupled with a high deduplication ratio. In particular, compared with the two state-of-art schemes FC and FLC, MGRM improves the deduplication ratio and restore performance by up to 114.83% and 99.34%, respectively.","Containers;Sorting;Cloud computing;Costs;Redundancy;Mathematical models;Lead;Cloud;data deduplication;data restore;fragmentation;rewriting algorithm","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2022 - MGRM - A Multi-Segment Greedy Rewriting Method to Alleviate Data Fragmentation in Deduplication-Based Cloud Backup Systems.pdf","","","",""
"Journal article","Zheng J,Li Z,Qiu Y,Lin H,Xiao H,Li Y,Liu Y","","WebAssembly-based Delta Sync for Cloud Storage Services","ACM Trans. Storage","","","","","","","2022","18","3","1-31","All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2022-09-21","","","","","1553-3077","","https://doi.org/10.1145/3502847;http://dx.doi.org/10.1145/3502847;https://dl.acm.org/doi/pdf/10.1145/3502847","10.1145/3502847","","","","Delta synchronization (sync) is crucial to the network-level efficiency of cloud storage services, especially when handling large files with small increments. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers—the most pervasive and OS-independent access method. To bridge this gap, prior work concentrates on either reversing the delta sync protocol or utilizing the native client, all striving around the tradeoffs among efficiency, applicability, and usability and thus forming an “impossible triangle.” Recently, we note the advent of WebAssembly (WASM), a portable binary instruction format that is efficient in both encoding size and load time. In principle, the unique advantages of WASM can make web-based applications enjoy near-native runtime speed without significant cloud-side or client-side changes. Thus, we implement a straightforward WASM-based delta sync solution, WASMrsync, finding its quasi-asynchronous working manner and conventional In-situ Separate Memory Allocation greatly increase sync time and memory usage. To address them, we strategically devise sync-async code decoupling and streaming compilation, together with Informed In-place File Construction. The resulting solution, WASMrsync+, achieves comparable sync time as the state-of-the-art (most efficient) solution with nearly only half of memory usage, letting the “impossible triangle” reach a reconciliation.","web browser, Cloud storage service, WebAssembly, delta synchronization","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zheng et al. 2022 - WebAssembly-based Delta Sync for Cloud Storage Services.pdf","","","24","August 2022"
"Journal article","Yang Z,Li J,Ren Y,Lee PP","","Tunable Encrypted Deduplication with Attack-resilient Key Management","ACM Trans. Storage","","","","","","","2022","18","4","1-38","All;Thesis;Grouped by Publication/ACM TOS;p-scailbib","Datasets;Tunable;Frequency","","","Association for Computing Machinery","New York, NY, USA","","","","2022-11-11","","","","","1553-3077","","https://doi.org/10.1145/3510614;http://dx.doi.org/10.1145/3510614;https://dl.acm.org/doi/pdf/10.1145/3510614","10.1145/3510614","","","","Conventional encrypted deduplication approaches retain the deduplication capability on duplicate chunks after encryption by always deriving the key for encryption/decryption from the chunk content, but such a deterministic nature causes information leakage due to frequency analysis. We present TED, a tunable encrypted deduplication primitive that provides a tunable mechanism for balancing the tradeoff between storage efficiency and data confidentiality. The core idea of TED is that its key derivation is based on not only the chunk content but also the number of duplicate chunk copies, such that duplicate chunks are encrypted by distinct keys in a controlled manner. In particular, TED allows users to configure a storage blowup factor, under which the information leakage quantified by an information-theoretic measure is minimized for any input workload. In addition, we extend TED with a distributed key management architecture and propose two attack-resilient key generation schemes that trade between performance and fault tolerance. We implement an encrypted deduplication prototype TEDStore to realize TED in networked environments. Evaluation on real-world file system snapshots shows that TED effectively balances the tradeoff between storage efficiency and data confidentiality, with small performance overhead.","Encrypted deduplication, cloud storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2022 - Tunable Encrypted Deduplication with Attack-resilient Key Management.pdf","","","32","November 2022"
"Journal article","Kisous R,Kolikant A,Duggal A,Sheinvald S,Yadgar G","","The what, The from, and The to: The Migration Games in Deduplicated Systems","ACM Trans. Storage","","","","","","","2022","","","","All;MS Traces;FSL Traces;Thesis;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2022-09-29","","","","","1553-3077","","https://doi.org/10.1145/3565025;http://dx.doi.org/10.1145/3565025;https://dl.acm.org/doi/pdf/10.1145/3565025","10.1145/3565025","","","","Deduplication reduces the size of the data stored in large-scale storage systems by replacing duplicate data blocks with references to their unique copies. This creates dependencies between files that contain similar content, and complicates the management of data in the system. In this paper, we address the problem of data migration, where files are remapped between different volumes as a result of system expansion or maintenance. The challenge of determining which files and blocks to migrate has been studied extensively for systems without deduplication. In the context of deduplicated storage, however, only simplified migration scenarios have been considered. In this paper, we formulate the general migration problem for deduplicated systems as an optimization problem whose objective is to minimize the system’s size while ensuring that the storage load is evenly distributed between the system’s volumes, and that the network traffic required for the migration does not exceed its allocation. We then present three algorithms for generating effective migration plans, each based on a different approach and representing a different tradeoff between computation time and migration efficiency. Our greedy algorithm provides modest space savings, but is appealing thanks to its exceptionally short runtime. Its results can be improved by using larger system representations. Our theoretically optimal algorithm formulates the migration problem as an ILP (integer linear programming) instance. Its migration plans consistently result in smaller and more balanced systems than those of the greedy approach, although its runtime is long and, as a result, the theoretical optimum is not always found. Our clustering algorithm enjoys the best of both worlds: its migration plans are comparable to those generated by the ILP-based algorithm, but its runtime is shorter, sometimes by an order of magnitude. It can be further accelerated at a modest cost in the quality of its results.","capacity planning, data migration, deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kisous et al. 2022 - The what, The from, and The to - The Migration Games in Deduplicated Systems.pdf","","","",""
"Conference paper","Zhou P,Wang Z,Xia W,Zhang H","","UltraCDC:A Fast and Stable Content-Defined Chunking Algorithm for Deduplication-based Backup Storage Systems","","","","","2022 IEEE International Performance, Computing, and Communications Conference (IPCCC)","","","2022","","","298-304","All","","","","","","","","","2022-11","","","","","2374-9628","","http://dx.doi.org/10.1109/IPCCC55026.2022.9894295;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9894295","10.1109/IPCCC55026.2022.9894295","","","","Content-Defined Chunking(CDC) is the key stage of data deduplication since it has a significant impact on deduplication system’s throughput and deduplication efficiency. However, existing CDC algorithms suffer from high computation overhead, weak stability, and poor ability to handle low-entropy strings. In this paper, we propose UltraCDC, a fast and stable, high-efficiency deal with low-entropy strings, CDC algorithm for deduplication-based storage systems. There are four key techniques behind UltraCDC, namely, rolling compute boundary conditions, skipping sub-minimum chunk size, normalized chunking, and jumping to detect low-entropy strings. Using a sliding window to rolling compute boundary conditions not only accelerates the chunking stage but also makes it more resistant to boundary shift, the two techniques of skipping sub-minimum chunk size and normalized chunking can complement each other to speed up chunking without sacrificing deduplication ratio too much, and the jumping detection can detect more low-entropy strings than AE-opt2 without affecting chunking speed. We implemented UltraCDC in Destor, and the experimental results show that using the above four techniques, chunking speed is 1.5–10× faster than the state-of-the-art CDC approaches, while deduplication ratio is comparable or even higher than the classic Rabin-base CDC. In terms of the capability to detect low-entropy strings, UltraCDC is a CDC approach with the highest ability to detect low-entropy strings, 102× and 2× higher than Rabin-based CDC and AE-opt2, respectively.","Resistance;System performance;Boundary conditions;Throughput;Data deduplication;content-defined chunking algorithm;storage system;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2022 - UltraCDC - A Fast and Stable Content-Defined Chunking Algorithm for Deduplication-based Backup Storage Systems.pdf","","","",""
"Conference paper","Nam YJ,Park D,Du DH","","Assuring Demanded Read Performance of Data Deduplication Storage with Backup Datasets","","","","","2012 IEEE 20th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems","","","2012","","","201-208","All","","","","ieeexplore.ieee.org","","","","","2012-08","","","","","2375-0227","","http://dx.doi.org/10.1109/MASCOTS.2012.32;https://ieeexplore.ieee.org/abstract/document/6298180/?casa_token=DbJlAVw0xlEAAAAA:oInQAurlKSifGxmrJnkV2oZriOLV5j_7KJ9rrViJscxS8AA9suz5KalvIauysVoLOzZj2x2M2A5DTQ;https://ieeexplore.ieee.org/iel5/6297612/6298144/06298180.pdf?casa_token=rO8PShWTFnAAAAAA:qyzVlDldhBjagKuFzjxfaJ0Gp9uEAxaUCQdDeHGKjiDirAZJ6Ar35cGGeyMdS8VcprBOewubUJCUzQ","10.1109/MASCOTS.2012.32","","","","Data deduplication has been widely adopted in contemporary backup storage systems. It not only saves storage space considerably, but also shortens the data backup time significantly. Since the major goal of the original data deduplication lies in saving storage space, its design has been focused primarily on improving write performance by removing as many duplicate data as possible from incoming data streams. Although fast recovery from a system crash relies mainly on read performance provided by deduplication storage, little investigation into read performance improvement has been made. In general, as the amount of deduplicated data increases, write performance improves accordingly, whereas associated read performance becomes worse. In this paper, we newly propose a deduplication scheme that assures demanded read performance of each data stream while achieving its write performance at a reasonable level, eventually being able to guarantee a target system recovery time. For this, we first propose an indicator called cache aware Chunk Fragmentation Level (CFL) that estimates degraded read performance on the fly by taking into account both incoming chunk information and read cache effects. We also show a strong correlation between this CFL and read performance in the backup datasets. In order to guarantee demanded read performance expressed in terms of a CFL value, we propose a read performance enhancement scheme called selective duplication that is activated whenever the current CFL becomes worse than the demanded one. The key idea is to judiciously write non-unique (shared) chunks into storage together with unique chunks unless the shared chunks exhibit good enough spatial locality. We quantify the spatial locality by using a selective duplication threshold value. Our experiments with the actual backup datasets demonstrate that the proposed scheme achieves demanded read performance in most cases at the reasonable cost of write performance.","Containers;Throughput;Correlation;Indexing;Monitoring;Educational institutions;data deduplication;storage;read performance","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","","","[No title]","","","","","","","","","","","","All","","","","","","","","","","","2022-10-24","","","","","https://www.researchgate.net/profile/Viji-D/publication/351212965_Comparative_Analysis_for_Content_Defined_Chunking_Algorithms_in_Data_Deduplication/links/6242763557084c718b72b81a/Comparative-Analysis-for-Content-Defined-Chunking-Algorithms-in-Data-Deduplication.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/[No title] - [No title].pdf","","","",""
"Conference paper","Zhang C,Qi D,Huang W","","Shadow Data: A Method to Optimize Incremental Synchronization in Data Center","","","","","Network and Parallel Computing","","","2021","","","339-348","All","","","","Springer International Publishing","","","","","2021","","","","","","","http://dx.doi.org/10.1007/978-3-030-79478-1_29;https://link.springer.com/chapter/10.1007/978-3-030-79478-1_29","10.1007/978-3-030-79478-1_29","","","","With the continuous increase of data, the data center that plays the role of backup is facing the problem of energy hunger. In practice, to reduce the bandwidth, the local data is synchronized to the data center based on incremental synchronization. In this process, the data center will generate a huge CPU load. To solve this pressure of the data center, first, we analyze the process of the Rsync algorithm, the most commonly used in incremental synchronization, and CDC algorithms, another way of chunking algorithm. Then we propose a data structure called Shadow Data, which greatly reduces the CPU load of the data center by sacrificing part of the hard disk space in the local node.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Vuong H,Nguyen H,Tran L","","A Design of Parallel Content-Defined Chunking System Using Non-Hashing Algorithms on FPGA","IEEE Access","","","","","","","2022","10","","82036-82048","All","","","","","","","","","2022","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2022.3196775;https://ieeexplore.ieee.org/abstract/document/9851445/;https://ieeexplore.ieee.org/iel7/6287639/9668973/09851445.pdf","10.1109/ACCESS.2022.3196775","","","","Content-defined chunking is a common method in many applications such as data deduplication and data synchronization. In recent years, new CDC algorithms using non-hashing methods have been developed, and positive results have been obtained. However, most of the algorithms are developed for single-thread computation on microprocessors. After analyzing some popular CDC algorithms, we observed that the algorithms using the basic sliding window protocol are more feasible to process in parallel. In this work, we proposed a new parallel chunking method that aims for hardware implementation. Additionally, we used the PCI algorithm, which does not include hash functions, to implement a multi-thread chunking system on FPGA devices. By exploiting the strength of the FPGAs, our proposed design achieves not only high computational speed but also great scalability.","Field programmable gate arrays;Random access memory;Urban areas;Software algorithms;Fingerprint recognition;Hash functions;Hardware;FPGA;content-defined chunking;non-hashing method;PCI algorithm;hardware design","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Vuong et al. 2022 - A Design of Parallel Content-Defined Chunking System Using Non-Hashing Algorithms on FPGA.pdf","","","",""
"Journal article","Zhang C,Qi D,Li W,Huang W,Wang X","","SimpleSync: A parallel delta synchronization method based on Flink","Concurr. Comput.","Concurrency and Computation: Practice & Experience","","","","","","2021","33","20","","All;FSL Traces;Thesis","","","","Wiley","","","","","2021-10-25","","","","","1532-0626","1532-0634","https://onlinelibrary.wiley.com/doi/10.1002/cpe.6327;https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6327;http://dx.doi.org/10.1002/cpe.6327","10.1002/cpe.6327","","","","Abstract Cloud storage service has been in full swing in the industry. Delta synchronization technology as a key technology of cloud storage services has not made a key breakthrough. Almost all the existing researches are based on the synchronization process proposed by the Rsync algorithm and mix some optimization appropriately, but the particularity of cloud storage service is not fully considered. This paper proposes a new incremental synchronization method SimpleSync, which makes use of the characteristic that the server does not actively modify the backup files in the cloud storage service, removes the redundant steps in Rsync, and enables the synchronization between the client and the server only through a single communication. Besides, according to the server-side synchronization request processing logic, this paper puts forward the design idea of parallel processing with the Flink framework, to the best of our knowledge, for the first time. After the server receives the synchronization request, SimpleSync first puts it into Kafka for buffering and then uses Flink to process the synchronization request in parallel. In the experimental part, a large number of experiments are designed to compare SimpleSync with other delta synchronization algorithms. Experimental results show that SimpleSync has obvious advantages in synchronization performance. Meanwhile, experiments show that SimpleSync has correctness.","","","http://onlinelibrary.wiley.com/termsAndConditions#vor","School of Computer Science and Engineering South China University of Technology Guangzhou China; School of Information Science and Technology Beijing Forestry University Beijing China","en","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2021 - SimpleSync - A parallel delta synchronization method based on Flink.pdf","","","",""
"Website","Zheng H,Zeng S,Li H,Member,Li Z","","Secure batch deduplication without dual servers in backup system","","","","","","","","","","","","All","","","","","","","","","","","2022-09-06","","","","","https://eprint.iacr.org/2022/1133.pdf","","","","","Cloud storage provides highly available and low cost resources to users. However, as massive amounts of outsourced data grow rapidly, an effective data deduplication scheme is necessary. This is a hot and challenging field, in which there are quite a few researches. However, most of previous works require dual-server fashion to be against brute-force attacks and do not support batch checking. It is not practicable for the massive data stored in the cloud. In this paper, we present a secure batch deduplication scheme for backup system. Besides, our scheme resists the brute-force attacks without the aid of other servers. The core idea of the batch deduplication is to separate users into different groups by using short hashes. Within each group, we leverage group key agreement and symmetric encryption to achieve secure batch checking and semantically secure storage. We also extensively evaluate its performance and overhead based on different datasets. We show that our scheme saves the data storage by up to 89.84%. These results show that our scheme is efficient and scalable for cloud backup system and can also ensure data confidentiality.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zheng et al. - Secure batch deduplication without dual servers in backup system.pdf","","","",""
"Website","Zou X,Xia W,Shilane P,Zhang H,Wang X,Technologies D","","Building a high-performance fine-grained deduplication framework for backup storage with high deduplication ratio","","","","","","","","","","","","Grouped by Publication/Usenix ATC;All","","","","","","","","","","","","","","","","https://www.usenix.org/system/files/atc22-zou.pdf","","","","","Fine-grained deduplication, which first removes identical chunks and then eliminates redundancies between similar but non-identical chunks (i.e., delta compression), could exploit workloads' compressibility to achieve a very high deduplication ratio but suffers from poor backup/restore performance. This makes it not as popular as chunk-level deduplication thus far. This is because allowing workloads to share more references among similar chunks further reduces spatial/temporal locality, causes more I/O overhead, and leads to worse backup/restore performance. In this paper, we address issues for different forms of poor locality with several techniques, and propose MeGA, which achieves backup and restore speed close to chunklevel deduplication while preserving fine-grained deduplication's significant deduplication ratio advantage. Specifically, MeGA applies 1 a backup-workflow-oriented delta selector to address poor locality when reading base chunks, and 2 a delta-friendly data layout and ""Always-Forward-Reference"" traversing in the restore workflow to deal with the poor spatial/temporal locality of deduplicated data. Evaluations on four datasets show that MeGA achieves a better performance than other fine-grained deduplication approaches. In particular, compared with the traditional greedy approach, MeGA achieves a 4.47-34.45× higher backup performance and a 30-105× higher restore performance while maintaining a very high deduplication ratio.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zou et al. - Building a high-performance fine-grained deduplication framework for backup storage with high deduplication ratio.pdf","","","",""
"Journal article","Tan,Wen,Yan,Jiang,Srisa-an,et al.","","FGDEFRAG: A fine-grained defragmentation approach to improve restore performance","Proc. 33th Symp. Mass","","","","","","","2018","","","","All","","","","","","","","","2018-04","","","","","","","https://ranger.uta.edu/~jiang/publication/Conferences/2017/2017-MSST2-FGdefrag-%20A%20Fine-Grained%20Defragmentation%20Approach%20to%20Improve%20Restore%20Performance.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan et al. 2018 - FGDEFRAG - A fine-grained defragmentation approach to improve restore performance.pdf","","","",""
"Journal article","Tan Y,Wang B,Wen J,Yan Z,Jiang H,Srisa-an W","","Improving Restore Performance in Deduplication-Based Backup Systems via a Fine-Grained Defragmentation Approach","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2018","29","10","2254-2267","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","","","","","","","","","2018-10","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2018.2828842;https://ieeexplore.ieee.org/abstract/document/8344493/?casa_token=ScJ8Aa3DDYAAAAAA:DO3yE0A7zJ0kVK3s1ap1ZIH-hS9NdfVAqDD2C1ccHiqUpbh6x3D7DX9NXzxl59tGhedHsDOzXRxF;https://ieeexplore.ieee.org/iel7/71/4359390/08344493.pdf?casa_token=9VP6ojwWbpEAAAAA:NrSiLBfGONkGeHrwPp3r2bjr7kht8u96NT2yuE3LltKIXH3mrvu66hyALG40s3R7f2AgMJaRtxVj","10.1109/TPDS.2018.2828842","","","","In deduplication-based backup systems, the removal of redundant data transforms the otherwise logically adjacent data chunks into physically scattered chunks on the disks. This, in effect, changes the retrieval operations from sequential to random and significantly degrades the performance of restoring data. These scattered chunks are called fragmented data and many techniques have been proposed to identify and sequentially rewrite such fragmented data to new address areas, trading off the increased storage space for reduced number of random reads (disk seeks) to improve the restore performance. However, existing solutions for backup workloads share a common assumption that every read operation involves a large fixed-size window of contiguous chunks, which restricts the fragment identification to a fixed-size read window. This can lead to inaccurate identifications due to false positives since the data fragments can vary in size and appear in any different and unpredictable address locations. Based on these observations, we propose FGDEFRAG, a Fine-Grained defragmentation approach that uses variable-sized and adaptively located data groups, instead of using fixed-size read windows, to accurately identify and effectively remove fragmented data. When we compare its performance to those of existing solutions, FGDEFRAG not only reduces the amount of rewritten data but also significantly improves the restore performance. Our experimental results show that FGDEFRAG can improve the restore performance by 14 to 329 percent, while simultaneously reducing the rewritten data by 25 to 87 percent.","Containers;Bandwidth;Distributed databases;Transforms;Approximation algorithms;Microsoft Windows;Indexes;Data deduplication;defragmentation;deduplication ratio;restore performance","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan et al. 2018 - [no filename]","","","",""
"Journal article","Stefanov E,Van Dijk M,Shi E,Chan TH,Fletcher C,Ren L,Yu X,Devadas S","","Path ORAM: An Extremely Simple Oblivious RAM Protocol","J. ACM","Journal of the ACM","","","","","","2018","65","4","1-26","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","Association for Computing Machinery","New York, NY, USA","","","","2018-04-12","","","","","0004-5411","","https://doi.org/10.1145/3177872;http://dx.doi.org/10.1145/3177872;https://dl.acm.org/doi/abs/10.1145/3177872?casa_token=ML5StAKgImUAAAAA:mQcH-C5VraMlxD222TIb9pasMm_XpWCo_FN7NZa03gP6QNx8-jTJtMu6Plx5Ycb8WZ7deCMCVE8o;https://dl.acm.org/doi/pdf/10.1145/3177872?casa_token=lvs3a8d-lEsAAAAA:yDKF-x5dB4MhVX5u5hlIP5SUT9cX5OJ49DztvjBJtxKfOih7VGxoBJODF1eU2GH6em0GC6iR9K8_","10.1145/3177872","","","","We present Path ORAM, an extremely simple Oblivious RAM protocol with a small amount of client storage. Partly due to its simplicity, Path ORAM is the most practical ORAM scheme known to date with small client storage. We formally prove that Path ORAM has a O(log N) bandwidth cost for blocks of size B = Ω (log2 N) bits. For such block sizes, Path ORAM is asymptotically better than the best-known ORAM schemes with small client storage. Due to its practicality, Path ORAM has been adopted in the design of secure processors since its proposal.","access pattern, Oblivious RAM, ORAM, Path ORAM","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Stefanov et al. 2018 - Path ORAM - An Extremely Simple Oblivious RAM Protocol.pdf","","","18","August 2018"
"Journal article","","","Building-a-High-performance-Fine-grained-Deduplication-Framework-for-Backup-Storage-with-High-Deduplication-Ratio.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://www.researchgate.net/publication/361910229;https://www.researchgate.net/profile/Xiangyu-Zou-4/publication/361910229_Building_a_High-performance_Fine-grained_Deduplication_Framework_for_Backup_Storage_with_High_Deduplication_Ratio/links/62cc3db6cab7ba7426e4c603/Building-a-High-performance-Fine-grained-Deduplication-Framework-for-Backup-Storage-with-High-Deduplication-Ratio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Building-a-High-performance-Fine-grai... - Building-a-High-performance-F ... uplication-Framework-for-Backup-Storage-with-High-Deduplication-Ratio.pdf","","","",""
"Conference paper","Zhang W,Tang H,Jiang H,Yang T,Li X,Zeng Y","","Multi-level Selective Deduplication for VM Snapshots in Cloud Storage","","","","","2012 IEEE Fifth International Conference on Cloud Computing","","","2012","","","550-557","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","Two-stage","","","","","","","","2012-06","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD.2012.78;https://ieeexplore.ieee.org/abstract/document/6253550/;https://sites.cs.ucsb.edu/projects/psc/2012IEEECloud.pdf","10.1109/CLOUD.2012.78","","","","In a virtualized cloud computing environment, frequent snapshot backup of virtual disks improves hosting reliability but storage demand of such operations is huge. While dirty bit-based technique can identify unmodified data between versions, full deduplication with fingerprint comparison can remove more redundant content at the cost of computing resources. This paper presents a multi-level selective deduplication scheme which integrates inner-VM and cross-VM duplicate elimination under a stringent resource requirement. This scheme uses popular common data to facilitate fingerprint comparison while reducing the cost and it strikes a balance between local and global deduplication to increase parallelism and improve reliability. Experimental results show the proposed scheme can achieve high deduplication ratio while using a small amount of cloud resources.","Indexes;Distributed databases;Image segmentation;Memory management;Fault tolerance;Fault tolerant systems;Cloud storage backup;Virtual machine snap-shots;Distributed data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2012 - Multi-level Selective Deduplication for VM Snapshots in Cloud Storage.pdf","","","",""
"Journal article","Zhang Y,Yuan Y,Feng D,Wang C,Wu X,Yan L,Pan D,Wang S","","Improving Restore Performance for In-Line Backup System Combining Deduplication and Delta Compression","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2020","31","10","2302-2314","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","","","","","","","","","2020-10","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2020.2991030;https://ieeexplore.ieee.org/abstract/document/9080096?casa_token=V0F2fj0xMOcAAAAA:KHdcbWmcDvTsBtQP9oYaH3Fqx5ISjoXkPqMrzxp1ffr1HNf8GWYrvHr1pmJrm75B3HZb3auofUInqQ","10.1109/TPDS.2020.2991030","","","","Data deduplication, though being efficient in removing duplicate chunks, introduces chunk fragmentation which decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Delta compression is often used as a complement for data deduplication to further improve storage efficiency. We observe that delta compression introduces a new type of chunk fragmentation stemming from improper delta compression for chunks of which the base chunks are fragmented. The new type of chunk fragmentation severely decreases restore performance and cannot be addressed by existing rewriting algorithms. To address this problem, we propose SDC, a scheme performing post-deduplication delta compression only for the chunks of which the bases can be directly found in the restore cache to eliminate additional disk reads for base chunks, thus avoiding the new type of chunk fragmentation. In addition, self-referenced chunks can be fragmented, which decrease restore performance, and these fragmented chunks can serve as bases to decrease the restore performance repeatedly. We propose a hybrid rewriting scheme for SDC to rewrite such fragmented chunks. Experimental results show that SDC improves the restore performance of the approach that directly performs delta compression after data deduplication by 2.9-16.9x, and achieves more than 95 percent of its compression gains.","Containers;Acceleration;Computer science;Indexes;Measurement;Data storage systems;Redundancy;Data deduplication;delta compression;storage system;chunk fragmentation;restore performance","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2020 - Improving Restore Performance for In-Line Backup System Combining Deduplication and Delta Compression.pdf","","","",""
"Conference paper","Li N,Hao M,Li H,Lin X,Emami T,Gunawi HS","","Fantastic SSD internals and how to learn and use them","","","","","Proceedings of the 15th ACM International Conference on Systems and Storage","","","2022","","","72-84","All","","","","Association for Computing Machinery","New York, NY, USA","","Haifa, Israel","","2022-06-06","","2022-06-29","9781450393805","","","","https://doi.org/10.1145/3534056.3534940;http://dx.doi.org/10.1145/3534056.3534940;https://dl.acm.org/doi/10.1145/3534056.3534940","10.1145/3534056.3534940","","","","This work presents (a) Queenie, an application-level tool that can automatically learn 10 internal properties of block-level SSDs, (b) Kelpie, the learning and analysis results of running Queenie on 21 different SSD models from 7 major SSD vendors, and (c) Newt, a set of storage performance optimization examples that use the learned properties. By bringing numerous observations and unique findings, this work exposes substantial improvement spaces for both SSD users and vendors, enlightening possibilities of unleashing more SSD performance potential and highlighting the necessity of further exploring SSD internals.","performance characterization, solid-state drive","","","","","","SYSTOR '22","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2022 - Fantastic SSD internals and how to learn and use them.pdf","","","",""
"Conference paper","Bae J,Park J,Jun Y,Seo E","","Dedup-for-speed: storing duplications in fast programming mode for enhanced read performance","","","","","Proceedings of the 15th ACM International Conference on Systems and Storage","","","2022","","","128-139","All","","","","Association for Computing Machinery","New York, NY, USA","","Haifa, Israel","","2022-06-06","","2022-06-29","9781450393805","","","","https://doi.org/10.1145/3534056.3534937;http://dx.doi.org/10.1145/3534056.3534937;https://dl.acm.org/doi/10.1145/3534056.3534937","10.1145/3534056.3534937","","","","Storage deduplication improves write latency, increases available space, and reduces the wear of storage media by eliminating redundant writes. The flash translation layer (FTL) of a flash solid state disk (SSD) easily enables deduplication in an SSD through the simple mapping of duplicated logical pages to the same physical page. Therefore, a few deduplicating FTLs have been proposed. However, the deduplication of partially duplicated files breaks the sequentiality of data storage at the flash page level, resulting in a significant degradation of the read performance. Although increasing available storage space, reducing flash write, and extending lifespan are barely perceptible to users, extended read latency is critical to user-perceived performance. In this paper, we propose a novel deduplication FTL called Dedup-for-Speed (DFS). The DFS FTL trades surplus capacity gained through inline deduplication for improved read performance by storing duplicated pages in fast flash modes, such as the pseudo-single-level-cell(pSLC) mode. The flash mode of a page is determined by its degree of deduplications. Duplicate pages are migrated to fast flash blocks during idle intervals to minimize interference with host-issued operations. Contrary to conventional deduplication schemes, DFS improves the read performance while maintaining the aforementioned benefits of deduplication. Our evaluation of six real-world traces showed that DFS improved the read latency by 16% on average and up to 34%. It also enhanced the write latency by 64% on average and up to 82%.","","","","","","","SYSTOR '22","","","","","","","","","","","","","","","","","","All Papers/B/Bae et al. 2022 - Dedup-for-speed - storing duplications in fast programming mode for enhanced read performance.pdf","","","",""
"Miscellaneous","Kim WB,Lee IY","","Client-Side Deduplication for Protection of a Private Data in Cloud Storage","Advanced Science Letters","","","","","","","2016","22","9","2448-2452","All","","","","","","","","","2016","","","","","","","http://dx.doi.org/10.1166/asl.2016.7854","10.1166/asl.2016.7854","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Singh AK","","Efficient Method of Secure Authorized Data Deduplication at Small Block Level Approach in Cloud Storage","Journal of Advanced Research in Dynamical and Control Systems","","","","","","","2020","12","SP7","513-519","All","","","","","","","","","2020","","","","","","","http://dx.doi.org/10.5373/jardcs/v12sp7/20202134","10.5373/jardcs/v12sp7/20202134","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Gode RV,Dalvi R","","An effective storage management in a twin cloud architecture using an authorized deduplication technique","2017 IEEE International Conference on Power, Control, Signals and Instrumentation Engineering (ICPCSI)","","","","","","","2017","","","","All","","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1109/icpcsi.2017.8391823","10.1109/icpcsi.2017.8391823","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Mounika P,CVR College of Engineering,JNTU,Hyderabad,India,CVR College of Engineering,JNTU,Hyderabad,India,Jyothsna . S","","Client-Side Authorized Deduplication In Cloud Using PoW","International Journal of Computer Sciences and Engineering","","","","","","","2017","5","11","120-126","All","","","","","","","","","2017","","","","","","","http://dx.doi.org/10.26438/ijcse/v5i11.120126","10.26438/ijcse/v5i11.120126","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Youn TY,Jho NS,Rhee KH,Shin SU","","Authorized Client-Side Deduplication Using CP-ABE in Cloud Storage","Proc. Int. Wirel. Commun. Mob. Comput. Conf.","Proceedings of the ... International Wireless Communications & Mobile Computing Conference / Association for Computing Machinery. International Wireless Communications & Mobile Computing Conference","","","","","","2019","2019","","","All","","","","Hindawi","","","","","2019-05-15","","2022-06-26","","","1530-8669","1530-8669","https://www.hindawi.com/journals/wcmc/2019/7840917/;http://dx.doi.org/10.1155/2019/7840917","10.1155/2019/7840917","","","","Since deduplication inevitably implies data sharing, control over access permissions in an encrypted deduplication storage is more important than a traditional encrypted storage. Therefore, in terms of flexibility, data deduplication should be combined with data access control techniques. In this paper, we propose an authorized deduplication scheme using CP-ABE to solve this problem. The proposed scheme provides client-side deduplication while providing confidentiality through client-side encryption to prevent exposure of users’ sensitive data on untrusted cloud servers. Also, unlike existing convergent encryption schemes, it provides authorized convergent encryption by using CP-ABE to allow only authorized users to access critical data. The proposed authorized deduplication scheme provides an adequate trade-off between storage space efficiency and security in cloud environment and is very suitable for the hybrid cloud model considering both the data security and the storage efficiency in a business environment.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/Y/Youn et al. 2019 - Authorized Client-Side Deduplication Using CP-ABE in Cloud Storage.pdf","","","",""
"Journal article","Al-Amer A,Ouda O","","Secure and Efficient Proof of Ownership Scheme for Client-Side Deduplication in Cloud Environments","International Journal of Advanced Computer Science and Applications; West Yorkshire","","","","","","","2021","12","12","","All","","","","Science and Information (SAI) Organization Limited","West Yorkshire, United Kingdom, West Yorkshire","","","","2021","","","","","2158-107X","","https://login.ezproxy.lib.bbk.ac.uk/login?url=https://www.proquest.com/scholarly-journals/secure-efficient-proof-ownership-scheme-client/docview/2655113476/se-2;http://dx.doi.org/10.14569/IJACSA.2021.01212111;https://www.proquest.com/docview/2655113476?pq-origsite=gscholar&fromopenview=true","10.14569/IJACSA.2021.01212111","","","","Data deduplication is an effective mechanism that reduces the required storage space of cloud storage servers by avoiding storing several copies of the same data. In contrast with server-side deduplication, client-side deduplication can not only save storage space but also reduce network bandwidth. Client-side deduplication schemes, however, might suffer from serious security threats. For instance, an adversary can spoof the server and gain access to a file he/she does not possess by claiming that she/he owns it. In order to thwart such a threat, the concept of proof-of-ownership (PoW) has been introduced. The security of the existing PoW scheme cannot be assured without affecting the computational complexity of the client-side deduplication. This paper proposes a secure and efficient PoW scheme for client-side deduplication in cloud environments with minimal computational overhead. The proposed scheme utilizes convergent encryption to encrypt a sufficiently large block specified by the server to challenge the client that claims possession of the file requested to be uploaded. To ensure that the client owns the entire file contents, and hence resist collusion attacks, the server challenges the client by requesting him to split the file he asks to upload into fixed-sized blocks and then encrypting a randomly chosen block using a key formed from extracting one bit at a specified location in all other blocks. This ensures a significant reduction in the communication overhead between the server and client. Computational complexity analysis and experimental results demonstrate that the proposed PoW scheme outperforms state-of-the-art PoW techniques.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/A/Al-Amer and Ouda 2021 - Secure and Efficient Proof of Ownership Scheme for Client-Side Deduplication in Cloud Environments.pdf","","","",""
"Journal article","Sehat H,Kloborg AL,Mørup C,Pagnin E,Lucani DE","","Bonsai: A Generalized Look at Dual Deduplication","arXiv [cs.CR]","","","","","","","2022","","","","All","","","","arXiv","","","","","2022-02-28","","2022-06-25","","","","","http://arxiv.org/abs/2202.13925;http://dx.doi.org/10.48550/ARXIV.2202.13925","10.48550/ARXIV.2202.13925","","2202.13925","10.48550/arXiv.2202.13925","Cloud Service Providers (CSPs) offer a vast amount of storage space at competitive prices to cope with the growing demand for digital data storage. Dual deduplication is a recent framework designed to improve data compression on the CSP while keeping clients' data private from the CSP. To achieve this, clients perform lightweight information-theoretic transformations to their data prior to upload. We investigate the effectiveness of dual deduplication, and propose an improvement for the existing state-of-the-art method. We name our proposal Bonsai as it aims at reducing storage fingerprint and improving scalability. In detail, Bonsai achieves (1) significant reduction in client storage, (2) reduction in total required storage (client + CSP), and (3) reducing the deduplication time on the CSP. Our experiments show that Bonsai achieves compression rates of 68\% on the cloud and 5\% on the client, while allowing the cloud to identify deduplications in a time-efficient manner. We also show that combining our method with universal compressors in the cloud, e.g., Brotli, can yield better overall compression on the data compared to only applying the universal compressor or plain Bonsai. Finally, we show that Bonsai and its variants provide sufficient privacy against an honest-but-curious CPS that knows the distribution of the Clients' original data.","","","","","","","","","arXiv","2202.13925","cs.CR","","","","","","","","","","","","","","All Papers/S/Sehat et al. 2022 - Bonsai - A Generalized Look at Dual Deduplication.pdf","","","",""
"Journal article","Miao M,Tian G,Susilo W","","New proofs of ownership for efficient data deduplication in the adversarial conspiracy model","Int. J. Intell. Syst.","International Journal of Intelligent Systems","","","","","","2021","","int.22400","","All;Thesis","PoW","","","Wiley","","","","","2021-03-12","","","","","0884-8173","1098-111X","https://onlinelibrary.wiley.com/doi/10.1002/int.22400;http://dx.doi.org/10.1002/int.22400","10.1002/int.22400","","","","Abstract The primitive of proofs of ownership (PoWs) enables a prover to efficiently convince a verifier that he/she indeed owns a certain message in a knowledge-proof manner. As a result, it can prevent an adversary who only has a short information of the message from accessing the whole one. We argue that the existing PoWs based on Merkle hash tree and specific encodings are not much efficient if the size of message is sufficiently huge. In this paper, we first propose a new PoW protocol based on the chameleon hash function without key exposure. Interestingly, it is equivalent to having the prover compute a new collision of chameleon hashing as the proof in our construction. Therefore, the proposed protocol is much efficient since the computation and storage overhead of proof is independent of the size of the message. Moreover, we utilize the proposed PoWs to design a deduplication scheme over ciphertext.","","","http://onlinelibrary.wiley.com/termsAndConditions#vor","School of Cyberspace Security Xi'an University of Posts and Telecommunications Xi'an China; State Key Laboratory of Integrated Service Networks (ISN) Xidian University Xi'an China; State Key Laboratory of Integrated Service Networks (ISN) Xidian University Xi'an China; School of Computing and Information Technology University of Wollongong Wollongong New South Wales Australia","en","","","","","","","","","","","","","","","","","","","","All Papers/M/Miao et al. 2021 - New proofs of ownership for efficient data deduplication in the adversarial conspiracy model.pdf","","","",""
"Preprint","Ménétrey J,Göttel C,Pasin M,Felber P,Schiavoni V","","An Exploratory Study of Attestation Mechanisms for Trusted Execution Environments","","","","","","arXiv [cs.CR]","","2022","","","","All","","","","","","","","","2022-04-14","","","","","","","http://arxiv.org/abs/2204.06790;http://dx.doi.org/10.48550/arXiv.2204.06790","10.48550/arXiv.2204.06790","","2204.06790","","Attestation is a fundamental building block to establish trust over software systems. When used in conjunction with trusted execution environments, it guarantees that genuine code is executed even when facing strong attackers, paving the way for adoption in several sensitive application domains. This paper reviews existing remote attestation principles and compares the functionalities of current trusted execution environments as Intel SGX, Arm TrustZone and AMD SEV, as well as emerging RISC-V solutions.","","","","","","","","","arXiv","2204.06790","cs.CR","","","","","","","","","","","","","","All Papers/M/Ménétrey et al. 2022 - An Exploratory Study of Attestation Mechanisms for Trusted Execution Environments.pdf","","","",""
"Conference paper","Ren Y,Li J,Yang Z,Lee PP,Zhang X","","Accelerating Encrypted Deduplication via {SGX}","","","","","2021 USENIX Annual Technical Conference (USENIX ATC 21)","","","2021","","","957-971","All;p-scailbib","","","","","","","","","2021","","","9781939133236","","","","https://www.usenix.org/conference/atc21/presentation/ren-yanjing;https://www.usenix.org/system/files/atc21-ren-yanjing.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Ren et al. 2021 - Accelerating Encrypted Deduplication via {SGX}.pdf","","","",""
"Journal article","You W,Lei L,Chen B,Liu L","","What If Keys Are Leaked? towards Practical and Secure Re-Encryption in Deduplication-Based Cloud Storage","Information","Information. An International Interdisciplinary Journal","","","","","","2021","12","4","142","All","","","","Multidisciplinary Digital Publishing Institute","","","","","2021-03-26","","2022-06-22","","","1343-4500","","https://www.mdpi.com/2078-2489/12/4/142;http://dx.doi.org/10.3390/info12040142","10.3390/info12040142","","","","By only storing a unique copy of duplicate data possessed by different data owners, deduplication can significantly reduce storage cost, and hence is used broadly in public clouds. When combining with confidentiality, deduplication will become problematic as encryption performed by different data owners may differentiate identical data which may then become not deduplicable. The Message-Locked Encryption (MLE) is thus utilized to derive the same encryption key for the identical data, by which the encrypted data are still deduplicable after being encrypted by different data owners. As keys may be leaked over time, re-encrypting outsourced data is of paramount importance to ensure continuous confidentiality, which, however, has not been well addressed in the literature. In this paper, we design SEDER, a SEcure client-side Deduplication system enabling Efficient Re-encryption for cloud storage by (1) leveraging all-or-nothing transform (AONT), (2) designing a new delegated re-encryption (DRE), and (3) proposing a new proof of ownership scheme for encrypted cloud data (PoWC). Security analysis and experimental evaluation validate security and efficiency of SEDER, respectively.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/Y/You et al. 2021 - What If Keys Are Leaked - towards Practical and Secure Re-Encryption in Deduplication-Based Cloud Storage.pdf","","","",""
"Conference paper","Yang Z,Li J,Lee PP","","Secure and Lightweight Deduplicated Storage via Shielded Deduplication-Before-Encryption","","","","","2022 USENIX Annual Technical Conference (USENIX ATC 22)","","","2022","","","37-52","All;Grouped by Publication/Usenix ATC;Thesis;p-scailbib","","","","","","","","","2022","","2022-06-18","","","","","https://www.cse.cuhk.edu.hk/~pclee/www/pubs/tech_debe.pdf","","","","","Outsourced storage should fulfill confidentiality and storage efficiency for large-scale data management. Conventional approaches often combine encryption and deduplication based on deduplication-after-encryption (DaE), which first performs encryption followed by deduplication on encrypted data. We argue that DaE has fundamental limitations that lead to various drawbacks in performance, storage savings, and security in secure deduplication systems. In this paper, we study an unexplored paradigm called deduplication-before-encryption (DbE), which first performs deduplication and encrypts only non-duplicate data. DbE has the benefits of mitigating the performance and storage penalties caused by the management of duplicate data, but its deduplication process is no longer protected by encryption. To this end, we design DEBE, a shielded DbE-based deduplicated storage system that protects deduplication via Intel SGX. DEBE builds on frequency-based deduplication that first removes duplicates of frequent data in a space-constrained SGX enclave and then removes all remaining duplicates outside the enclave. Experiments show that DEBE outperforms state-of-the-art DaE approaches.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2022 - Secure and Lightweight Deduplicated Storage via Shielded Deduplication-Before-Encryption.pdf","","","",""
"Journal article","Zou X,Yuan J,Shilane P,Xia W,Zhang H,Wang X","","From Hyper-dimensional Structures to Linear Structures: Maintaining Deduplicated Data’s Locality","ACM Trans. Storage","","","","","","","2022","18","3","1-28","All;Grouped by Publication/ACM TOS","Datasets","","","Association for Computing Machinery","New York, NY, USA","","","","2022-08-24","","","","","1553-3077","","https://doi.org/10.1145/3507921;http://dx.doi.org/10.1145/3507921;https://dl.acm.org/doi/pdf/10.1145/3507921","10.1145/3507921","","","","Data deduplication is widely used to reduce the size of backup workloads, but it has the known disadvantage of causing poor data locality, also referred to as the fragmentation problem. This results from the gap between the hyper-dimensional structure of deduplicated data and the sequential nature of many storage devices, and this leads to poor restore and garbage collection (GC) performance. Current research has considered writing duplicates to maintain locality (e.g., rewriting) or caching data in memory or SSD, but fragmentation continues to lower restore and GC performance.Investigating the locality issue, we design a method to flatten the hyper-dimensional structured deduplicated data to a one-dimensional format, which is based on classification of each chunk’s lifecycle, and this creates our proposed data layout. Furthermore, we present a novel management-friendly deduplication framework, called MFDedup, that applies our data layout and maintains locality as much as possible. Specifically, we use two key techniques in MFDedup: Neighbor-duplicate-focus indexing (NDF) and Across-version-aware Reorganization scheme (AVAR). NDF performs duplicate detection against a previous backup, then AVAR rearranges chunks with an offline and iterative algorithm into a compact, sequential layout, which nearly eliminates random I/O during file restores after deduplication.Evaluation results with five backup datasets demonstrate that, compared with state-of-the-art techniques, MFDedup achieves deduplication ratios that are 1.12× to 2.19× higher and restore throughputs that are 1.92× to 10.02× faster due to the improved data layout. While the rearranging stage introduces overheads, it is more than offset by a nearly-zero overhead GC process. Moreover, the NDF index only requires indices for two backup versions, while the traditional index grows with the number of versions retained.","garbage collection, restore, Fragmentation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zou et al. 2022 - From Hyper-dimensional Structures to Linear Structures - Maintaining Deduplicated Data’s Locality.pdf;All Papers/Z/Zou et al. 2022 - From Hyper-dimensional Structures to Linear Structures - Maintaining Deduplicated Data’s Locality.pdf","","","25","August 2022"
"Journal article","Zuo C,Wang F,Zheng M,Hu Y,Feng D","","Ensuring high reliability and performance with low space overhead for deduplicated and delta‐compressed storage systems","Concurr. Comput.","Concurrency and computation: practice & experience","","","","","","2021","","","","All","","","","Wiley","","","","","2021-11-10","","","","","1532-0626","1532-0634","https://onlinelibrary.wiley.com/doi/10.1002/cpe.6706;http://dx.doi.org/10.1002/cpe.6706","10.1002/cpe.6706","","","","","","Linux Kernel 8KB","http://onlinelibrary.wiley.com/termsAndConditions#vor","Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology Huazhong University of Science and Technology, Ministry of Education of China Wuhan China; Department of Electrical and Computer Engineering Iowa State University Ames Iowa USA","en","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zuo et al. 2021 - Ensuring high reliability and performance with low space overhead for deduplicated and delta‐compressed storage systems.pdf","","","",""
"Conference paper","Li W,Jean-Baptise G,Riveros J,Narasimhan G,Zhang T,Zhao M","","CacheDedup: In-line Deduplication for Flash Caching","","","","","14th USENIX Conference on File and Storage Technologies (FAST 16)","","","2016","","","301-314","All","","","","","","","","","2016","","","","","","","https://www.usenix.org/conference/fast16/technical-sessions/presentation/li-wenji;https://www.usenix.org/system/files/conference/fast16/fast16-papers-li-wenji.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2016 - CacheDedup - In-line Deduplication for Flash Caching.pdf","","","",""
"Conference paper","Kim H,Yeom HY,Son Y","","An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging","","","","","2020 IEEE 13th International Conference on Cloud Computing (CLOUD)","","","2020","","","405-413","All","","","","","","","","","2020-10","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD49709.2020.00062;https://ieeexplore.ieee.org/abstract/document/9284224/?casa_token=S6Q3zw4-nxIAAAAA:piiiYKJ3YPDz16AB6Nn-QZMGz5g2S7HISBnpUNPdWwynVifeKPADKBFQINEs5n_XRWqHPY2aK5ojSQ;https://ieeexplore.ieee.org/iel7/9284144/9284201/09284224.pdf?casa_token=dkBCHaZNy6cAAAAA:w_h7iHqKmFm67bjMlIZIEJ0xVCaYjhhJjF11IVtDAS3oYzSPBSGA9I2AUOTz0tn8475jWf03mRpvAg","10.1109/CLOUD49709.2020.00062","","","","Many cloud services perform periodic database backup to keep the data safe from failures such as sudden system crashes. In the database system, two techniques are widely used for data backup and recovery: a physical backup and a logical backup. The physical backup uses raw data by copying the files in the database, whereas the logical backup extracts data from the database and dumps it into separated files as a sequence of query statements. Both techniques support a full backup strategy that contains data of the entire database and incremental backup strategy that contains changed data since a previous backup. However, both strategies require additional I/O operations to perform the backup and need a long time to restore a backup. In this paper, we propose an efficient backup and recovery scheme by exploiting write-ahead logging (WAL) in database systems. In the proposed scheme, for backup, we devise a backup system to use log data generated by the existing WAL to eliminate the additional I/O operations. To restore a backup, we utilize and optimize the existing crash recovery procedure of WAL to reduce recovery time. For example, we divide the recovery range and applying the backup data for each range independently via multiple threads. We implement our scheme in MySQL, a popular database management system. The experimental result demonstrates that the proposed scheme provides instant backup while reducing recovery time compared with the existing schemes.","Cloud computing;Instruction sets;Conferences;Database systems;Computer crashes;Data mining;Database Management System;Data Management;Database Backup and Recovery;Write-Ahead Logging","Cites Sorted Deduplication but just says it can be used to backup database data. What?","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2020 - An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging.pdf","","","",""
"Journal article","Süß,Kaya,Mäsker,Brinkmann","","Deduplication analyses of multimedia system images","USENIX Workshop on Hot","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://www.usenix.org/conference/hotedge18/presentation/suess;https://www.usenix.org/system/files/conference/hotedge18/hotedge18-papers-suess.pdf","","","","","","","Authors self-citing.","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Süß et al. - Deduplication analyses of multimedia system images.pdf","","","",""
"Conference paper","Wildani A,Miller EL,Rodeh O","","HANDS: A heuristically arranged non-backup in-line deduplication system","","","","","2013 IEEE 29th International Conference on Data Engineering (ICDE)","","","2013","","","","All","","","","IEEE","","2013 29th IEEE International Conference on Data Engineering (ICDE 2013)","Brisbane, QLD","2013/4/8-2013/4/12","2013-04","","2022-05-26","9781467349109","9781467349093","","","http://www.mathcs.emory.edu/~avani/wildani-icde13dedup.pdf;http://ieeexplore.ieee.org/document/6544846/;http://dx.doi.org/10.1109/icde.2013.6544846","10.1109/icde.2013.6544846","","","","Deduplicating in-line data on primary storage is hampered by the disk bottleneck problem, an issue which results from the need to keep an index mapping portions of data to hash values in memory in order to detect duplicate data without paying the performance penalty of disk paging. The index size is proportional to the volume of unique data, so placing the entire index into RAM is not cost effective with a deduplication ratio below 45%. HANDS reduces the amount of in-memory index storage required by up to 99% while still achieving between 30% and 90% of the deduplication a full memory-resident index provides, making primary deduplication cost effective in workloads with deduplication rates as low as 8%. HANDS is a framework that dynamically pre-fetches fingerprints from disk into memory cache according to working sets statistically derived from access patterns. We use a simple neighborhood grouping as our statistical technique to demonstrate the effectiveness of our approach. HANDS is modular and requires only spatio-temporal data, making it suitable for a wide range of storage systems without the need to modify host file systems.","","Cites Sorted deduplication but just says it's not appropriate for primary storage deduplication.","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wildani et al. 2013 - HANDS - A heuristically arranged non-backup in-line deduplication system.pdf","","","",""
"Journal article","Williams JW","","Algorithm 232","Communications of the ACM","","","","","","","1964","","","","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","1964","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Yan Z,Zhang L,Ding W,Zheng Q","","Heterogeneous Data Storage Management with Deduplication in Cloud Computing","IEEE Transactions on Big Data","","","","","","","2019","5","3","393-407","All","","","","","","","","","2019","","","","","","","http://dx.doi.org/10.1109/tbdata.2017.2701352","10.1109/tbdata.2017.2701352","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yan et al. 2019 - Heterogeneous Data Storage Management with Deduplication in Cloud Computing.pdf","","","",""
"Conference paper","Islam MS,Kuzu M,Kantarcioglu M","","Access pattern disclosure on searchable encryption: Ramification, attack and mitigation","","","","","Network and Distributed System Security Symposium (NDSS)","","","2012","","","1-12","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","2012","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/I/Islam et al. 2012 - Access pattern disclosure on searchable encryption - Ramification, attack and mitigation.pdf","","","",""
"Conference paper","Vestergaard R,Zhang Q,Lucani DE","","CIDER: A Low Overhead Approach to Privacy Aware Client-side Deduplication","","","","","GLOBECOM 2020 - 2020 IEEE Global Communications Conference","","","2020","","","1-6","All","client-side deduplication","","","","","","","","2020-12","","","","","2576-6813","","http://dx.doi.org/10.1109/GLOBECOM42002.2020.9348272","10.1109/GLOBECOM42002.2020.9348272","","","","In cloud storage systems, malicious users may exploit client-side deduplication responses to infer what other users are storing in the cloud. We propose CIDER, a low overhead approach to mitigate this side channel by obfuscating the existence status of data stored in the cloud. We analyze the scheme's ability to obfuscate the side-channel and discuss attacks that a user may still employ and what he could learn from such attacks. Finally, we use simulated and real data to examine the performance under realistic deduplication workloads, revealing that CIDER ends up transmitting less redundant information than similar methods, thus enabling a more efficient utilization of the available bandwidth.","Cloud computing;Privacy;Data privacy;Conferences;Data models;Global communication;Data communication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Vestergaard et al. 2020 - CIDER - A Low Overhead Approach to Privacy Aware Client-side Deduplication.pdf","","","",""
"Miscellaneous","Xia W,Wei C,Li Z,Wang X,Zou X","","NetSync: a Network Adaptive and Deduplication-Inspired Delta Synchronization Approach for Cloud Storage Services","IEEE Transactions on Parallel and Distributed Systems","","","","","","","2022","","","1-1","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","","","","","","","","","2022","","","","","","","http://dx.doi.org/10.1109/tpds.2022.3145025","10.1109/tpds.2022.3145025","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2022 - NetSync - a Network Adaptive and Deduplication-Inspired Delta Synchronization Approach for Cloud Storage Services.pdf","","","",""
"Conference paper","He Y,Xiang L,Xia W,Jiang H,Li Z,Wang X,Zou X","","Dsync: a lightweight delta synchronization approach for cloud storage services","","","","","36th Symposium on Mass Storage Systems and Technologies (MSST'20)","","","2020","","","","Grouped by Publication/MSST;All","","","","","","","","","2020","","","","","","","http://www.greenorbs.org/people/lzh/papers/[MSST'20]%20DSync.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/He et al. 2020 - Dsync - a lightweight delta synchronization approach for cloud storage services.pdf","","","",""
"Journal article","Shell M,Member","","How to Use the IEEEtran LTEX Class","","","","","","","","","","","","All","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shell and Member - How to Use the IEEEtran LTEX Class.pdf","","","",""
"Conference paper","Kachmar M,Kaeli D","","CALC: A Content-Aware Learning Cache for Storage Systems","","","","","2021 IEEE International Conference on Networking, Architecture and Storage (NAS)","","","2021","","","1-8","Grouped by Publication/NAS;All","","","","","","","","","2021-10","","","","","","","http://dx.doi.org/10.1109/NAS51552.2021.9605381","10.1109/NAS51552.2021.9605381","","","","In today’s enterprise storage systems, supported services such as data deduplication are becoming a common feature adopted in the data center, especially as new storage technologies mature. Static partitioning of storage system resources, including CPU cores and memory caches, may lead to missing Service Level Agreement (SLAs) thresholds, such as the Data Reduction Rate (DRR) or IO latency. However, typical storage system applications exhibit a workload pattern that can be learned. By learning these pattern, we are better equipped to address several storage system resource partitioning challenges, issues that cannot be overcome with traditional manual tuning and primitive feedback mechanisms.We propose a Content-Aware Learning Cache (CALC) that uses online reinforcement learning models (Q-Learning, SARSA and Actor-Critic) to actively partition the storage system cache between a data digest cache, content cache, and address-based data cache to improve cache hit performance, while maximizing data reduction rates. Using traces from popular storage applications, we show how our machine learning approach is robust and can out-perform an iterative search method for various datasets and cache sizes. Our content-aware learning cache improves hit rates by 7.1% when compared to iterative search methods, and 18.2% when compared to traditional LRU-based data cache implementation.","Data centers;Search methods;Conferences;Reinforcement learning;Cache storage;Data models;Iterative methods;enterprise storage systems;data reduction;machine learning;performance guarantees;DRR;QoS","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kachmar and Kaeli 2021 - CALC - A Content-Aware Learning Cache for Storage Systems.pdf","","","",""
"Conference paper","Zuo C,Wang F,Huang P,Hu Y,Feng D","","RepEC-Duet: Ensure High Reliability and Performance for Deduplicated and Delta-Compressed Storage Systems","","","","","2019 IEEE 37th International Conference on Computer Design (ICCD)","","","2019","","","190-198","All","","","","","","","","","2019-11","","","","","2576-6996","","http://dx.doi.org/10.1109/ICCD46524.2019.00032;https://ieeexplore.ieee.org/abstract/document/8988716/?casa_token=0Dqts4nF4UoAAAAA:nkmnTF-n1hK0vJuCGiCvOOM-zn8-ZCk4zHJTJNnK8LMXlFTm8FKB2WWJUjMvkWBJJM-eNUJVjIOpgA;https://ieeexplore.ieee.org/iel7/8970097/8988587/08988716.pdf?casa_token=4-MeP6oHwMMAAAAA:dJI1uIhNc8MKXi9ilntyXTYQ9Vp7AIe6FxqnX-J91V6cKRjDPvynBkm3NpsHohnLMHgZHM4n29EqgQ","10.1109/ICCD46524.2019.00032","","","","Data deduplication is a widely deployed technique to remove duplicate content to save storage space, which is however incapable of eliminating the redundancy between nonidentical but similar data blocks. To achieve further space savings in deduplicated storage systems, delta compression is employed to compress post-deduplication data. Both deduplication and delta compression introduce content references among blocks, which inevitably undermines the reliability of deduplicated and delta compressed storage systems. To ensure better reliability, existing approaches utilize either replication or erasure codes to redundantly distribute data across multiple nodes. In deduplicated and delta compressed storage systems, we observe that delta compressed chunks (DCCs) are far smaller than regular chunks called non-DCCs. Motivated by this observation, we suggest a straightforward approach in which replication is used to protect DCCs and erasure code is deployed to protect non-DCCs. However, we need to address two critical challenges to ensure this solution effective. First, the random placement of DCCs replicas destroys cache locality. Second, the separate and individual recovery and restore cache could cause storage containers to be accessed repeatedly. To address these two challenges, in this paper, we propose RepEC-Duet which employs both replication and erasure codes to ensure high reliability and performance for deduplicated and delta-compressed storage systems. RepEC-Duet introduces a delta-utilization-aware filter to select and replicate containers based on the percentage of DCCs in the containers to maintain cache locality. Moreover, to avoid unnecessary container reads, we design a cooperative cache scheme that is aware of both failure recovery and regular restore cache. Our experimental results based on three real-world datasets demonstrate that RepEC-Duet significantly improves the restore performance by 26%-59%, and reduces the storage overhead by 54%-98% than the existing approaches.","Containers;Distributed databases;Decoding;Reliability engineering;Redundancy;System performance;Data deduplication, Delta compression, Reliability, Restore performance","Doesn't compare with RMD, only mentions it as dedup technology.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zuo et al. 2019 - RepEC-Duet - Ensure High Reliability and Performance for Deduplicated and Delta-Compressed Storage Systems.pdf","","","",""
"Conference paper","Dagnaw G,Hua W,Zhou K","","CACH-Dedup: Content Aware Clustered and Hierarchical Deduplication","","","","","2018 IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS)","","","2018","","","399-407","All","","","","","","","","","2018-12","","","","","1521-9097","","http://dx.doi.org/10.1109/PADSW.2018.8644884;https://ieeexplore.ieee.org/abstract/document/8644884/?casa_token=e2Qx8vCViQgAAAAA:xmlUUkF_I3PZ59jWaX22Rprxpo9vrdLcAsb6gAHRSXkiEbPGuueBHQolUw0xJaTY0tLKApy431R0HA;https://ieeexplore.ieee.org/iel7/8635632/8644527/08644884.pdf?casa_token=QGVFYf6A6p8AAAAA:u6K2oQsOnOhRopvLL44Oe4Dn6FRYp0U-cep5ozlN5iVCvNNx_z8CSehxVPzkTwOYeSc39qMFI_uSYw","10.1109/PADSW.2018.8644884","","","","Distributed deduplication overcomes, to some extent, index-lookup disk bottleneck problem by dividing deduplication tasks among many nodes. However, the task of selecting these nodes is an important challenge because it could result in high communication cost and the storage node island effect problem. Moreover, intelligent data routing is required to exploit the peculiar nature of data from different applications which share insignificant amount of content. In this paper, we explore CACH-Dedup, a content aware clustered and hierarchical deduplication system, which exploits the negligibly small amount of content shared among chunks from different file types to create groups of files and storage nodes with out loss of deduplication effectiveness. It uses hierarchical deduplication to reduce the size of fingerprint indexes at the global level, where only files and big sized segments are deduplicated. It also makes advantage of locality first using the big sized segments deduplicated at the global level and second by routing a set of consecutive files together to one storage node. Furthermore, it exploits similarity by making use of similarity bloom filters of streams for stateful routing which results in duplicate elimination rate in a par with single node deduplication with a minimal cost of computation and communication. CACH-Dedup is evaluated using a prototype deployed on windows server environment distributed over four separate machines. It is shown to have duplicate elimination effectiveness in a par with a single node deduplication system, with a minimal communication overhead and an acceptable deduplication throughput.","Conferences;Clustered deduplication;Content Aware Grouping;Hierarchical deduplication;Data routing;Similarity bloom filters","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dagnaw et al. 2018 - CACH-Dedup - Content Aware Clustered and Hierarchical Deduplication.pdf","","","",""
"Miscellaneous","Chen SH,Liang YP,Chang YH,Wei HW,Shih WK","","Boosting the Profitability of NVRAM-based Storage Devices via the Concept of Dual-Chunking Data Deduplication","2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC)","","","","","","","2020","","","","All","","","","","","","","","2020","","","","","","","http://dx.doi.org/10.1109/asp-dac47756.2020.9045622","10.1109/asp-dac47756.2020.9045622","","","","","","Dual-Chunk refers to fixed chunking and variable chunking.","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chen et al. 2020 - Boosting the Profitability of NVRAM-based Storage Devices via the Concept of Dual-Chunking Data Deduplication.pdf","","","",""
"Conference paper","Qin Y,Zhang X,Lilja DJ","","PBCCF: Accelerated Deduplication by Prefetching Backup Content Correlated Fingerprints","","","","","2020 IEEE 38th International Conference on Computer Design (ICCD)","","","2020","","","146-154","All","","","","","","","","","2020-10","","","","","2576-6996","","http://dx.doi.org/10.1109/ICCD50377.2020.00038","10.1109/ICCD50377.2020.00038","","","","Deduplication provides significant benefits for accelerating large-scale storage systems, particularly backup systems, by eliminating the redundancy of the streaming data. Given the extraordinary growth of data, modern deduplication backup systems are challenged with the task of effectively and efficiently identifying data duplicates while having limited memory for fingerprint indexing. Based on our observation about an enterprise backup system, for the newly created client, there are no historical backups so that the prefetching algorithm has no reference basis to perform effective fingerprint prefetching. The generic prefetching approach such as Progressive Sampling requires large memory to maintain the prefetching performance. In our paper, we discovered the backup content correlation exists among the backups from some different clients based on the study of the real-world dataset. We propose a fingerprint prefetching algorithm, prefetching backup content correlated fingerprint (PBCCF) to improve the prefetching performance, by applying lightweight machine learning and statistical techniques to discover the backup patterns and generalize their features only using the high-level meta data. The experimental results reveal that PBCCF succeeds at identifying the highly correlated backups and fingerprints to maintain a good deduplication rate while significantly saving memory compared to the Progressive Sampling.","Machine learning algorithms;Correlation;Prefetching;Memory management;Fingerprint recognition;Metadata;Acceleration;Deduplication;Backup systems;Fingerprint prefetching;Partial indexing;Machine learning","Machine Learning and statistical techniques.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Q/Qin et al. 2020 - PBCCF - Accelerated Deduplication by Prefetching Backup Content Correlated Fingerprints.pdf","","","",""
"Conference paper","Zhang Z,Hu H,Xue Z,Chen C,Yu Y,Fu C,Zhou X,Li F","","SLIMSTORE: A Cloud-based Deduplication System for Multi-version Backups","","","","","2021 IEEE 37th International Conference on Data Engineering (ICDE)","","","2021","","","1841-1846","All","","","","","","","","","2021-04","","","","","2375-026X","","http://dx.doi.org/10.1109/ICDE51399.2021.00164;https://ieeexplore.ieee.org/abstract/document/9458636/?casa_token=BIMuFp0M3LAAAAAA:9rqsXzZiltDF6Qe6Iepjlug5GkDLtGsJemOO972zL2bvO1T6yaxYR0kpxBC0F41BbmMJrh_5MFkBaA;https://ieeexplore.ieee.org/iel7/9458599/9458600/09458636.pdf?casa_token=my_vaPVpWVEAAAAA:rst14BVlxyx12jwKSbOIAXWsrh50QGvqaWkPonQXs6mPxpfuLfjwHjzj1KKqdEo-O6Ef3ffoe4XEWA","10.1109/ICDE51399.2021.00164","","","","Cloud backup is becoming the preferred way for users to support disaster recovery. In addition to its convenience, users are deeply concerned about reducing storage costs in the face of large-scale backup data. Data deduplication is an effective method for backup storage. However, current deduplicate methods lack the utilization of cloud resources to provide scalable backup service for cloud backup users, and cannot meet the biased preference for different backup versions. For new backup versions, users want higher deduplicate and restore speed to reduce the waiting time. Conversely, reducing storage costs is more necessary for old backup versions.In this paper, we present SLIMSTORE, with a cloud-based deduplication architecture that disassembles the system into a storage layer and a computing layer to support elastic utilization of cloud resources. We propose two types of processing nodes with different design focuses to meet the needs of cloud-based backup. The L-node exploits locality and similarity, and adopts a history-aware strategy to provide fast online deduplication service. L-node also optimizes online restoration to realize high restore efficiency. Meanwhile, the G-node provides exact deduplication offline for the old versions, and helps the restore performance of the new versions by optimizing their physical storage. We compare SLIMSTORE with some state-of-art deduplicate and restore methods. Experimental results show that SLIMSTORE can achieve fast deduplication, efficient restoration, and effective space reduction. Furthermore, SLIMSTORE attains scalable deduplication and restoration.","Conferences;Computer architecture;Data engineering;Faces;n/a","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2021 - SLIMSTORE - A Cloud-based Deduplication System for Multi-version Backups.pdf","","","",""
"Journal article","Yang R,Deng Y,Zhou Y,Huang P","","Boosting the Restoring Performance of Deduplication Data by Classifying Backup Metadata","ACM/IMS Trans. Data Sci.","","","","","","","2021","2","2","1-16","All","Restore optimization ","","","Association for Computing Machinery","New York, NY, USA","","","","2021-04-21","","","","","2691-1922","","https://doi.org/10.1145/3437261;http://dx.doi.org/10.1145/3437261","10.1145/3437261","","","","Restoring data is the main purpose of data backup in storage systems. The fragmentation issue, caused by physically scattering logically continuous data across a variety of disk locations, poses a negative impact on the restoring performance of a deduplication system. Rewriting algorithms are used to alleviate the fragmentation problem by improving the restoring speed of a deduplication system. However, rewriting methods give birth to a big sacrifice in terms of deduplication ratio, leading to a huge storage space waste. Furthermore, traditional backup approaches treat file metadata and chunk metadata as the same, which causes frequent on-disk metadata accesses. In this article, we start by analyzing storage characteristics of backup metadata. An intriguing finding shows that with 10 million files, the file metadata merely takes up approximately 340 MB. Motivated by this finding, we propose a Classified-Metadata based Restoring method (CMR) that classifies backup metadata into file metadata and chunk metadata. Because the file metadata merely takes up a meager amount of space, CMR maintains all file metadata in memory, whereas chunk metadata are aggressively prefetched to memory in a greedy manner. A deduplication system with CMR in place exhibits three salient features: (i) It avoids rewriting algorithms’ additional overhead by reducing the number of disk reads in a restoring process, (ii) it increases the restoring throughput without sacrificing the deduplication ratio, and (iii) it thoroughly leverages the hardware resources to boost the restoring performance. To quantitatively evaluate the performance of CMR, we compare our CMR against two state-of-the-art approaches, namely, a history-aware rewriting method (HAR) and a context-based rewriting scheme (CAP). The experimental results show that compared to HAR and CAP, CMR reduces the restoring time by 27.2% and 29.3%, respectively. Moreover, the deduplication ratio is improved by 1.91% and 4.36%, respectively.","fragmentation, data backup, data restoring, metadata, Data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2021 - Boosting the Restoring Performance of Deduplication Data by Classifying Backup Metadata.pdf","","","13","May 2021"
"Conference paper","Nam YJ,Park D,Du DH","","Assuring Demanded Read Performance of Data Deduplication Storage with Backup Datasets","","","","","2012 IEEE 20th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems","","","2012","","","201-208","All","Restore optimization ","","","","","","","","2012-08","","","","","2375-0227","","http://dx.doi.org/10.1109/MASCOTS.2012.32","10.1109/MASCOTS.2012.32","","","","Data deduplication has been widely adopted in contemporary backup storage systems. It not only saves storage space considerably, but also shortens the data backup time significantly. Since the major goal of the original data deduplication lies in saving storage space, its design has been focused primarily on improving write performance by removing as many duplicate data as possible from incoming data streams. Although fast recovery from a system crash relies mainly on read performance provided by deduplication storage, little investigation into read performance improvement has been made. In general, as the amount of deduplicated data increases, write performance improves accordingly, whereas associated read performance becomes worse. In this paper, we newly propose a deduplication scheme that assures demanded read performance of each data stream while achieving its write performance at a reasonable level, eventually being able to guarantee a target system recovery time. For this, we first propose an indicator called cache aware Chunk Fragmentation Level (CFL) that estimates degraded read performance on the fly by taking into account both incoming chunk information and read cache effects. We also show a strong correlation between this CFL and read performance in the backup datasets. In order to guarantee demanded read performance expressed in terms of a CFL value, we propose a read performance enhancement scheme called selective duplication that is activated whenever the current CFL becomes worse than the demanded one. The key idea is to judiciously write non-unique (shared) chunks into storage together with unique chunks unless the shared chunks exhibit good enough spatial locality. We quantify the spatial locality by using a selective duplication threshold value. Our experiments with the actual backup datasets demonstrate that the proposed scheme achieves demanded read performance in most cases at the reasonable cost of write performance.","Containers;Throughput;Correlation;Indexing;Monitoring;Educational institutions;data deduplication;storage;read performance","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Nam et al. 2012 - Assuring Demanded Read Performance of Data Deduplication Storage with Backup Datasets.pdf","","","",""
"Conference paper","Deng C,Chen Q,Zou X,Xia W","","imDedup: a Lossless Deduplication Scheme to Eliminate Fine-grained Redundancy among Images","","","","","The 38th IEEE International Conference on Data Engineering (ICDE '22)","","","2022","","","","All","Similarity/Resemblance","","","unknown","","","","","2022-01-01","","2022-04-22","","","","","http://dx.doi.org/;https://www.researchgate.net/publication/359685410_imDedup_a_Lossless_Deduplication_Scheme_to_Eliminate_Fine-grained_Redundancy_among_Images;https://www.researchgate.net/profile/Xiangyu-Zou-4/publication/359685410_imDedup_a_Lossless_Deduplication_Scheme_to_Eliminate_Fine-grained_Redundancy_among_Images/links/6247ff0257084c718b7f01f5/imDedup-a-Lossless-Deduplication-Scheme-to-Eliminate-Fine-grained-Redundancy-among-Images.pdf;https://www.researchgate.net/publication/359685410","","","","","Images occupy a large amount of storage in data centers. To cope with the explosive growth of the image storage requirement, image compression techniques are devised to shrink the size of every single image at first. Furthermore, image deduplication methods are proposed to reduce the storage cost as they could be used to eliminate redundancy among images. However, state-of-the-art image deduplication methods either can only eliminate file-level coarse-grained redundancy or cannot guarantee lossless deduplication. In this work, we propose a new lossless image deduplication framework to eliminate fine-grained redundancy among images. It first decodes images to expose similarity, then eliminates fine-grained redundancy on the decoded data by delta compression , and finally re-compresses the remaining data by image compression encoding. Based on this framework, we propose a novel lossless similarity-based deduplication (SBD) scheme for decoded image data (called imDedup). Specifically, imDedup uses a novel and fast sampling method (called Feature Map) to detect similar images in a two-dimensional way, which greatly reduces computation overhead. Meanwhile, it uses a novel delta encoder (called Idelta) which incorporates image compression encoding characteristics into deduplication to guarantee the remaining deduplicated image data to be friendly re-compressed via image encoding, which significantly improves the compression ratio. We implement a prototype of imDedup for JPEG images, and demonstrate its superiority on four datasets: Compared with exact image deduplication, imDedup achieves a 19%-38% higher compression ratio by efficiently eliminating fine-grained redundancy. Compared with the similarity detector and delta encoder of state-of-the-art SBD schemes running on the decoded image data, imDedup achieves a 1.8×-3.4× higher throughput and a 1.3×-1.6× higher compression ratio, respectively.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Deng et al. 2022 - imDedup - a Lossless Deduplication Scheme to Eliminate Fine-grained Redundancy among Images.pdf;All Papers/D/Deng et al. 2022 - imDedup - a Lossless Deduplication Scheme to Eliminate Fine-grained Redundancy among Images.pdf","","","",""
"Journal article","Hurst A,Lucani DE,Assent I,Zhang Q","","GLEAN: Generalized Deduplication Enabled Approximate Edge Analytics","IEEE Internet of Things Journal","","","","","","","2022","","","1-1","All","Datasets","","","","","","","","undefined 2022","","","","","2327-4662","","http://dx.doi.org/10.1109/JIOT.2022.3166455","10.1109/JIOT.2022.3166455","","","","The Internet of Things (IoT) has brought about exponential growth in sensor data. This has led to increasing demands for efficient and novel data transmission, storage and analytics solutions for sustainable IoT ecosystems. It has been shown that the Generalized Deduplication (GD) compression algorithm offers not only competitive compression ratio and throughput, but also random access properties that enable direct analytics of compressed data. In this paper, we thoroughly stresstest existing methods for direct analytics of GD compressed data with a diverse collection of 103 datasets, identify the need to optimise GD for analytics and develop a new version of GD to this end. We also propose the Generalized Deduplication Enabled Approximate Edge Analytics (GLEAN) framework. This framework applies the aforementioned analytics techniques at the Edge server to deliver end-to-end lossless data compression and highquality Edge analytics in the IoT, thereby addressing challenges related to data transmission, storage and analytics. Impressive analytics performance was achieved using this framework, with a median increase in k-means clustering error of just 2% relative to analytics performed on uncompressed data, while running 7.5x faster and requiring 3.9x less storage at the Edge server compared to universal compressors.","Internet of Things;Servers;Data analysis;Cloud computing;Data compression;Edge computing;Task analysis;approximate analytics;Edge Computing;data compression;Internet of Things.","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hurst et al. 2022 - GLEAN - Generalized Deduplication Enabled Approximate Edge Analytics.pdf","","","",""
"Journal article","Zhang D,Deng Y,Zhou Y,Zhu Y,Qin X","","Improving the Performance of Deduplication-Based Backup Systems via Container Utilization Based Hot Fingerprint Entry Distilling","ACM Trans. Storage","","","","","","","2021","17","4","1-23","SCAIL Bibliography;Grouped by Publication/ACM TOS;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All;FSL Traces","Restore optimization ;Datasets","","","Association for Computing Machinery","New York, NY, USA","","","","2021-10-16","","","","","1553-3077","","https://doi.org/10.1145/3459626;http://dx.doi.org/10.1145/3459626","10.1145/3459626","","","","Data deduplication techniques construct an index consisting of fingerprint entries to identify and eliminate duplicated copies of repeating data. The bottleneck of disk-based index lookup and data fragmentation caused by eliminating duplicated chunks are two challenging issues in data deduplication. Deduplication-based backup systems generally employ containers storing contiguous chunks together with their fingerprints to preserve data locality for alleviating the two issues, which is still inadequate. To address these two issues, we propose a container utilization based hot fingerprint entry distilling strategy to improve the performance of deduplication-based backup systems. We divide the index into three parts: hot fingerprint entries, fragmented fingerprint entries, and useless fingerprint entries. A container with utilization smaller than a given threshold is called a sparse container. Fingerprint entries that point to non-sparse containers are hot fingerprint entries. For the remaining fingerprint entries, if a fingerprint entry matches any fingerprint of forthcoming backup chunks, it is classified as a fragmented fingerprint entry. Otherwise, it is classified as a useless fingerprint entry. We observe that hot fingerprint entries account for a small part of the index, whereas the remaining fingerprint entries account for the majority of the index. This intriguing observation inspires us to develop a hot fingerprint entry distilling approach named HID. HID segregates useless fingerprint entries from the index to improve memory utilization and bypass disk accesses. In addition, HID separates fragmented fingerprint entries to make a deduplication-based backup system directly rewrite fragmented chunks, thereby alleviating adverse fragmentation. Moreover, HID introduces a feature to treat fragmented chunks as unique chunks. This feature compensates for the shortcoming that a Bloom filter cannot directly identify certain duplicated chunks (i.e., the fragmented chunks). To take full advantage of the preceding feature, we propose an evolved HID strategy called EHID. EHID incorporates a Bloom filter, to which only hot fingerprints are mapped. In doing so, EHID exhibits two salient features: (i) EHID avoids disk accesses to identify unique chunks and the fragmented chunks; (ii) EHID slashes the false positive rate of the integrated Bloom filter. These salient features push EHID into the high-efficiency mode. Our experimental results show our approach reduces the average memory overhead of the index by 34.11% and 25.13% when using the Linux dataset and the FSL dataset, respectively. Furthermore, compared with the state-of-the-art method HAR, EHID boosts the average backup throughput by up to a factor of 2.25 with the Linux dataset, and EHID reduces the average disk I/O traffic by up to 66.21% when it comes to the FSL dataset. EHID also marginally improves the system's restore performance.","storage system, disk bottleneck, data backup, data fragmentation, Data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2021 - Improving the Performance of Deduplication-Based Bac ... tems via Container Utilization Based Hot Fingerprint Entry Distilling.pdf","","","30","November 2021"
"Journal article","Fu Y,Xiao N,Jiang H,Hu G,Chen W","","Application-Aware Big Data Deduplication in Cloud Environment","IEEE Transactions on Cloud Computing","","","","","","","2019","7","4","921-934","All;Thesis;p-scailbib","Application Aware;Super-chunking;Similarity/Resemblance;Routing with Superchunks","","","","","","","","2019-10","","","","","2168-7161","","http://dx.doi.org/10.1109/TCC.2017.2710043;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7936577","10.1109/TCC.2017.2710043","","","","Deduplication has become a widely deployed technology in cloud data centers to improve IT resources efficiency. However, traditional techniques face a great challenge in big data deduplication to strike a sensible tradeoff between the conflicting goals of scalable deduplication throughput and high duplicate elimination ratio. We propose AppDedupe, an application-aware scalable inline distributed deduplication framework in cloud environment, to meet this challenge by exploiting application awareness, data similarity and locality to optimize distributed deduplication with inter-node two-tiered data routing and intra-node application-aware deduplication. It first dispenses application data at file level with an application-aware routing to keep application locality, then assigns similar application data to the same storage node at the super-chunk granularity using a handprinting-based stateful data routing scheme to maintain high global deduplication efficiency, meanwhile balances the workload across nodes. AppDedupe builds application-aware similarity indices with super-chunk handprints to speedup the intra-node deduplication process with high efficiency. Our experimental evaluation of AppDedupe against state-of-the-art, driven by real-world datasets, demonstrates that AppDedupe achieves the highest global deduplication efficiency with a higher global deduplication effectiveness than the high-overhead and poorly scalable traditional scheme, but at an overhead only slightly higher than that of the scalable but low duplicate-elimination-ratio approaches.","Routing;Cloud computing;Fingerprint recognition;Distributed databases;Big Data;Indexes;Electromagnetic compatibility;Big data deduplication;application awareness;data routing;handprinting;similarity index","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2019 - Application-Aware Big Data Deduplication in Cloud Environment.pdf","","","",""
"Conference paper","Beller M,Gousios G,Zaidman A","","TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration","","","","","2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR)","","","2017","","","447-450","All","","","","","","","","","2017-05","","","","","","","http://dx.doi.org/10.1109/MSR.2017.24;https://ieeexplore.ieee.org/abstract/document/7962393/;http://pure.tudelft.nl/ws/portalfiles/portal/21809391/bellerMSR2017miningchallenge.pdf","10.1109/MSR.2017.24","","","","Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.","Testing;Data mining;Java;History;Rails;Software","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Beller et al. 2017 - TravisTorrent - Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration.pdf","","","",""
"Conference paper","Kim H,Yeom HY,Son Y","","An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging","","","","","2020 IEEE 13th International Conference on Cloud Computing (CLOUD)","","","2020","","","405-413","All","","","","","","","","","2020-10","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD49709.2020.00062;https://ieeexplore.ieee.org/abstract/document/9284224/?casa_token=sDypkVvAR5MAAAAA:gZ65rx2SqhdehG8Bc7kORfIvWpYR0lqEQ1mFBLCj-pBBYHhYAf6ZiSVcGLk6A_Fpkk3zTOZ6TjJH9w;https://ieeexplore.ieee.org/iel7/9284144/9284201/09284224.pdf?casa_token=GMg0KybrIM8AAAAA:eZ_wHfePfTpLEcSh22qD0fNyL6SdQuHLWcxllfv9hMMUY8v0bdcX57fCkZm2JE2grsxPHcCCLQPmDQ","10.1109/CLOUD49709.2020.00062","","","","Many cloud services perform periodic database backup to keep the data safe from failures such as sudden system crashes. In the database system, two techniques are widely used for data backup and recovery: a physical backup and a logical backup. The physical backup uses raw data by copying the files in the database, whereas the logical backup extracts data from the database and dumps it into separated files as a sequence of query statements. Both techniques support a full backup strategy that contains data of the entire database and incremental backup strategy that contains changed data since a previous backup. However, both strategies require additional I/O operations to perform the backup and need a long time to restore a backup. In this paper, we propose an efficient backup and recovery scheme by exploiting write-ahead logging (WAL) in database systems. In the proposed scheme, for backup, we devise a backup system to use log data generated by the existing WAL to eliminate the additional I/O operations. To restore a backup, we utilize and optimize the existing crash recovery procedure of WAL to reduce recovery time. For example, we divide the recovery range and applying the backup data for each range independently via multiple threads. We implement our scheme in MySQL, a popular database management system. The experimental result demonstrates that the proposed scheme provides instant backup while reducing recovery time compared with the existing schemes.","Cloud computing;Instruction sets;Conferences;Database systems;Computer crashes;Data mining;Database Management System;Data Management;Database Backup and Recovery;Write-Ahead Logging","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2020 - An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging.pdf","","","",""
"Website","Amazon Web Services","","Overview of AWS Data Transfer Costs","","","","","","","","2025","","","","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","2025","","2022-04-12","","","","","https://aws.amazon.com/blogs/architecture/overview-of-data-transfer-costs-for-common-architectures/","","","","","View detailed pricing for the Amazon Web Services Data Transfers cloud service.","","","","","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Website","Microsoft Azure","","Pricing – Bandwidth","","","","{Microsoft Azure}","","","","2025","","","","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","2025","","2025-01-11","","","","","https://azure.microsoft.com/en-gb/pricing/details/bandwidth/","","","","","View detailed pricing for the Azure Data Transfers cloud service. No upfront costs. Pay as you go. FREE trial.","","","","","en","Article","","","","","","","","","","","","","","","","","","","All Papers/M/Microsoft Azure 2025 - Pricing – Bandwidth.pdf","","","",""
"Journal article","Wajahat M,Yele A,Estro T,Gandhi A,Zadok E","","Analyzing the distribution fit for storage workload and Internet traffic traces","Performance Evaluation","","","","","","","2020","142","","102121","All","Datasets","","","","","","","","2020-09-01","","","","","0166-5316","","https://www.sciencedirect.com/science/article/pii/S0166531620300419;http://dx.doi.org/10.1016/j.peva.2020.102121","10.1016/j.peva.2020.102121","","","","Understanding workloads and modeling their performance is important for optimizing systems and services. A useful first step towards understanding the characteristics of workloads is to analyze their inter-arrival times and service requirements. If these characteristics are found to follow certain probability distributions, then corresponding stochastic models can be employed to efficiently estimate the performance of workloads. Such approaches have been explored in specific domains using an assortment of distributions, including the Normal, Weibull, and Exponential. Our primary goal in this work is to understand and model storage workload performance. However, our analysis and others’ past attempts revealed that none of the commonly-employed distributions provided a good fit for storage workloads. We analyzed over 250 traces across 5 different workload families using 20 widely used distributions, including ones seldom used for storage modeling. We found that the Hyper-exponential distribution with just two phases (H2) was superior in modeling the storage traces compared to other distributions under five diverse metrics of accuracy, including metrics that assess the risk of over-fitting. Based on these results, we developed a Markov-chain-based stochastic model that accurately estimates the storage system performance across several workload traces. To assess the applicability of the Hyper-exponential for distribution fitting beyond storage traces, we evaluated distribution fitting for Internet traffic traces using over 1,600 traces from 3 different sources. We again found that the Hyper-exponential distribution provided a superior fit compared to other probability distributions. To highlight the applicability of our model, we conducted what-if analyses to investigate (i) the storage performance impact of workload variability and garbage collection under various scenarios and (ii) the impact on service response time of Internet flash crowds.","Distribution fitting; Storage traces; Hyper-exponential; Performance modeling","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wajahat et al. 2020 - Analyzing the distribution fit for storage workload and Internet traffic traces.pdf","","","",""
"Website","Abad CL,Luu H,Lu Y,Campbell RH","","Metadata workloads for testing big storage systems","","","","","","","","","","","","All","Datasets","","","","","","","","","","2022-04-11","","","","","https://www.ideals.illinois.edu/bitstream/handle/2142/30013/cabad_Technical_Report.pdf?sequence=2&isAllowed=y","","","","","Efficient namespace metadata management is becoming more important as next-generation file systems are designed for the peta and exascale era. A number of new metadata management schemes have been proposed. However, evaluation of these designs has been insufficient, mainly due to a lack of appropriate namespace metadata traces. Specifically, no Big Data storage system metadata trace is publicly available, and existing traces are a poor replacement. We studied publicly available traces and one Big Data trace from Yahoo! and note some of the differences and their implications to metadata management studies. We discuss the insufficiency of existing evaluation approaches and present a first step towards a statistical metadata workload model that can capture the relevant characteristics of a workload and is suitable for synthetic workload generation.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Abad et al. - Metadata workloads for testing big storage systems.pdf","","","",""
"Conference paper","Luo X,Gao X,Tan Z,Liu J,Yang X,Chen G","","D2-Tree: A Distributed Double-Layer Namespace Tree Partition Scheme for Metadata Management in Large-Scale Storage Systems","","","","","2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)","","","2018","","","110-119","All","Datasets","","","","","","","","2018-07","","","","","2575-8411","","http://dx.doi.org/10.1109/ICDCS.2018.00021;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8416284","10.1109/ICDCS.2018.00021","","","","The behavior of metadata server (MDS) cluster is critically important to the overall performance of today's petabyte-scale or even exabyte-scale distributed file system. How to maintain a high level of both system locality and load balancing is a significant challenge to MDS clusters. However, traditional metadata management schemes, including hash-based mapping and subtree partitioning, have severe bias on either system locality or load balancing. In this paper, we propose D2-Tree, a distributed double-layer namespace tree partition scheme, for metadata management in large-scale storage systems. The innovative idea is to design a greedy strategy to split the namespace tree into global layer and local layer subtrees, of which global layer is replicated to maintain load balancing and the lower-half subtrees are allocated separately to MDS's by a mirror division method to preserve locality. Both theoretical analysis based on empirical cumulative distribution and extensive experiments are provided to validate the efficiency of D2-Tree. Experiments using actual trace data on Amazon EC2 also exhibit the superior performance of D2-Tree compared with much previous literature.","Metadata;Load management;Servers;Optimization;Computer science;Resource management;Standards;Metadata Management;Distributed File Systems;Locality;Load Balancing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Luo et al. 2018 - D2-Tree - A Distributed Double-Layer Namespace Tree Partition Scheme for Metadata Management in Large-Scale Storage Systems.pdf","","","",""
"Journal article","Gao Y,Gao X,Yang X,Liu J,Chen G","","An Efficient Ring-Based Metadata Management Policy for Large-Scale Distributed File Systems","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2019","30","9","1962-1974","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","Datasets","","","","","","","","2019-09","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2019.2901883;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8653345","10.1109/TPDS.2019.2901883","","","","The growing size of modern file system is expected to reach EB-scale. Therefore, an efficient and scalable metadata service is critical to system performance. Distributed metadata management schemes, which use multiple metadata servers (MDS's) to store metadata, provide a highly effective approach to alleviate the workload of a single server. However, it is difficult to maintain good metadata locality and load balancing among MDS's at the same time. In this paper, we propose a novel hashing scheme called AngleCut to partition metadata namespace tree and serve large-scale distributed storage systems. AngleCut first uses a locality preserving hashing (LPH) function to project the namespace tree into linear keyspace, i.e., multiple Chord-like rings. Then we design a history-based allocation strategy to adjust the workload of MDS's dynamically. Besides, we propose a two-layer metadata cache mechanism, including server-side cache and client-side cache to provide the two stage access acceleration. Last but not least, we introduce a distributed metadata processing 2PC Protocol Based on Message Queue (2PC-MQ) to ensure data consistency. In general, our scheme preserves good metadata locality as well as maintains a high load balancing between MDS's. The theoretical proof and extensive experiments on Amazon EC2 demonstrate the superiority of AngleCut over previous literature.","Metadata;Servers;Load management;Distributed databases;Resource management;Protocols;File systems;Metadata management;locality preserving hashing;distributed storage system;namespace tree","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Gao et al. 2019 - An Efficient Ring-Based Metadata Management Policy for Large-Scale Distributed File Systems.pdf","","","",""
"Journal article","Adya A,Bolosky WJ,Castro M,Cermak G,Chaiken R,Douceur JR,Howell J,Lorch JR,Theimer M,Wattenhofer RP","","Farsite: federated, available, and reliable storage for an incompletely trusted environment","Oper. Syst. Rev.","ACM SIGOPS Operating Systems Review","","","","","","2003","36","SI","1-14","All;Thesis;p-scailbib","Distributed File Systems;CE/MLE","","","Association for Computing Machinery","New York, NY, USA","","","","2003-12-31","","","","","0163-5980","","https://doi.org/10.1145/844128.844130;http://dx.doi.org/10.1145/844128.844130;https://dl.acm.org/doi/abs/10.1145/844128.844130;https://www.usenix.org/events/osdi02/tech/full_papers/adya/adya.pdf","10.1145/844128.844130","","","","Farsite is a secure, scalable file system that logically functions as a centralized file server but is physically distributed among a set of untrusted computers. Farsite provides file availability and reliability through randomized replicated storage; it ensures the secrecy of file contents with cryptographic techniques; it maintains the integrity of file and directory data with a Byzantine-fault-tolerant protocol; it is designed to be scalable by using a distributed hint mechanism and delegation certificates for pathname translations; and it achieves good performance by locally caching file data, lazily propagating file updates, and varying the duration and granularity of content leases. We report on the design of Farsite and the lessons we have learned by implementing much of that design.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Adya et al. 2003 - Farsite - federated, available, and reliable storage for an incompletely trusted environment.pdf","","","","Winter 2002"
"Conference paper","Anwar A,Mohamed M,Tarasov V,Littley M,Rupprecht L,Cheng Y,Zhao N,Skourtis D,Warke AS,Ludwig H,Others","","Improving docker registry design based on production workload analysis","","","","","16th USENIX Conference on File and Storage Technologies (FAST 18)","","","2018","","","265-278","All","","","","","","","","","2018","","","","","","","https://www.usenix.org/conference/fast18/presentation/anwar;https://www.usenix.org/system/files/conference/fast18/fast18-anwar.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Anwar et al. 2018 - Improving docker registry design based on production workload analysis.pdf","","","",""
"Conference paper","Ni F,Jiang S","","RapidCDC: Leveraging Duplicate Locality to Accelerate Chunking in CDC-based Deduplication Systems","","","","","Proceedings of the ACM Symposium on Cloud Computing","","","2019","","","220-232","Thesis;All","","","","Association for Computing Machinery","New York, NY, USA","","Santa Cruz, CA, USA","","2019-11-20","","2022-03-26","9781450369732","","","","https://doi.org/10.1145/3357223.3362731;http://dx.doi.org/10.1145/3357223.3362731;https://dl.acm.org/doi/abs/10.1145/3357223.3362731;https://dl.acm.org/doi/pdf/10.1145/3357223.3362731","10.1145/3357223.3362731","","","","I/O deduplication is a key technique for improving storage systems' space and I/O efficiency. Among various deduplication techniques content-defined chunking (CDC) based deduplication is the most desired one for its high deduplication ratio. However, CDC is compute-intensive and time-consuming, and has been recognized as a major performance bottleneck of the CDC-based deduplication system.In this paper we leverage the existence of a property in the duplicate data, named duplicate locality, that reveals the fact that multiple duplicate chunks are likely to occur together. In other words, one duplicate chunk is likely to be immediately followed by a sequence of contiguous duplicate chunks. The longer the sequence, the stronger the locality is. After a quantitative analysis of duplicate locality in real-world data, we propose a suite of chunking techniques that exploit the locality to remove almost all chunking cost for deduplicatable chunks in CDC-based deduplication systems. The resulting deduplication method, named RapidCDC, has two salient features. One is that its efficiency is positively correlated to the deduplication ratio. RapidCDC can be as fast as a fixed-size chunking method when applied on data sets with high data redundancy. The other feature is that its high efficiency does not rely on high duplicate locality strength. These attractive features make RapidCDC's effectiveness almost guaranteed for datasets with high deduplication ratio. Our experimental results with synthetic and real-world datasets show that RapidCDC's chunking speedup can be up to 33x higher than regular CDC. Meanwhile, it maintains (nearly) the same deduplication ratio.","content-defined chunking, storage systems, locality, CDC, deduplication","","","","","","SoCC '19","","","","","","","","","","","","","","","","","","All Papers/N/Ni and Jiang 2019 - RapidCDC - Leveraging Duplicate Locality to Accelerate Chunking in CDC-based Deduplication Systems.pdf","","","",""
"Conference paper","Kim H,Yeom HY,Son Y","","An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging","","","","","2020 IEEE 13th International Conference on Cloud Computing (CLOUD)","","","2020","","","405-413","All","","","","","","","","","2020-10","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD49709.2020.00062;https://ieeexplore.ieee.org/abstract/document/9284224/;https://ieeexplore.ieee.org/iel7/9284144/9284201/09284224.pdf","10.1109/CLOUD49709.2020.00062","","","","Many cloud services perform periodic database backup to keep the data safe from failures such as sudden system crashes. In the database system, two techniques are widely used for data backup and recovery: a physical backup and a logical backup. The physical backup uses raw data by copying the files in the database, whereas the logical backup extracts data from the database and dumps it into separated files as a sequence of query statements. Both techniques support a full backup strategy that contains data of the entire database and incremental backup strategy that contains changed data since a previous backup. However, both strategies require additional I/O operations to perform the backup and need a long time to restore a backup. In this paper, we propose an efficient backup and recovery scheme by exploiting write-ahead logging (WAL) in database systems. In the proposed scheme, for backup, we devise a backup system to use log data generated by the existing WAL to eliminate the additional I/O operations. To restore a backup, we utilize and optimize the existing crash recovery procedure of WAL to reduce recovery time. For example, we divide the recovery range and applying the backup data for each range independently via multiple threads. We implement our scheme in MySQL, a popular database management system. The experimental result demonstrates that the proposed scheme provides instant backup while reducing recovery time compared with the existing schemes.","Cloud computing;Instruction sets;Conferences;Database systems;Computer crashes;Data mining;Database Management System;Data Management;Database Backup and Recovery;Write-Ahead Logging","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2020 - An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging.pdf","","","",""
"Conference paper","Xu Z,Zhang W","","QuickCDC: A Quick Content Defined Chunking Algorithm Based on Jumping and Dynamically Adjusting Mask Bits","","","","","2021 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)","","","2021","","","288-299","Thesis;All","","","","","","","","","2021-09","","","","","","","http://dx.doi.org/10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00049;https://ieeexplore.ieee.org/abstract/document/9644788/;http://www.cloud-conf.net/ispa2021/proc/pdfs/ISPA-BDCloud-SocialCom-SustainCom2021-3mkuIWCJVSdKJpBYM7KEKW/264600a288/264600a288.pdf","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00049","","","","Data chunking refers to the process of splitting files or data stream into multiple chunks of fixed length or variable length. Chunking determines the performance of underlying storage system. Content Defined Chunking (CDC) has been widely employed and plays an important role in backup and primary storage systems. Due to time-consuming byte by byte calculation on data stream, chunking speed and deduplication ratio of Rabin CDC are low, which is CPU intensive and based on sliding window. QuickCDC utilizes three technologies to enhance chunking speed, deduplication ratio and throughput of CDC. Firstly, for duplicate chunks that appear many times, QuickCDC can jump directly to their chunk boundaries. The mapping of the first n bytes and the last m bytes of the duplicate chunk to chunk length needs to be recorded. When executing chunking, the first n bytes and the last m bytes of the current chunk are judged whether they are in the mapping table. If they are in the mapping table, QuickCDC can skip corresponding chunk length. Secondly, for unique chunks, QuickCDC can skip the minimum chunk length on them. Thirdly, QuickCDC is able to dynamically adjust mask bits length so that the chunk length distributes in a small specific region, and is always greater than the minimum chunk length. We should employ longer mask bits when current chunk length is less than the expected chunk length, and use shorter mask bits when current chunk length is larger than the expected chunk length. Experiments reveal that chunking speed of QuickCDC is 11.4x of RapidCDC, and corresponding deduplication ratio is slightly improved, the maximum improvement of deduplication ratio is 222.3% and the improvement of throughput is 111.4%.","Costs;Multithreading;Gears;Heuristic algorithms;Instruction sets;Pipelines;Graphics processing units;data chunking;Content Defined Chunking;jump;dynamically adjust mask bits;data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xu and Zhang 2021 - QuickCDC - A Quick Content Defined Chunking Algorithm Based on Jumping and Dynamically Adjusting Mask Bits.pdf","","","",""
"Conference paper","Meister D,Brinkmann A","","Multi-level comparison of data deduplication in a backup scenario","","","","","Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference","","","2009","","","1-12","All","Causality Based","","Article 8","Association for Computing Machinery","New York, NY, USA","","Haifa, Israel","","2009-05-04","","2022-03-25","9781605586236","","","","https://doi.org/10.1145/1534530.1534541;http://dx.doi.org/10.1145/1534530.1534541;https://dl.acm.org/doi/abs/10.1145/1534530.1534541?casa_token=BezZBCE2YHQAAAAA:8k4sRicBwkFdupLp4yLR2xVE8WsgS1R4fvjF83TthicIkUaxGZcinenbEngJbAUgA9FAJCUeLQHrBw;https://dl.acm.org/doi/pdf/10.1145/1534530.1534541?casa_token=USHktIXuZhcAAAAA:zH6IIhPPp1qHThCQRVBf75Dac70ErB7R-b5NueuGMG1vHtgDMRTPUk118a-0pjrr7a-ryyhuEi9K5Q","10.1145/1534530.1534541","","","","Data deduplication systems detect redundancies between data blocks to either reduce storage needs or to reduce network traffic. A class of deduplication systems splits the data stream into data blocks (chunks) and then finds exact duplicates of these blocks.This paper compares the influence of different chunking approaches on multiple levels. On a macroscopic level, we compare the chunking approaches based on real-life user data in a weekly full backup scenario, both at a single point in time as well as over several weeks.In addition, we analyze how small changes affect the deduplication ratio for different file types on a microscopic level for chunking approaches and delta encoding. An intuitive assumption is that small semantic changes on documents cause only small modifications in the binary representation of files, which would imply a high ratio of deduplication. We will show that this assumption is not valid for many important file types and that application-specific chunking can help to further decrease storage capacity demands.","deduplication, backup, compression, temporal redundancy","","","","","","SYSTOR '09","8","","","","","","","","","","","","","","","","","All Papers/M/Meister and Brinkmann 2009 - Meister and Brinkmann 2009 - Multi-level comparison of data deduplication in a backup scenario.pdf;All Papers/M/Meister and Brinkmann 2009 - Multi-level comparison of data deduplication in a backup scenario.pdf","","","",""
"Conference paper","Tan Y,Jiang H,Feng D,Tian L,Yan Z","","CABdedupe: A Causality-Based Deduplication Performance Booster for Cloud Backup Services","","","","","2011 IEEE International Parallel Distributed Processing Symposium","","","2011","","","1266-1277","All","Causality Based","","","","","","","","2011-05","","","","","1530-2075","","http://dx.doi.org/10.1109/IPDPS.2011.76;https://ieeexplore.ieee.org/abstract/document/6012934/?casa_token=sHUWPc-FSV0AAAAA:46sKpovGIC2o_z0erL-FN_VW22YpLvjpe7yHU0hwml03PcNL585yW6KrqEk6MuHjkf7f5lTClGCLxw;https://ieeexplore.ieee.org/iel5/6011824/6012804/06012934.pdf?casa_token=XykxHVQewAAAAAAA:TeIurHR3lLLCXHU06iCeD4079OAZVmB2ixbloEAZetovkaaAIqW7rE6OkDfKMeSPuHwbYDjgqMs9Mg","10.1109/IPDPS.2011.76","","","","Due to the relatively low bandwidth of WAN (Wide Area Network) that supports cloud backup services, both the backup time and restore time in the cloud backup environment are in desperate need for reduction to make cloud backup a practical and affordable service for small businesses and telecommuters alike. Existing solutions that employ the deduplication technology for cloud backup services only focus on removing redundant data from transmission during backup operations to reduce the backup time, while paying little attention to the restore time that we argue is an important aspect and affects the overall quality of service of the cloud backup services. In this paper, we propose a CAusality Based deduplication performance booster for both cloud backup and restore operations, called CABdedupe, which captures the causal relationship among chronological versions of datasets that are processed in multiple backups/restores, to remove the unmodified data from transmission during not only backup operations but also restore operations, thus to improve both the backup and restore performances. CABdedupe is a middleware that is orthogonal to and can be integrated into any existing backup system. Our extensive experiments, where we integrate CABdedupe into two existing backup systems and feed real world datasets, show that both the backup time and restore time are significantly reduced, with a reduction ratio of up to 103 : 1.","Redundancy;Monitoring;Data communication;Servers;Wide area networks;Data mining;Bandwidth","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan et al. 2011 - CABdedupe - A Causality-Based Deduplication Performance Booster for Cloud Backup Services.pdf","","","",""
"Conference paper","Simha DN,Lu M,Chiueh TC","","A scalable deduplication and garbage collection engine for incremental backup","","","","","Proceedings of the 6th International Systems and Storage Conference","","","2013","","","1-12","All","Incremental Backup/Sync;Garbage Collection;Causality Based","","Article 16","Association for Computing Machinery","New York, NY, USA","","Haifa, Israel","","2013-06-30","","2022-03-25","9781450321167","","","","https://doi.org/10.1145/2485732.2485753;http://dx.doi.org/10.1145/2485732.2485753;https://dl.acm.org/doi/abs/10.1145/2485732.2485753?casa_token=j9LiiwiV_yMAAAAA:iS3YsOMddhJPAA6Mr8JUlX_kkmE2et2YxLWltIGix_M0SLyorDin2rMkilttUTcxaqak1PahF3qqEA;https://dl.acm.org/doi/pdf/10.1145/2485732.2485753?casa_token=g3WZR9WcDbsAAAAA:I6rUZIb3irMDEcYwLJtlnEd2vhngoBq1MaALLD4SUzmyGvsapuGwpvPYwz8KLH7IzpfAr9lDL97nKw","10.1145/2485732.2485753","","","","Very large block-level data backup systems need scalable data deduplication and garbage collection techniques to make efficient use of the storage space and to minimize the performance overhead of doing so. Although the deduplication and garbage collection logic is conceptually straight-forward, their implementations pose a significant technical challenge because only a small portion of their associated data structures could fit into memory. In this paper, we describe the design, implementation and evaluation of a data deduplication and garbage collection engine called Sungem that is designed to remove duplicate blocks in incremental data backup streams. Sungem features three novel techniques to maximize the deduplication throughput without compromising the deduplication ratio. First, Sungem puts related fingerprint sequences, rather than fingerprints from the same backup stream, into the same container in order to increase the fingerprint prefetching efficiency. Second, to make the most of the memory space reserved for storing fingerprints, Sungem varies the sampling rates for fingerprint sequences based on their stability. Third, Sungem combines reference count and expiration time in a unique way to arrive at the first known incremental garbage collection algorithm whose bookkeeping overhead is proportional to the size of a disk volume's incremental backup snapshot rather than its full backup snapshot. We evaluated the Sungem prototype using a real-world data backup trace, and showed that the average throughput of Sungem is more than 200,000 fingerprint lookups per second on a standard X86 server, including the garbage collection cost.","deduplication, garbage collection, backup storage","","","","","","SYSTOR '13","16","","","","","","","","","","","","","","","","","All Papers/S/Simha et al. 2013 - A scalable deduplication and garbage collection engine for incremental backup.pdf","","","",""
"Conference paper","Kaiser J,Brinkmann A,Süß T,Meister D","","Deriving and comparing deduplication techniques using a model-based classification","","","","","Proceedings of the Tenth European Conference on Computer Systems","","","2015","","","1-13","All","","","Article 11","Association for Computing Machinery","New York, NY, USA","","Bordeaux, France","","2015-04-17","","2022-03-24","9781450332385","","","","https://doi.org/10.1145/2741948.2741952;http://dx.doi.org/10.1145/2741948.2741952;https://dl.acm.org/doi/abs/10.1145/2741948.2741952?casa_token=vUsMlKE_W_UAAAAA:znnOZ-7pM3X0fG65zw7k0J5g3mIyhGGNenDWrB9j3InX2D7g9vFqCQeNQZsSv6HX2KkCBj9IWihpdQ;https://dl.acm.org/doi/pdf/10.1145/2741948.2741952?casa_token=udLzrgfzFvMAAAAA:50aw7myypGR1gT0byRguS1ezu0D1zhacxZMhrQC3HwAoh7IzTvLoTF7peVMLBS307JpXsV2HmKxcbA","10.1145/2741948.2741952","","","","Data deduplication has been a hot research topic and a large number of systems have been developed. These systems are usually seen as an inherently linked set of characteristics. However, a detailed analysis shows independent concepts that can be used in other systems.In this work, we perform this analysis on the main representatives of deduplication systems. We embed the results in a model, which shows two yet unexplored combinations of characteristics. In addition, the model enables a comprehensive evaluation of the representatives and the two new systems. We perform this evaluation based on real world data sets.","","","","","","","EuroSys '15","11","","","","","","","","","","","","","","","","","All Papers/K/Kaiser et al. 2015 - Deriving and comparing deduplication techniques using a model-based classification.pdf","","","",""
"Conference paper","Zuo C,Wang F,Huang P,Hu Y,Feng D","","RepEC-Duet: Ensure High Reliability and Performance for Deduplicated and Delta-Compressed Storage Systems","","","","","2019 IEEE 37th International Conference on Computer Design (ICCD)","","","2019","","","190-198","All","","","","","","","","","2019-11","","","","","2576-6996","","http://dx.doi.org/10.1109/ICCD46524.2019.00032;https://ieeexplore.ieee.org/abstract/document/8988716/?casa_token=aLHUfGYzE4AAAAAA:KnkkIGA2AFUoI7UImeFWkj_F9zJWhmbKHN7Y1JpateDZW5sJRp9zcgSjvIfUb1nAA2Alu_Ym3vsT4A;https://ieeexplore.ieee.org/iel7/8970097/8988587/08988716.pdf?casa_token=h2zrsxrMJ4wAAAAA:VAXBBmlWJPR1R6oNLPiL2ThT--HOU-KEs-X4m2oneYIOlX3c4XNarguvcjGodjYKYLPh-6cTy4MsKg","10.1109/ICCD46524.2019.00032","","","","Data deduplication is a widely deployed technique to remove duplicate content to save storage space, which is however incapable of eliminating the redundancy between nonidentical but similar data blocks. To achieve further space savings in deduplicated storage systems, delta compression is employed to compress post-deduplication data. Both deduplication and delta compression introduce content references among blocks, which inevitably undermines the reliability of deduplicated and delta compressed storage systems. To ensure better reliability, existing approaches utilize either replication or erasure codes to redundantly distribute data across multiple nodes. In deduplicated and delta compressed storage systems, we observe that delta compressed chunks (DCCs) are far smaller than regular chunks called non-DCCs. Motivated by this observation, we suggest a straightforward approach in which replication is used to protect DCCs and erasure code is deployed to protect non-DCCs. However, we need to address two critical challenges to ensure this solution effective. First, the random placement of DCCs replicas destroys cache locality. Second, the separate and individual recovery and restore cache could cause storage containers to be accessed repeatedly. To address these two challenges, in this paper, we propose RepEC-Duet which employs both replication and erasure codes to ensure high reliability and performance for deduplicated and delta-compressed storage systems. RepEC-Duet introduces a delta-utilization-aware filter to select and replicate containers based on the percentage of DCCs in the containers to maintain cache locality. Moreover, to avoid unnecessary container reads, we design a cooperative cache scheme that is aware of both failure recovery and regular restore cache. Our experimental results based on three real-world datasets demonstrate that RepEC-Duet significantly improves the restore performance by 26%-59%, and reduces the storage overhead by 54%-98% than the existing approaches.","Containers;Distributed databases;Decoding;Reliability engineering;Redundancy;System performance;Data deduplication, Delta compression, Reliability, Restore performance","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zuo et al. 2019 - RepEC-Duet - Ensure High Reliability and Performance for Deduplicated and Delta-Compressed Storage Systems.pdf","","","",""
"Conference paper","Patil J,Barve SS","","DDFP: Duplicate detection and fragment placement in deduplication system for security and storage space","","","","","2017 1st International Conference on Intelligent Systems and Information Management (ICISIM)","","","2017","","","225-229","All","","","","","","","","","2017-10","","","","","","","http://dx.doi.org/10.1109/ICISIM.2017.8122177;https://ieeexplore.ieee.org/abstract/document/8122177/?casa_token=Y3Eu5quXf6kAAAAA:0YPOOVZfjs6HB74tegKV7yHOfqhTHHrT4fNOP3od5xQYRhlmNVaUOcz7I6koj17GU49IqTKLQdT3fg;https://ieeexplore.ieee.org/iel7/8114611/8122132/08122177.pdf?casa_token=NHX8jQ42IWgAAAAA:k75KOlb0Xp6EK4kPwyM1QNcT8d8sh-O2nuyZy9o6Z4p1LK0tYcrTNAzz3-LF_YXkRWsmGi_OpvzVkA","10.1109/ICISIM.2017.8122177","","","","Due to increasing volume of data in information technology, saving storage space and providing security to data has acquired more attention and popularity. In data processing and data mining, duplicates can effect severely. Data deduplication is a technique that eliminates duplicate data and store only one copy, promoting single instance storage. The main challenges are to identify maximum duplicate segment and selecting the storage nodes for distributing fragments of files. In this paper we proposed, Duplicate Detection and Fragment Placement (DDFP) a deduplication system that effectively eliminates duplicate data and fragments placement that allocates unique instances of a data file on storage nodes. For repeated data, reference pointer is used and only unique data is stored on the storage node. This increases the percentage of duplicate data detection. A fragment placement algorithm is used for placing fragments on different storage nodes. To select nodes T-coloring is used, Set T is used, which restricts the nodes that are at distance T from one another. DDFP considerably achieves duplicate elimination and obtain the high level of security on data fragments by storing fragments of the data file using T-coloring. This Selects the nodes that are not adjacent which prevent unauthorized access to data from other users.","Indexes;Algorithm design and analysis;Fingerprint recognition;Secure storage;Silicon;Computer security;Data Duplication;Chunk fragmentation;Distributed;Locality;Storage system","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Patil and Barve 2017 - DDFP - Duplicate detection and fragment placement in deduplication system for security and storage space.pdf","","","",""
"Conference paper","Dagnaw G,Hua W,Zhou K","","CACH-Dedup: Content Aware Clustered and Hierarchical Deduplication","","","","","2018 IEEE 24th International Conference on Parallel and Distributed Systems (ICPADS)","","","2018","","","399-407","All","Similarity/Resemblance","","","","","","","","2018-12","","","","","1521-9097","","http://dx.doi.org/10.1109/PADSW.2018.8644884;https://ieeexplore.ieee.org/abstract/document/8644884/?casa_token=wjbGvIlbrP0AAAAA:dMjTBm8IMjx0eDkV6Xev2UgbetF62XB1jG3P0t54MGOro4UJvBtwKD7Clr0XavVkpWezTEZr-Z5S_Q;https://ieeexplore.ieee.org/iel7/8635632/8644527/08644884.pdf?casa_token=ZBPcscH7I3gAAAAA:qYJgxxo1v8s5JSJ70uBpDkboA9nwjlQJGFcnnmiSai0NKDfuJK4BV48-pgE6QWVbXvxtXGpWutfZFg","10.1109/PADSW.2018.8644884","","","","Distributed deduplication overcomes, to some extent, index-lookup disk bottleneck problem by dividing deduplication tasks among many nodes. However, the task of selecting these nodes is an important challenge because it could result in high communication cost and the storage node island effect problem. Moreover, intelligent data routing is required to exploit the peculiar nature of data from different applications which share insignificant amount of content. In this paper, we explore CACH-Dedup, a content aware clustered and hierarchical deduplication system, which exploits the negligibly small amount of content shared among chunks from different file types to create groups of files and storage nodes with out loss of deduplication effectiveness. It uses hierarchical deduplication to reduce the size of fingerprint indexes at the global level, where only files and big sized segments are deduplicated. It also makes advantage of locality first using the big sized segments deduplicated at the global level and second by routing a set of consecutive files together to one storage node. Furthermore, it exploits similarity by making use of similarity bloom filters of streams for stateful routing which results in duplicate elimination rate in a par with single node deduplication with a minimal cost of computation and communication. CACH-Dedup is evaluated using a prototype deployed on windows server environment distributed over four separate machines. It is shown to have duplicate elimination effectiveness in a par with a single node deduplication system, with a minimal communication overhead and an acceptable deduplication throughput.","Conferences;Clustered deduplication;Content Aware Grouping;Hierarchical deduplication;Data routing;Similarity bloom filters","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dagnaw et al. 2018 - CACH-Dedup - Content Aware Clustered and Hierarchical Deduplication.pdf","","","",""
"Journal article","Dagnaw G,Zhou K,Wang H","","DCACH: Content aware clustered and hierarchical distributed deduplication","J. Softw. Eng. Appl.","Journal of software engineering and applications","","","","","","2019","12","11","460-490","All;FSL Traces;Thesis","Application Aware","","","Scientific Research Publishing, Inc,","","","","","2019","","","","","1945-3116","1945-3124","https://www.scirp.org/journal/doi.aspx?doi=10.4236/jsea.2019.1211029;http://dx.doi.org/10.4236/jsea.2019.1211029;https://www.scirp.org/journal/paperinformation.aspx?paperid=96634","10.4236/jsea.2019.1211029","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dagnaw et al. 2019 - DCACH - Content aware clustered and hierarchical distributed deduplication.pdf","","","",""
"Conference paper","Xu G,Tang B,Lu H,Yu Q,Sung CW","","LIPA: A Learning-based Indexing and Prefetching Approach for Data Deduplication","","","","","2019 35th Symposium on Mass Storage Systems and Technologies (MSST)","","","2019","","","299-310","Grouped by Publication/MSST;All","","","","","","","","","2019-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2019.00010;https://ieeexplore.ieee.org/abstract/document/8890070/?casa_token=zJKK2xbP9l0AAAAA:G3SxLbA-YI3BReNk9L62yhHjs96Ct3uIsTt4rBNqOS86Af3VBlSbx9YjWxS8CVqkv1k_chokm9RSRQ;https://ieeexplore.ieee.org/iel7/8882740/8890059/08890070.pdf?casa_token=HcYgFqlgj1EAAAAA:z7FsEIihfbyxZbKPf7wt5I5CY2cEZ-Zc7xod6PfGQOqkqMP57rrPg4z_rasUqoKPUWOyVVaL2Zj6kA","10.1109/MSST.2019.00010","","","","In this paper, we present a learning based data deduplication algorithm, called LIPA, which uses the reinforcement learning framework to build an adaptive indexing structure. It is rather different from previous inline chunk-based deduplication methods to solve the chunk-lookup disk bottleneck problem for large-scale backup. In previous methods, a full chunk index or a sampled chunk index often is often required to identify duplicate chunks, which is a critical stage for data deduplication. The full chunk index is hard to fit in RAM and the sampled chunk index directly affects the deduplication ratio dependent on the sampling ratio. Our learning based method only requires little memory overheads to store the index but achieves the same or even better deduplication ratio than previous methods. In our method, after the data stream is broken into relatively large segments, one or more representative chunk fingerprints are chosen as the feature of a segment. An incoming segment may share the same feature with previous segments. Thus we use a key-value structure to record the relationship between features and segments: a feature maps to a fixed number of segments. We train the similarities of these segments to a feature represented as scores by the reinforcement learning method. For an incoming segment, our method adaptively prefetches a segment and the successive ones into cache by using multi-armed bandits model. Our experimental results show that our method significantly reduces memory overheads and achieves effective deduplication.","Prefetching;Reinforcement learning;Indexing;Memory management;Random access memory;Distributed databases;Deduplication;Reinforcement learning;Data prefetching;Chunk index","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xu et al. 2019 - LIPA - A Learning-based Indexing and Prefetching Approach for Data Deduplication.pdf","","","",""
"Conference paper","Ye X,Tang J,Tian W,Li R,Xiao W,Geng Y,Xu Z","","Fast Variable-Grained Resemblance Data Deduplication For Cloud Storage","","","","","2021 IEEE International Conference on Networking, Architecture and Storage (NAS)","","","2021","","","1-8","Grouped by Publication/NAS;All","","","","","","","","","2021-10","","","","","","","http://dx.doi.org/10.1109/NAS51552.2021.9605398;https://ieeexplore.ieee.org/abstract/document/9605398/?casa_token=evo4F_nYh9YAAAAA:VVL3fbxTJkXfGw6bafjxLv7khiBXaQpDuvHiG8_f1SY4RTMeHifjoJoyxEvqO7c889z1ir60fKgr6A;https://ieeexplore.ieee.org/iel7/9605358/9605359/09605398.pdf?casa_token=nLkkjfbG2JEAAAAA:J013UzywvaGlwAH9URm6IStWFCLGzMefzXtpQlKy1qdJCs9O6v11WiehgingbTmz4ilG6p5uimC4dg","10.1109/NAS51552.2021.9605398","","","","With the prevalence of cloud storage, data deduplication has been a widely used technology by removing cross users’ duplicate data and saving network bandwidth. Nevertheless, traditional data deduplication hardly detects duplicate data among resemblance chunks. Currently, a resemblance data deduplication, called Finesse, has been proposed to detect and remove the duplicate data among similar chunks efficiently. However, we observe that the chunks following the similar chunk have a high chance of resembling data locality property, and vice versa. Processing these adjacent similar chunks in small average chunk size level increases the metadata, which deteriorates the deduplication system performance. Moreover, existing resemblance data deduplication schemes ignore the performance impact from metadata. Therefore, we propose a fast variable-grained resemblance data deduplication for cloud storage. It dynamically combines the adjacent resemblance chunks or unique chunks or breaks those chunks, located at the transition region between resemblance chunks and unique chunks. Finally, we implement a prototype and conduct a serial of experiments on real-world datasets. The results show that our method dramatically reduces the metadata size while achieving the high deduplication ratio.","Cloud computing;Costs;System performance;Conferences;Prototypes;Bandwidth;Metadata;Resemblance Data Deduplication;Cloud Storage;Metadata Size;Variabe-Grained","Requires deduplicaton status of chunks to decide whether to coalesce them to reduce metadata.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Ye et al. 2021 - Fast Variable-Grained Resemblance Data Deduplication For Cloud Storage.pdf","","","",""
"Journal article","Zhang Y,Mao Y,Xu M,Xu F,Zhong S","","Towards Thwarting Template Side-Channel Attacks in Secure Cloud Deduplications","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","2021","18","3","1008-1018","All","","","","","","","","","2021-05","","","","","1941-0018","","http://dx.doi.org/10.1109/TDSC.2019.2911502;https://ieeexplore.ieee.org/document/8691793","10.1109/TDSC.2019.2911502","","","","As one of a few critical technologies to cloud storage service, deduplication allows cloud servers to save storage space by deleting redundant file copies. However, it often leaks side channel information regarding whether an uploading file gets deduplicated or not. Exploiting this information, adversaries can easily launch a template side-channel attack and severely harm cloud users' privacy. To thwart this kind of attack, we resort to the k-anonymity privacy concept to design secure threshold deduplication protocols. Specifically, we have devised a novel cryptographic primitive called “dispersed convergent encryption” (DCE) scheme, and proposed two different constructions of it. With these DCE schemes, we successfully construct secure threshold deduplication protocols that do not rely on any trusted third party. Our protocols not only support confidentiality protections and ownership verifications, but also enjoy formal security guarantee against template side-channel attacks even when the cloud server could be a “covert adversary” who may violate the predefined threshold and perform deduplication covertly. Experimental evaluations show our protocols enjoy very good performance in practice.","Servers;Protocols;Cloud computing;Privacy;Encryption;Side-channel attacks;Cloud;secure deduplication;privacy;proofs of ownership","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2021 - Towards Thwarting Template Side-Channel Attacks in Secure Cloud Deduplications.pdf","","","",""
"Journal article","Burns-Joe ZN,Rubin HA","","Secure Deletion for a Versioning File System","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://www.usenix.org/event/fast05/tech/full_papers/peterson/peterson_html/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Li J,Qin C,Lee PP,Li J","","Rekeying for Encrypted Deduplication Storage","","","","","2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","","","2016","","","618-629","All;p-scailbib","","","","ieeexplore.ieee.org","","","","","2016-06","","","","","2158-3927","","http://dx.doi.org/10.1109/DSN.2016.62;https://ieeexplore.ieee.org/abstract/document/7579777/?casa_token=si2VnWo3k7QAAAAA:lfGNNzw_q9tNn05a1rJzpwgnUUSUF7kDbzzfL8feuGuGIM8hZJs2pey4lCWSgnIik1_H1z_taeqM4g;https://ieeexplore.ieee.org/iel7/7579391/7579714/07579777.pdf?casa_token=F7uCR3qJP2YAAAAA:EudD8aahnXW3pfBEk6wkxbuUzhmVPCMs49fd3DjTmiaKy7juS5fXbn6_7OYieTB9R8INVgH3775rCQ","10.1109/DSN.2016.62","","","","Rekeying refers to an operation of replacing an existing key with a new key for encryption. It renews security protection, so as to protect against key compromise and enable dynamic access control in cryptographic storage. However, it is non-trivial to realize efficient rekeying in encrypted deduplication storage systems, which use deterministic content-derived encryption keys to allow deduplication on ciphertexts. We design and implement REED, a rekeying-aware encrypted deduplication storage system. REED builds on a deterministic version of all-or-nothing transform (AONT), such that it enables secure and lightweight rekeying, while preserving the deduplication capability. We propose two REED encryption schemes that trade between performance and security, and extend REED for dynamic access control. We implement a REED prototype with various performance optimization techniques. Our trace-driven testbed evaluation shows that our REED prototype maintains high performance and storage efficiency.","Maximum likelihood estimation;Encryption;Cloud computing;Genomics;Bioinformatics","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2016 - Rekeying for Encrypted Deduplication Storage.pdf","","","",""
"Journal article","Luo L,Guo D,Ma RT,Rottenstreich O,Luo X","","Optimizing Bloom Filter: Challenges, Solutions, and Comparisons","IEEE Communications Surveys Tutorials","","","","","","","2019","21","2","1912-1949","All","","","","","","","","","Secondquarter 2019","","","","","1553-877X","","http://dx.doi.org/10.1109/COMST.2018.2889329;https://ieeexplore.ieee.org/abstract/document/8586915/?casa_token=SXSlpa294BYAAAAA:_0tYEtEf3-nWQDyX9VQWTHsFkrBeC56Bbg3E4qkUcoOkzMI_nqrUSJRpnDcaVLJyEwAEeWaACYvCWA;https://ieeexplore.ieee.org/iel7/9739/5451756/08586915.pdf?casa_token=QJxuu0ZOu-0AAAAA:QoXWozPt08sBlqbJ9QaoZoIO1CNwPQ25l8E5DoZJM7R9WEnbuf04222FsczSZDGERZUNEj2xgPAinw","10.1109/COMST.2018.2889329","","","","Bloom filter (BF) has been widely used to support membership query, i.e., to judge whether a given element x is a member of a given set S or not. Recent years have seen a flourish design explosion of BF due to its characteristic of space-efficiency and the functionality of constant-time membership query. The existing reviews or surveys mainly focus on the applications of BF, but fall short in covering the current trends, thereby lacking intrinsic understanding of their design philosophy. To this end, this survey provides an overview of BF and its variants, with an emphasis on the optimization techniques. Basically, we survey the existing variants from two dimensions, i.e., performance and generalization. To improve the performance, dozens of variants devote themselves to reducing the false positives and implementation costs. Besides, tens of variants generalize the BF framework in more scenarios by diversifying the input sets and enriching the output functionalities. To summarize the existing efforts, we conduct an in-depth study of the existing literature on BF optimization, covering more than 60 variants. We unearth the design philosophy of these variants and elaborate how the employed optimization techniques improve BF. Furthermore, comprehensive analysis and qualitative comparison are conducted from the perspectives of BF components. Lastly, we highlight the future trends of designing BFs. This is, to the best of our knowledge, the first survey that accomplishes such goals.","Optimization;Tutorials;Probabilistic logic;Market research;Proposals;Bloom filter;performance;generalization;false positive;false negative","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Luo et al. 2019 - Optimizing Bloom Filter - Challenges, Solutions, and Comparisons.pdf","","","",""
"Journal article","Joe V,Raj JS,Smys S.","","Towards efficient big data storage with MapReduce deduplication system","Int. j. inf. technol. web eng.","International journal of information technology and web engineering","","","","","","2021","16","2","45-57","All","","","","IGI Global","","","","","2021-04","","","","","1554-1045","1554-1053","http://dx.doi.org/10.4018/ijitwe.2021040103","10.4018/ijitwe.2021040103","","","","In the big data era, there is a high requirement for data storage and processing. The conventional approach faces a great challenge, and de-duplication is an excellent approach to reduce the storage space and computational time. Many existing approaches take much time to pinpoint the similar data. MapReduce de-duplication system is proposed to attain high duplication ratio. MapReduce is the parallel processing approach that helps to process large number of files in less time. The proposed system uses two threshold two divisor with switch algorithm for chunking. Switch is the average parameter used by TTTD-S to minimize the chunk size variance. Hashing using SHA-3 and fractal tree indexing is used here. In fractal index tree, read and write takes place at the same time. Data size after de-duplication, de-duplication ratio, throughput, hash time, chunk time, and de-duplication time are the parameters used. The performance of the system is tested by college scorecard and ZCTA dataset. The experimental results show that the proposed system can lessen the duplicity and processing time.","","","","VV College of Engineering, India; Gnanamani College of Technology, India; RVS Technical Campus, India","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Wei J,Jiang H,Zhou K,Feng D","","DBA: A Dynamic Bloom Filter Array for Scalable Membership Representation of Variable Large Data Sets","","","","","2011 IEEE 19th Annual International Symposium on Modelling, Analysis, and Simulation of Computer and Telecommunication Systems","","","2011","","","466-468","All","","","","","","","","","2011-07","","","","","2375-0227","","http://dx.doi.org/10.1109/MASCOTS.2011.36;https://ieeexplore.ieee.org/abstract/document/6005420/?casa_token=DOs-sR38N68AAAAA:p5Fw0TMzhn7YOOb8ghcMoiYRpRu8Ci7Kze8WMYueZCVX1xCxph9NyZEZokBalcbOr71XjSkVuY3kNA;https://ieeexplore.ieee.org/iel5/6004863/6005352/06005420.pdf?casa_token=-Jh18wk25NEAAAAA:MT8YpTxE6WTRxU2jQFKDlLrDDxO7-chvUqpQDO5iPNJUoqL2samtMEySexyOfi2e_F498iv5e6g0Pw","10.1109/MASCOTS.2011.36","","","","This paper proposes a Dynamic Bloom filter Array (DBA) to represent membership for variable large data sets in storage systems in a scalable way. DBA consists of dynamically created groups of space-efficient Bloom Filters (BFs) to accommodate changes in set sizes. In each group, BFs are homogeneous and the data layout is optimized at the bit level, so that they can be accessed in parallel to achieve high query performance. DBA can effectively control its query accuracy by partially adjusting the error rate of constructing BFs, where each BF corresponds to an independent subset of the data set to facilitate element location and membership confirmation. Further, DBA supports element deletion by introducing a lazy update policy. We prototype and evaluate our DBA scheme as a scalable fast index in the MAD2 deduplication storage system. Experimental results show that DBA (with 64 BFs per group) is capable of maintaining 90% of the peek query performance while scaling up to 160 BFs. DBA is also shown to excel in performance and space efficiency by theoretical analysis and other experiments based on real-world data sets.","Error analysis;Indexes;Fingerprint recognition;Filtering theory;Random access memory;Arrays;Accuracy;Data management;membership representation;fast index;Bloom filter","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Wei J,Jiang H,Zhou K,Feng D","","Efficiently Representing Membership for Variable Large Data Sets","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2014","25","4","960-970","Grouped by Publication/IEEE Trans Parallel Distrib Syst;All","","","","","","","","","2014-04","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2013.66;https://ieeexplore.ieee.org/abstract/document/6471979/;https://ieeexplore.ieee.org/iel7/71/4359390/06471979.pdf","10.1109/TPDS.2013.66","","","","Cloud computing has raised new challenges for the membership representation scheme of storage systems that manage very large data sets. This paper proposes DBA, a dynamic Bloom filter array aimed at representing membership for variable large data sets in storage systems in a scalable way. DBA consists of dynamically created groups of space-efficient Bloom filters (BFs) to accommodate changes in set sizes. Within a group, BFs are homogeneous and the data layout is optimized at the bit level to enable parallel access and thus achieve high query performance. DBA can effectively control its query accuracy by partially adjusting the error rate of the constructing BFs, where each BF only represents an independent subset to help locate elements and confirm membership. Further, DBA supports element deletion by introducing a lazy update policy. We prototype and evaluate our DBA scheme as a scalable fast index in the MAD2 deduplication storage system. Experimental results reveal that DBA (with 64 BFs per group) shows significantly higher query performance than the state-of-the-art approach while scaling up to 160 BFs. DBA is also shown to excel in scalability, query accuracy, and space efficiency by theoretical analysis and experimental evaluation.","Indexes;Error analysis;Peer-to-peer computing;Arrays;Random access memory;Servers;Distributed databases;Data management;fast index;membership representation;Bloom filter","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wei et al. 2014 - Efficiently Representing Membership for Variable Large Data Sets.pdf","","","",""
"Conference paper","Sun Y,Hua Y,Jiang S,Li Q,Cao S,Zuo P","","${$SmartCuckoo$}$: A Fast and ${$Cost-Efficient$}$ Hashing Index Scheme for Cloud Storage Systems","","","","","2017 USENIX Annual Technical Conference (USENIX ATC 17)","","","2017","","","553-565","All","","","","usenix.org","","","","","2017","","","","","","","https://www.usenix.org/conference/atc17/technical-sessions/presentation/sun;https://www.usenix.org/system/files/conference/atc17/atc17-sun.pdf","","","","","Fast query services are important to improve overall per-formance of large-scale storage systems when handling a large number of files. Open-addressing cuckoo hash schemes have …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sun et al. 2017 - ${$SmartCuckoo$}$ - A Fast and ${$Cost-Efficient$}$ Hashing Index Scheme for Cloud Storage Systems.pdf","","","",""
"Conference paper","Grawinkel M,Nagel L,Mäsker M,Padua F,Brinkmann A,Sorth L","","Analysis of the ${$ECMWF$}$ Storage Landscape","","","","","13th USENIX Conference on File and Storage Technologies (FAST 15)","","","2015","","","15-27","All","","","","","","","","","2015","","","","","","","https://www.usenix.org/conference/fast15/technical-sessions/presentation/grawinkel;https://www.usenix.org/system/files/conference/fast15/fast15-paper-grawinkel.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Grawinkel et al. 2015 - Analysis of the ${$ECMWF$}$ Storage Landscape.pdf","","","",""
"Conference paper","Li J,Chen X,Xhafa F,Barolli L","","Secure Deduplication Storage Systems with Keyword Search","","","","","2014 IEEE 28th International Conference on Advanced Information Networking and Applications","","","2014","","","971-977","All","","","","ieeexplore.ieee.org","","","","","2014-05","","","","","2332-5658","","http://dx.doi.org/10.1109/AINA.2014.118;https://ieeexplore.ieee.org/abstract/document/6838769/?casa_token=qepJ6NuUIcgAAAAA:QZYZPqgCQ8W__vLwI7oLzY7BzMyhpgE7DlJfH6M56q7dKTHzuyp4leAApDdB3IvRER9pNfsckx_Efg;https://ieeexplore.ieee.org/iel7/6835111/6838626/06838769.pdf?casa_token=QlXo4Z4RWxsAAAAA:KeeIkI6UgeLsOZny4B1sP1lBIt5obCgHuV_plCk2xW4P8kkek-FIeZmHo7J45JN3CqbDnnynQy3ltw","10.1109/AINA.2014.118","","","","Data deduplication is an attractive technology to reduce storage space and upload bandwidth for increasing vast amount of duplicated and redundant data. In a cloud storage system with data deduplication, duplicate copies of data will be eliminated and only one copy will be kept in the storage. To protect the confidentiality of sensitive data while supporting deduplication, the convergent encryption technique has been proposed to encrypt the data before outsourcing. However, the issue of keyword search over encrypted data in deduplication storage system has to be addressed for efficient data utilization. This paper firstly proposes two constructions which support secure keyword search in this scenario. In these constructions, the integrity of the data can be realized by just checking the convergent key, without other traditional integrity auditing mechanisms. Security analysis demonstrates that our keyword search schemes are secure in terms of the definitions specified in the proposed security model.","Servers;Cloud computing;Keyword search;Encryption;Indexes;Deduplication;distributed storage system;reliability;secret sharing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Secure Deduplication Storage Systems with Keyword Search.pdf","","","",""
"Journal article","Li J,Chen X,Xhafa F,Barolli L","","Secure deduplication storage systems supporting keyword search","J. Comput. System Sci.","Journal of Computer and System Sciences","","","","","","2015","81","8","1532-1541","All","","","","Elsevier","","","","","2015-12-01","","","","","0022-0000","","https://www.sciencedirect.com/science/article/pii/S0022000014001901;http://dx.doi.org/10.1016/j.jcss.2014.12.026","10.1016/j.jcss.2014.12.026","","","","Data deduplication is an attractive technology to reduce storage space for increasing vast amount of duplicated and redundant data. In a cloud storage system with data deduplication, duplicate copies of data will be eliminated and only one copy will be kept in the storage. To protect the confidentiality of sensitive data while supporting deduplication, the convergent encryption technique has been proposed to encrypt the data before outsourcing. However, the issue of keyword search over encrypted data in deduplication storage system has to be addressed for efficient data utilization. This paper firstly proposes two constructions which support secure keyword search in this scenario. In these constructions, the integrity of the data can be realized by just checking the convergent key, without other traditional integrity auditing mechanisms. Then, two extensions are presented to support fuzzy keyword search and block-level deduplication. Finally, security analysis is given.","Data deduplication; Outsourcing; Privacy; Keyword search","Data integrity constructions.","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2015 - Secure deduplication storage systems supporting keyword search.pdf","","","",""
"Journal article","Lin X,Hibler M,Eide E,Ricci R","","Using Deduplicating Storage for Efficient Disk Image Deployment","EAI Endorsed Trans. Scalable Inf. Syst.","","","","","","","2015","2","6","e1","All","","","","Citeseer","","","","","2015","","","","","","","https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.701.571&rep=rep1&type=pdf","","","","","Many clouds and network testbeds use disk images to initialize local storage on their compute devices. Large facilities must manage thousands or more images, requiring significant …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2015 - Using Deduplicating Storage for Efficient Disk Image Deployment.pdf","","","",""
"Conference paper","Long S,Li Z,Liu Z,Deng Q,Oh S,Komuro N","","A similarity clustering-based deduplication strategy in cloud storage systems","","","","","2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)","","","2020","","","35-43","All","Similarity/Resemblance","","","ieeexplore.ieee.org","","","","","2020-12","","","","","2690-5965","","http://dx.doi.org/10.1109/ICPADS51040.2020.00015;https://ieeexplore.ieee.org/abstract/document/9359136/?casa_token=p4G77i7PSCAAAAAA:7gVkPG36UlPPHUuvYt3SYkCJUeEwtWZB4IhW9djPo2o51uWkt5SQP82Nu_J5-OMJTK5lLrvwKl-EgQ;https://ieeexplore.ieee.org/iel7/9359105/9359106/09359136.pdf?casa_token=hbE37XwuW9IAAAAA:fn0grlZArRnfClYSsZg8lZEvj7Ic0QTS3bnxYt2TYBGHmXfB2QFe-pCk7_wuHoPirEBK_YqyttD2HA","10.1109/ICPADS51040.2020.00015","","","","Deduplication is a data redundancy elimination technique, designed to save system storage resources by reducing redundant data in cloud storage systems. With the development of cloud computing technology, deduplication has been increasingly applied to cloud data centers. However, traditional technologies face great challenges in big data deduplication to properly weigh the two conflicting goals of deduplication throughput and high duplicate elimination ratio. This paper proposes a similarity clustering-based deduplication strategy (named SCDS), which aims to delete more duplicate data without significantly increasing system overhead. The main idea of SCDS is to narrow the query range of fingerprint index by data partitioning and similarity clustering algorithms. In the data preprocessing stage, SCDS uses data partitioning algorithm to classify similar data together. In the data deletion stage, the similarity clustering algorithm is used to divide the similar data fingerprint superblock into the same cluster. Repetitive fingerprints are detected in the same cluster to speed up the retrieval of duplicate fingerprints. Experiments show that the deduplication ratio of SCDS is better than some existing similarity deduplication algorithms, but the overhead is only slightly higher than some high throughput but low deduplication ratio methods.","Cloud computing;Clustering algorithms;Estimation;Fingerprint recognition;Throughput;Partitioning algorithms;Classification algorithms;deduplication;cloud storage system;similarity clustering;data partitioning;block fingerprint","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Long et al. 2020 - A similarity clustering-based deduplication strategy in cloud storage systems.pdf","","","",""
"Preprint","Leibenger D,Sorge C","","sec-cs: Getting the Most out of Untrusted Cloud Storage","","","","","","arXiv [cs.CR]","","2016","","","","All","","","","","","","","","2016-06-10","","","","","","","http://arxiv.org/abs/1606.03368","","","1606.03368","","We present sec-cs, a hash-table-like data structure for file contents on untrusted storage that is both secure and storage-efficient. We achieve authenticity and confidentiality with zero storage overhead using deterministic authenticated encryption. State-of-the-art data deduplication approaches prevent redundant storage of shared parts of different contents irrespective of whether relationships between contents are known a priori or not. Instead of just adapting existing approaches, we introduce novel (multi-level) chunking strategies, ML-SC and ML-CDC, which are significantly more storage-efficient than existing approaches in presence of high redundancy. We prove sec-cs's security, publish a ready-to-use implementation, and present results of an extensive analytical and empirical evaluation that show its suitability for, e.g., future backup systems that should preserve many versions of files on little available cloud storage.","","","","","","","","","arXiv","1606.03368","cs.CR","","","","","","","","","","","","","","All Papers/L/Leibenger and Sorge 2016 - sec-cs - Getting the Most out of Untrusted Cloud Storage.pdf","","","",""
"Conference paper","Wu S,Tu Z,Wang Z,Shen Z,Mao B","","When Delta Sync Meets Message-Locked Encryption: a Feature-based Delta Sync Scheme for Encrypted Cloud Storage","","","","","2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)","","","2021","","","337-347","All","","","","","","","","","2021-07","","","","","2575-8411","","http://dx.doi.org/10.1109/ICDCS51616.2021.00040;https://ieeexplore.ieee.org/abstract/document/9546438/?casa_token=G8nxQ3oNB_UAAAAA:dd0SLW7LFK4hEFTMvKhuy8HwH1al09kQMj_Ol8Rlb6PqygFh12hbIlMcqXfGWRKQc-yxxZcZ6FxqAA;https://ieeexplore.ieee.org/iel7/9546301/9546401/09546438.pdf?casa_token=b7BSEjXCPNkAAAAA:Ga0EEMTH-kG_OylAC0CrKFH81FtkpkNnm-xcpuW4dblakkNx4ZEO62_1CpbHQGQpK8n8hk-YtHL2tw","10.1109/ICDCS51616.2021.00040","","","","As increasingly prevalent, more and more data are stored in the cloud storage, which brings us two major challenges. First, the modified files in the cloud should be quickly synchronized (sync) to ensure data consistency, e.g., delta sync achieves efficient cloud sync by synchronizing only the updated part of the file. Second, the huge data in the cloud needs to be deduplicated and encrypted, e.g., message-locked encryption (MLE) implements data deduplication by encrypting the content between different users. However, when both are combined, few updates in the content can cause large sync traffic amplification for both keys and ciphertext in the MLE-based cloud storage, which significantly degrading the cloud sync efficiency. In this paper, we propose an feature-based encryption sync scheme FeatureSync to improve the performance of synchronizing multiple encrypted files by merging several files before synchronizing. The performance evaluations on a lightweight prototype implementation of FeatureSync show that FeatureSync reduces the cloud sync time by 72.6% and the cloud sync traffic by 78.5% on average, compared with the state-of-the-art sync schemes.","Performance evaluation;Cloud computing;Maximum likelihood estimation;Conferences;Merging;Prototypes;Telecommunication traffic;Cloud Storage;Delta Synchronization;Message-Locked Encryption;Ciphertext Synchronization","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2021 - When Delta Sync Meets Message-Locked Encryption - a Feature-based Delta Sync Scheme for Encrypted Cloud Storage.pdf","","","",""
"Conference paper","Xing Y,Li Z,Dai Y","","PeerDedupe: Insights into the Peer-Assisted Sampling Deduplication","","","","","2010 IEEE Tenth International Conference on Peer-to-Peer Computing (P2P)","","","2010","","","1-10","All","","","","","","","","","2010-08","","","","","2161-3567","","http://dx.doi.org/10.1109/P2P.2010.5570004;https://ieeexplore.ieee.org/abstract/document/5570004/?casa_token=F80v3PvkoocAAAAA:0oPagyE9pOq3rL6he40gHeAYz1rqwCOz-leTeDp4wqwpY2va-GvhnAVYWGRwzPjvztqLLAzb1ta2dg;https://ieeexplore.ieee.org/iel5/5569876/5569954/05570004.pdf?casa_token=GnLGzbccp0oAAAAA:qbPx1V68sJmZnQVEvoFS4NNRTm-PGC5ca-fGMuOekwRtr-F_EqgRZ5kEgPMwq2k3hC4JuIk9Ei24aQ","10.1109/P2P.2010.5570004","","","","As the digital data rapidly inflates to a world-wide storage crisis, data deduplication is showing its increasingly prominent function in data storage. Driven by the problems behind the mainstream server-side deduplication schemes, recently there has been a tendency of introducing peer-assisted methods into the deduplication systems. However, this topic is still quite vague at present and lacks thorough research. In this paper, we conduct in-depth and quantitative investigation on the peer-assisted deduplication. Through measurements we observe that the inter-peer duplication accounts for a large proportion of the total duplication, and exhibits strong peer locality. Then based on our observations, we propose PeerDedupe, a novel peer-assisted sampling deduplication approach. Experiments show that PeerDedupe can remove over 98% duplication with each peer coordinating with no more than 5 other peers, and it requires much less server RAM usage than the existing works.","Estimation;Servers;Accuracy;Greedy algorithms;Weibull distribution;Sampling methods;Redundancy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xing et al. 2010 - PeerDedupe - Insights into the Peer-Assisted Sampling Deduplication.pdf","","","",""
"Conference paper","Li J,Wei G,Liang J,Ren Y,Lee PP,Zhang X","","Revisiting Frequency Analysis against Encrypted Deduplication via Statistical Distribution","","","","","IEEE INFOCOM 2022 - IEEE Conference on Computer Communications","","","2022","","","290-299","All","Frequency","","","IEEE","","","","","2022-05-02","","","9781665458221","9781665458238","2641-9874","0743-166X","http://dx.doi.org/10.1109/INFOCOM48880.2022.9796897;https://ieeexplore.ieee.org/abstract/document/9796897/?casa_token=d_JJBqHAk5YAAAAA:bZjEOzYEWvQeHy-fDC828ZuF9ovoa7tf6CJvvUofsx33d9XpYVf0rDHtn3xarYw9ycC0Zot_iO5IWw;https://ieeexplore.ieee.org/iel7/9796607/9796652/09796897.pdf?casa_token=zKVdoLWE5j4AAAAA:WB01DckASlECsC-AsJiGWIY2GEEjqKJkyFhq8YHyLZb4tpWhJVZ5-Dz5Z415qONZscKBJdoDtDX5mQ","10.1109/INFOCOM48880.2022.9796897","","","","Encrypted deduplication addresses both security and storage efficiency in large-scale storage systems: it ensures that each plaintext is encrypted to a ciphertext by a symmetric key derived from the content of the plaintext, so as to allow deduplication on the ciphertexts derived from duplicate plaintexts. However, the deterministic nature of encrypted deduplication leaks the frequencies of plaintexts, thereby allowing adversaries to launch frequency analysis against encrypted deduplication and infer the ciphertext-plaintext pairs in storage. In this paper, we revisit the security vulnerability of encrypted deduplication due to frequency analysis, and show that encrypted deduplication can be even more vulnerable to the sophisticated frequency analysis attack that exploits the underlying storage workload characteristics. We propose the distribution-based attack, which builds on a statistical approach to model the relative frequency distributions of plaintexts and ciphertexts, and improves the inference precision (i.e., have high confidence on the correctness of inferred ciphertext-plaintext pairs) of the previous attack. We evaluate the new attack against real-world storage workloads and provide insights into its actual damage.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2022 - Revisiting Frequency Analysis against Encrypted Deduplication via Statistical Distribution.pdf","","","",""
"Conference paper","Shi E,Chan TH,Stefanov E,Li M","","Oblivious RAM with O((logN)3) Worst-Case Cost","","","","","Advances in Cryptology – ASIACRYPT 2011","","","2011","","","197-214","All","","","","Springer Berlin Heidelberg","","","","","2011","","","","","","","http://dx.doi.org/10.1007/978-3-642-25385-0_11;https://link.springer.com/chapter/10.1007/978-3-642-25385-0_11;https://link.springer.com/content/pdf/10.1007/978-3-642-25385-0_11.pdf","10.1007/978-3-642-25385-0_11","","","","Oblivious RAM is a useful primitive that allows a client to hide its data access patterns from an untrusted server in storage outsourcing applications. Until recently, most prior works on Oblivious RAM aim to optimize its amortized cost, while suffering from linear or even higher worst-case cost. Such poor worst-case behavior renders these schemes impractical in realistic settings, since a data access request can occasionally be blocked waiting for an unreasonably large number of operations to complete.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shi et al. 2011 - Oblivious RAM with O((logN)3) Worst-Case Cost.pdf","","","",""
"Journal article","Youn TY,Chang KY,Rhee KH,Shin SU","","Efficient Client-Side Deduplication of Encrypted Data With Public Auditing in Cloud Storage","IEEE Access","","","","","","","2018","6","","26578-26587","All","","","","","","","","","2018","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2018.2836328;https://ieeexplore.ieee.org/abstract/document/8359096/;https://ieeexplore.ieee.org/iel7/6287639/6514899/08359096.pdf","10.1109/ACCESS.2018.2836328","","","","At present, there is a considerable increase in the amount of data stored in storage services, along with dramatic evolution of networking techniques. In storage services with huge data, the storage servers may want to reduce the volume of stored data, and the clients may want to monitor the integrity of their data with a low cost, since the cost of the functions related to data storage increase in proportion to the size of the data. To achieve these goals, secure deduplication and integrity auditing delegation techniques have been studied, which can reduce the volume of data stored in storage by eliminating duplicated copies and permit clients to efficiently verify the integrity of stored files by delegating costly operations to a trusted party, respectively. So far many studies have been conducted on each topic, separately, whereas relatively few combined schemes, which support the two functions simultaneously, have been researched. In this paper, we design a combined technique, which performs both secure deduplication of encrypted data and public integrity auditing of data. To support the two functions, the proposed scheme performs challenge-response protocols using the BLS signature-based homomorphic linear authenticator. We utilize a third party auditor for performing public audit, in order to help low-powered clients. The proposed scheme satisfies all the fundamental security requirements. We also propose two variances that provide higher security and better performance.","Cloud computing;Cryptography;Servers;Cascading style sheets;Protocols;Authentication;Cloud storage;cryptography;data security;information security;public audit;secure deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Youn et al. 2018 - Efficient Client-Side Deduplication of Encrypted Data With Public Auditing in Cloud Storage.pdf","","","",""
"Book chapter","Tarasov V,Mudrankit A,Buik W,Shilane P,Kuenning G,Zadok E","Kuenning G","FSL-Dedup Traces (SNIA IOTTA Trace Set 5228)","","","SNIA IOTTA Trace Repository","","","","","2016","","","","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","Storage Networking Industry Association","","","","","2016-05","","","","","","","http://iotta.snia.org/traces/static?only=5228","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Stefanov E,van Dijk M,Shi E,Fletcher C,Ren L,Yu X,Devadas S","","Path ORAM","Proceedings of the 2013 ACM SIGSAC conference on Computer & communications security - CCS '13","","","","","","","2013","","","","All","","","","","","","","","2013","","","","","","","http://dx.doi.org/10.1145/2508859.2516660","10.1145/2508859.2516660","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Stefanov et al. 2013 - Path ORAM.pdf","","","",""
"Miscellaneous","Ateniese G,Burns R,Curtmola R,Herring J,Kissner L,Peterson Z,Song D","","Provable data possession at untrusted stores","Proceedings of the 14th ACM conference on Computer and communications security  - CCS '07","","","","","","","2007","","","","SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;All","","","","","","","","","2007","","","","","","","http://dx.doi.org/10.1145/1315245.1315318","10.1145/1315245.1315318","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Ateniese et al. 2007 - Provable data possession at untrusted stores.pdf","","","",""
"Journal article","Saeed AS,George LE","","Fingerprint-Based Data Deduplication Using a Mathematical Bounded Linear Hash Function","Symmetry ","Symmetry","","","","","","2021","13","11","1978","All","","","","Multidisciplinary Digital Publishing Institute","","","","","2021-10-20","","2021-12-14","","","","","https://www.mdpi.com/1320586;http://dx.doi.org/10.3390/sym13111978;https://www.mdpi.com/2073-8994/13/11/1978/pdf","10.3390/sym13111978","","","","Due to the quick increase in digital data, especially in mobile usage and social media, data deduplication has become a vital and cost-effective approach for removing redundant data segments, reducing the pressure imposed by enormous volumes of data that must be kept. As part of the data deduplication process, fingerprints are employed to represent and identify identical data blocks. However, when the amount of data increases, the number of fingerprints grows as well, and due to the restricted memory size, the speed of data deduplication suffers dramatically. Various deduplication solutions show a bottleneck in the form of matching lookups and chunk fingerprint calculations, for which we pay in the form of storage and processors needed for storing hashes. Utilizing a fast hash algorithm to improve the fingerprint lookup performance is an appealing challenge. Thus, this study is focused on enhancing the deduplication system by suggesting a novel and effective mathematical bounded linear hashing algorithm that decreases the hashing time by more than two times compared to MD5 and SHA-1 and reduces the size of the hash index table by 50%. Due to the enormous number of chunk hash values, looking up and comparing hash values takes longer for large datasets; this work offers a hierarchal fingerprint lookup strategy to minimize the hash judgement comparison time by up to 78%. Our suggested system reduces the high latency imposed by deduplication procedures, primarily the hashing and matching phases. The symmetry of our work is based on the balance between the proposed hashing algorithm performance and its reflection on the system efficiency, as well as evaluating the approximate symmetries of the hashing and lookup phases compared to the other deduplication systems.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/S/Saeed and George 2021 - Fingerprint-Based Data Deduplication Using a Mathematical Bounded Linear Hash Function.pdf","","","",""
"Journal article","Saeed AS,George LE","","Data Deduplication System Based on Content-Defined Chunking Using Bytes Pair Frequency Occurrence","Symmetry ","Symmetry","","","","","","2020","12","11","1841","All","","","","Multidisciplinary Digital Publishing Institute","","","","","2020-11-06","","2021-12-14","","","","","https://www.mdpi.com/882258;http://dx.doi.org/10.3390/sym12111841;https://www.mdpi.com/2073-8994/12/11/1841/pdf","10.3390/sym12111841","","","","Every second, millions of data are being generated due to the use of emerging technologies. It is very challenging to store and handle such a large amount of data. Data deduplication is a solution for this problem. It is a new technique that eliminates duplicate data and stores only a single copy of data, reducing storage utilization and the cost of maintaining redundant data. Content-defined chunking (CDC) has been playing an important role in data deduplication systems due to its ability to detect high redundancy. In this paper, we focused on deduplication system optimization by tuning relevant factors in CDC to identify chunk cut-points and introduce an efficient fingerprint using a new hash function. We proposed a novel bytes frequency-based chunking (BFBC) algorithm and a new low-cost hashing function. To evaluate the efficiency of the proposed system, extensive experiments were done using two different datasets. In all experiments, the proposed system persistently outperformed the common CDC algorithms, achieving a better storage gain ratio and enhancing both chunking and hashing throughput. Practically, our experiments show that BFBC is 10 times faster than basic sliding window (BSW) and approximately three times faster than two thresholds two divisors (TTTD). The proposed triple hash function algorithm is five times faster than SHA1 and MD5 and achieves a better deduplication elimination ratio (DER) than other CDC algorithms. The symmetry of our work is based on the balance between the proposed system performance parameters and its reflection on the system efficiency compared to other deduplication systems.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/S/Saeed and George 2020 - Data Deduplication System Based on Content-Defined Chunking Using Bytes Pair Frequency Occurrence.pdf","","","",""
"Conference paper","Zhang Y,Wu Y,Yang G","","Droplet: A Distributed Solution of Data Deduplication","","","","","2012 ACM/IEEE 13th International Conference on Grid Computing","","","2012","","","114-121","All","","","","","","","","","2012-09","","","","","2152-1093","","http://dx.doi.org/10.1109/Grid.2012.21;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6319161","10.1109/Grid.2012.21","","","","Creating backup copies is the most commonly used technique to protect from data loss. In order to increase reliability, doing routinely backup is a best practice. Such backup activities will create multiple redundant data streams which is not economic to be directly stored on disk. Similarly, enterprise archival systems usually deal with redundant data, which needs to be stored for later accessing. Deduplication is an essential technique used under these situations, which could avoid storing identical data segments, and thus saves a significant portion of disk usage. Also, recent studies have shown that deduplication could also effectively reduce the disk space used to store virtual machine (VM) disk images. We present droplet, a distributed deduplication storage system that has been designed for high throughput and scalability. Droplet strips input data streams onto multiple storage nodes, thus limits number of stored data segments on each node and ensures the fingerprint index could be fitted into memory. The in-memory finger index avoids the disk bottleneck discussed in Data Domain, ChunkStash and provides excellent lookup performance. The buffering layer in droplet provides good write performance for small data segments. Compression on date segments reduces disk usage one step further.","Servers;Indexes;Containers;Random access memory;Fingerprint recognition;Throughput;Virtual machining;deduplication;storage system;cluster","Uses high-byte in fingerprint to distubute chunks to servers, but no encryption, uses 64k fixed size blocks.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2012 - Droplet - A Distributed Solution of Data Deduplication.pdf","","","",""
"Conference paper","Hamilton J,Olsen EW","","Design and implementation of a storage repository using commonality factoring","","","","","20th IEEE/11th NASA Goddard Conference on Mass Storage Systems and Technologies, 2003. (MSST 2003). Proceedings.","","","2003","","","178-182","Grouped by Publication/MSST;All","","","","","","","","","2003-04","","","","","","","http://dx.doi.org/10.1109/MASS.2003.1194854;https://ieeexplore.ieee.org/abstract/document/1194854/?casa_token=zGT2jCftaXEAAAAA:ZRHdflHaETwdsT-ftvucvKEwIkzIw1WsDlibyiIaeKe_40LX1QnA6NiBYxeJim_p2wre4A8LV2NOeg;https://ieeexplore.ieee.org/iel5/8502/26874/01194854.pdf?casa_token=xOZHarZTSg8AAAAA:M4inbDt1pgQCG3_kPCkw4Avk-Cmns_r_YjK4wNoZa-Z1Dunou9ZCswb5Fe7iXTNKFyOxBJzXsY1GHQ","10.1109/MASS.2003.1194854","","","","In this paper, we discuss the design of a data normalization system that we term commonality factoring. A real-world implementation of a storage system based upon data normalization requires design of the data normalization itself, of the storage repository for the data, and of the protocols to be used between applications performing data normalization and the server software of the repository. Each of these areas is discussed and potential applications are presented. Building on research begun in 1999, Avamar Technologies has implemented an initial application of this technology to provide a nearline, disk-based system for backup of primary storage.","Costs;Compression algorithms;Application software;Delay;Protocols;Software performance;Buildings;Genomics;Bioinformatics;Fuel economy","Tree of hashes for file system.","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hamilton and Olsen 2003 - Design and implementation of a storage repository using commonality factoring.pdf","","","",""
"Journal article","Bloom BH","","Space/time trade-offs in hash coding with allowable errors","Commun. ACM","Communications of the ACM","","","","","","1970","13","7","422-426","All","","","","Association for Computing Machinery","New York, NY, USA","","","","1970-07-01","","","","","0001-0782","","https://doi.org/10.1145/362686.362692;http://dx.doi.org/10.1145/362686.362692;https://dl.acm.org/doi/abs/10.1145/362686.362692?casa_token=eukfd9gFQnkAAAAA:8SN1-lFkbrAh_CH9TzZxXOSfx64YTurtH3KViibBOc4QZNuwgIgx39fhQdnFsNIRPz0gkd53H99d;https://dl.acm.org/doi/pdf/10.1145/362686.362692?casa_token=u4sKq6xtkiwAAAAA:QO8kF3DFr0aANnLsWZ17jCgUcYnZErTs17XtozxEZ2hb_QJAkUmKz3S4CIDMorUAPAMwHm2im7c3","10.1145/362686.362692","","","","In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency.The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods.Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.","searching, storage efficiency, retrieval trade-offs, scatter storage, hash coding, retrieval efficiency, storage layout, hash addressing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bloom 1970 - Space - time trade-offs in hash coding with allowable errors.pdf","","","","July 1970"
"Website","You LL","","Evaluation of efficient archival storage techniques","","","","","","","","","","","","All","Chunking;Metadata","","","","","","","","","","2021-11-19","","","","","https://www.ssrc.ucsc.edu/media/pubs/ec14866b71bd02292b58678a89ba37dc66247c26.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/You - Evaluation of efficient archival storage techniques.pdf","","","",""
"Thesis","Stephenson JP","","Fine-grain in-memory deduplication for large-scale workloads","","","","","","","","2013","","","","All","","","","","","","","","2013-12","","","","","","","","","","","","","","","","","","","","","","","","","Stanford University","Electical Engineering","","","","","","","","","","","All Papers/S/Stephenson 2013 - Fine-grain in-memory deduplication for large-scale workloads.pdf","","PhD","",""
"Website","Yang J,Plasson N,Gillis G,Talagala N","","Don't stack your Log on my Log","","","","","","","","","","","","All","","","","","","","","","","","2021-10-15","","","","","https://www.usenix.org/system/files/conference/inflow14/inflow14-yang.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. - Don't stack your Log on my Log.pdf","","","",""
"Conference paper","Kim H,Yeom HY,Son Y","","An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging","","","","","2020 IEEE 13th International Conference on Cloud Computing (CLOUD)","","","2020","","","405-413","All;Grouped by Publication/IEEE Trans Cloud Computing;Grouped by Publication/IEEE Cloud Conference","sorted deduplication","","","","","","","","2020-10","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD49709.2020.00062;https://ieeexplore.ieee.org/abstract/document/9284224/?casa_token=KrGVM5T0gFAAAAAA:IdzyIhoOW6H5-QipP9TpFAXpFGE8A98VrKxlrlW3t7pnjQG7cPsWhzMev9HfXOYKpAH_BU_5Ddh0;https://ieeexplore.ieee.org/iel7/9284144/9284201/09284224.pdf?casa_token=PvxhiX1ZDXwAAAAA:RTJKIjBIVxM0NifawIBhYpgD-9KTv8IfRxIfy6NFgS3Qj-V_QnCxHQFHIouSReXqWJfoI3gjkqNL","10.1109/CLOUD49709.2020.00062","","","","Many cloud services perform periodic database backup to keep the data safe from failures such as sudden system crashes. In the database system, two techniques are widely used for data backup and recovery: a physical backup and a logical backup. The physical backup uses raw data by copying the files in the database, whereas the logical backup extracts data from the database and dumps it into separated files as a sequence of query statements. Both techniques support a full backup strategy that contains data of the entire database and incremental backup strategy that contains changed data since a previous backup. However, both strategies require additional I/O operations to perform the backup and need a long time to restore a backup. In this paper, we propose an efficient backup and recovery scheme by exploiting write-ahead logging (WAL) in database systems. In the proposed scheme, for backup, we devise a backup system to use log data generated by the existing WAL to eliminate the additional I/O operations. To restore a backup, we utilize and optimize the existing crash recovery procedure of WAL to reduce recovery time. For example, we divide the recovery range and applying the backup data for each range independently via multiple threads. We implement our scheme in MySQL, a popular database management system. The experimental result demonstrates that the proposed scheme provides instant backup while reducing recovery time compared with the existing schemes.","Cloud computing;Instruction sets;Conferences;Database systems;Computer crashes;Data mining;Database Management System;Data Management;Database Backup and Recovery;Write-Ahead Logging","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2020 - An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging.pdf","","","",""
"Conference paper","Li J,Yang Z,Ren Y,Lee PP,Zhang X","","Balancing storage efficiency and data confidentiality with tunable encrypted deduplication","","","","","Proceedings of the Fifteenth European Conference on Computer Systems","","","2020","","","1-15","All","Datasets;Tunable;Frequency","","Article 22","Association for Computing Machinery","New York, NY, USA","","Heraklion, Greece","","2020-04-17","","2021-10-09","9781450368827","","","","https://doi.org/10.1145/3342195.3387531;http://dx.doi.org/10.1145/3342195.3387531;https://dl.acm.org/doi/abs/10.1145/3342195.3387531?casa_token=LN091qAbgHIAAAAA:2rt3rhUAnAjdq-mNKWro6irqMJEO61lRVl2Rh6Hrh-sQtAVOtCa__sF23M1abn48YszSl4j9cJznrg;https://dl.acm.org/doi/pdf/10.1145/3342195.3387531?casa_token=O--7Fdiq20MAAAAA:dNKHV345T_37NOvkZxvcDsGkhafxM_xrKPzAFIvXP7pMVbWSK2zZezo9SlIMGDjWAUXrpVdBUzSxUg;https://dl.acm.org/doi/pdf/10.1145/3342195.3387531","10.1145/3342195.3387531","","","","Conventional encrypted deduplication approaches retain the deduplication capability on duplicate chunks after encryption by always deriving the key for encryption/decryption from the chunk content, but such a deterministic nature causes information leakage due to frequency analysis. We present TED, a tunable encrypted deduplication primitive that provides a tunable mechanism for balancing the tradeoff between storage efficiency and data confidentiality. The core idea of TED is that its key derivation is based on not only the chunk content but also the number of duplicate chunk copies, such that duplicate chunks are encrypted by distinct keys in a controlled manner. In particular, TED allows users to configure a storage blowup factor, under which the information leakage quantified by an information-theoretic measure is minimized for any input workload. We implement an encrypted deduplication prototype TEDStore to realize TED in networked environments. Evaluation on real-world file system snapshots shows that TED effectively balances the trade-off between storage efficiency and data confidentiality, with small performance overhead.","","","","","","","EuroSys '20","22","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2020 - Balancing storage efficiency and data confidentiality with tunable encrypted deduplication.pdf","","","",""
"Journal article","Li J,Huang S,Ren Y,Yang Z,Lee PP,Zhang XS,Hao Y","","Enabling Secure and Space-Efficient Metadata Management in Encrypted Deduplication","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2021","","","1-1","All;Grouped by Publication/IEEE Transactions on Computers;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Two-stage","","","","","","","","undefined 2021","","","","","0018-9340","","http://dx.doi.org/10.1109/TC.2021.3067326;https://ieeexplore.ieee.org/abstract/document/9381688/?casa_token=EWv4_E_8H2YAAAAA:9Q8bc3gksSwS1A5xLP-oHSDrhA5KLLr09WRpCPPgMGveNJRPrgo5Zl1d4b7Y39KVeEDPY0IrqUMoXQ;https://ieeexplore.ieee.org/iel7/12/4358213/09381688.pdf?casa_token=IAUnue4pozYAAAAA:Dgm5Q3oKvfNKMW5ubGj6ClJdOldXvDHXWY2qpQXTLC18jgxd3f7DAWPBmcm9kIuZL_UDUSXdrQl64g","10.1109/TC.2021.3067326","","","","Encrypted deduplication combines encryption and deduplication in a seamless way to provide confidentiality guarantees for the physical data in deduplicated storage, yet it incurs substantial metadata storage overhead due to the additional storage of keys. We present a new encrypted deduplication storage system called Metadedup, which suppresses metadata storage by also applying deduplication to metadata. Its idea builds on indirection, which adds another level of metadata chunks that record metadata information. We find that metadata chunks are highly redundant in real-world workloads and hence can be effectively deduplicated. We further extend Metadedup to incorporate multiple servers via a distributed key management approach, so as to provide both fault-tolerant storage and security gaurantees. We extensively evaluate Metadedup from performance and storage efficiency perspectives. We show that Metadedup achieves high throughput in writing and restoring files, and saves the metadata storage by up to 93.94% for real-world backup workloads.","Metadata;Cryptography;Maximum likelihood estimation;Encryption;Security;Indexes;Servers;Encrypted deduplication;metadata management;cloud storage","This is Metadedup with distributed key management.","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2021 - Enabling Secure and Space-Efficient Metadata Management in Encrypted Deduplication.pdf;All Papers/L/Li et al. 2021 - tc21metadedup_supp.pdf","","","",""
"Conference paper","Zou X,Yuan J,Shilane P,Xia W,Zhang H,Wang X","","The Dilemma between Deduplication and Locality: Can Both be Achieved?","","","","","19th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 21)","","","2021","","","171-185","Grouped by Publication/FAST Papers;All","","","","","","","","","2021","","","","","","","https://www.usenix.org/conference/fast21/presentation/zou;https://www.usenix.org/system/files/fast21-zou.pdf","","","","","","","Restore performance","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zou et al. 2021 - The Dilemma between Deduplication and Locality - Can Both be Achieved.pdf","","","",""
"Journal article","Tian W,Li R,Xu Z","","TSS: A two‐party secure server‐aid chunking algorithm","Concurr. Comput.","Concurrency and computation: practice & experience","","","","","","2021","","","","All","","","","Wiley","","","","","2021-09-21","","","","","1532-0626","1532-0634","https://onlinelibrary.wiley.com/doi/10.1002/cpe.6577;http://dx.doi.org/10.1002/cpe.6577;https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6577","10.1002/cpe.6577","","","","Abstract Chunking is one of the most important processes in a secure deduplication system. It determines the deduplication ratio, metadata size, and the encryption key number. Existing chunking algorithms can be categorized as client-side chunking algorithm and the server-aid chunking algorithm. Although the former one can directly be applied into the secure deduplication, it ignores the management overhead caused by the metadata size and encryption key number. Thus, the conventional server-aid chunking algorithm is proposed. However, it is not secure because the interaction between the client and the server is based on the plaintext. Thus, we propose a two-party secure server-aid chunking algorithm, TSS. It supports the secure server-aid interaction and reduces the metadata size and the encryption key number in the secure deduplication with a comparable deduplication ratio. The theoretical proof and experimental results show that our method outperforms the state-of-the-art chunking algorithm, elastic chunking algorithm.","","","http://onlinelibrary.wiley.com/termsAndConditions#vor","School of Computer Science and Technology University of South China Hengyang China; School of Computer Science and Technology Huazhong University of Science and Technology Wuhan China; Math and Computer Science Department Suffolk University Boston Massachusetts USA; Shenzhen Institute of Advanced Technology Chinese Academy of Science Shenzhen China","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Quero LC,Lee YS,Kim JS","","Self-sorting SSD: Producing sorted data inside active SSDs","","","","","2015 31st Symposium on Mass Storage Systems and Technologies (MSST)","","","2015","","","1-7","All;Grouped by Publication/MSST","","","","","","","","","2015-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2015.7208281;https://ieeexplore.ieee.org/abstract/document/7208281?casa_token=kQ5YrThQtnIAAAAA:Tl97UjHCg9NrlmqZ5pj6jRolHM3GmuQN_Pzfwf5ssq5EEgL2JEAJ6TrjVH1PJncjsTwuKvYhvKLpxQ","10.1109/MSST.2015.7208281","","","","Nowadays solid state drives (SSDs) are gaining popularity and are replacing magnetic hard disk drives (HDDs) in enterprise storage systems. As a result, extracting the maximum performance from SSDs is becoming crucial to deal with the increasing storage volume and performance needs. Active disks were introduced as a way to offload data-processing tasks from the host into disks freeing system resources and achieving better performance. In this work, we present an active SSD architecture called Self-Sorting SSD that targets to offload sorting operations which are commonly used in data-intensive and database environments and that require heavy data transfer. Processing sorting operations directly on the SSD reduces data transfer from/to the storage devices, increasing system performance and the lifetime of SSDs. Experiments on a real SSD platform reveal that our proposed architecture outperforms traditional external merge sort by up to 60.75%, reduces energy consumption by up to 58.86%, and eliminates all the data transfer overhead to compute sorted results.","Sorting;Ash;Indexing;Data structures;Memory management","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Q/Quero et al. 2015 - Self-sorting SSD - Producing sorted data inside active SSDs.pdf","","","",""
"Journal article","Zhang B,Hsu M","","Unsafe operations in B-trees","Acta Inform.","Acta Informatica","","","","","","1989","26","5","421-438","All","","","","","","","","","1989-03-01","","","","","0001-5903","1432-0525","https://doi.org/10.1007/BF00289145;http://dx.doi.org/10.1007/BF00289145;https://link.springer.com/article/10.1007/BF00289145","10.1007/BF00289145","","","","A simple mathematical model for analyzing the dynamics of a B-tree node is presented. From the solution of the model, it is shown that the simple technique of allowing a B-tree node to be slightly less than half full can significantly reduce the rate of split, merge and borrow operations. We call split, merge, borrow and balance operations unsafe operations in this paper. In a multi-user environment, a lower unsafe operation rate implies less blocking and higher throughput, even when tailored concurrency control algorithms (e.g., that proposed by Lehman and Yao [10]) are used. A lower unsafe operation rate also means a longer life time of an optimally initialized B-tree (e.g., compact B-tree). It is in general useful to have an analytical model which can predict the rate of unsafe operations in a dynamic data structure, not only for comparing the behavior of variations of B-trees, but also for characterizing workload for performance evaluation of different concurrency control algorithms for such data structures. The model presented in this paper represents a starting point in this direction.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","O'Neil PE","","TheSB-tree an index-sequential structure for high-performance sequential access","Acta Inform.","Acta Informatica","","","","","","1992","29","3","241-265","All","","","","","","","","","1992-03-01","","","","","0001-5903","1432-0525","https://doi.org/10.1007/BF01185680;http://dx.doi.org/10.1007/BF01185680;https://link.springer.com/article/10.1007/BF01185680;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.9482&rep=rep1&type=pdf","10.1007/BF01185680","","","","A variant of aB-tree known as anSB-tree is introduced, with the object of offering high-performance sequential disk access for long range retrievals. The key to this efficiency is a structure that supports multi-page reads (or writes) during sequential access to any node level below the root, even following significant node splitting. In addition, theSB-tree will support a policy to ‘stripe’ successive multi-page blocks on multiple disks to achieve maximum parallelism. Compared to traditionalB-tree structures,SB-tree performance characteristics are less subject to degradation resulting from modifications entailed in growing and shrinking;SB-trees are therefore more appropriate for use in situations where frequent reorganization is not possible. A performance analysis reveals the strengths of theSB-tree by comparing its performance under various circumstances to theB+-tree and the bounded disorder (BD) file of [11]. The performance analysis formulates a new useful concept, the ‘effective depth’ of anSB- orB+-tree, defined as the expected number of pages read from disk to perform a random retrieval search given standard buffering behavior. A graph of effective depth against tree size is shown to have a scalloped appearance, reflecting the changing effectiveness of incremental additions to buffer space.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/O/O'Neil 1992 - TheSB-tree an index-sequential structure for high-performance sequential access.pdf","","","",""
"Conference paper","Baiwen F,Lianying S","","Analysis on the Comparison Methods of Two Sorted Lists in Total-Order Set","","","","","2014 International Conference on Virtual Reality and Visualization","","","2014","","","367-371","All","","","","","","","","","2014-08","","","","","","","http://dx.doi.org/10.1109/ICVRV.2014.59;https://ieeexplore.ieee.org/abstract/document/7281094?casa_token=NBHUzG0H7iQAAAAA:dZGjb-6WqJfnmAu-8MUZuxGwhHgNXk1jFg37MZLwUgcyHyR9yMIbjCDgPAknHsf3PXcKPlaZNPTTZIU","10.1109/ICVRV.2014.59","","","","The comparison methods for two sorted lists on total-order set have been discussed. Several cases are put forward to and the maximum value, the minimum value and the calculation formula for expected value of comparison times between two sorted lists are given and its correctness is proved. The relations between the comparison times and the size of the problem have been discovered by the experiment results and some advices are proposed for the design of lottery and some other activities of this type. The method has been applied in computer simulation for one kind of sports lottery and the consistency is shown by the experiment results between the values of computer simulation and the values of theory method.","Algorithm design and analysis;Computer simulation;Software engineering;Electronic mail;Games;Sun;Probability;algorithm;comparison;sorted list;probability","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Baiwen and Lianying 2014 - Analysis on the Comparison Methods of Two Sorted Lists in Total-Order Set.pdf","","","",""
"Journal article","Ozalp OC,Akin M","","Optimization of Merge Based Sort Algorithms on Nearly Sorted Lists","wseas.us","","","","","","","","","","","All","","","","","","","","","","","","","","","","http://www.wseas.us/e-library/conferences/2012/Sliema/MACMESE/MACMESE-21.pdf","","","","","… Abstract: The Merge Sort is a well-known comparison -based sort algorithm that has the same … time complexity of merge operation, our algorithm is basically based on finding two non- decreasing … and Table-2, we've seen that our algorithm gives better results comparing to other …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/O/Ozalp and Akin - Optimization of Merge Based Sort Algorithms on Nearly Sorted Lists.pdf","","","",""
"Journal article","Christen P","","A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication","IEEE Trans. Knowl. Data Eng.","IEEE transactions on knowledge and data engineering","","","","","","2012","24","9","1537-1555","All","","","","","","","","","2012-09","","","","","1041-4347","1558-2191","http://dx.doi.org/10.1109/TKDE.2011.127;https://ieeexplore.ieee.org/abstract/document/5887335/?casa_token=cxmehBMEC6UAAAAA:-_lEFlXWPuWl9gl2xMDBviq3EHWAgetryxIlNiw4UloYv4eKys9o0KHYVAU6TyXMRoYOirVGDP2SzM8;https://ieeexplore.ieee.org/iel5/69/4358933/05887335.pdf?casa_token=ckzpX8pp6E0AAAAA:PhFJkBpvHhmlqVkAY7oMXsnx-RHJnqdXrq1s-tbB7aCSJ-1LYul2q--ikVoYNNaymF1hSqodtFskuR8","10.1109/TKDE.2011.127","","","","Record linkage is the process of matching records from several databases that refer to the same entities. When applied on a single database, this process is known as deduplication. Increasingly, matched data are becoming important in many application areas, because they can contain information that is not available otherwise, or that is too costly to acquire. Removing duplicate records in a single database is a crucial step in the data cleaning process, because duplicates can severely influence the outcomes of any subsequent data processing or data mining. With the increasing size of today's databases, the complexity of the matching process becomes one of the major challenges for record linkage and deduplication. In recent years, various indexing techniques have been developed for record linkage and deduplication. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious nonmatching pairs, while at the same time maintaining high matching quality. This paper presents a survey of 12 variations of 6 indexing techniques. Their complexity is analyzed, and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets. No such detailed survey has so far been published.","Couplings;Indexing;Encoding;Complexity theory;Data linkage;data matching;entity resolution;index techniques;blocking;experimental evaluation;scalability","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Christen 2012 - A Survey of Indexing Techniques for Scalable Record Linkage and Deduplication.pdf","","","",""
"Conference paper","Kaiser J,Brinkmann A,Süß T,Meister D","","Deriving and comparing deduplication techniques using a model-based classification","","","","","Proceedings of the Tenth European Conference on Computer Systems","","","2015","","","1-13","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","","","Article 11","Association for Computing Machinery","New York, NY, USA","","Bordeaux, France","","2015-04-17","","2021-07-30","9781450332385","","","","https://doi.org/10.1145/2741948.2741952;http://dx.doi.org/10.1145/2741948.2741952;https://dl.acm.org/doi/abs/10.1145/2741948.2741952?casa_token=ICM7l5pJkMsAAAAA:3dDdhHfh-7GvdtLjjO4dgdo1eAWhbf3h2ndKnM8yryyuWXNmASVztmCzXkVW0BhZHaCXZ-uiQWnz3wA;https://dl.acm.org/doi/pdf/10.1145/2741948.2741952?casa_token=2gGcsod5RyYAAAAA:S3QfAbPBjCBHhiLf1LfOfF2dt2LWdoztWT8E1ttT87C8DP9viHAhqmcyRytYGA52NRre0rs2lOL_U5g","10.1145/2741948.2741952","","","","Data deduplication has been a hot research topic and a large number of systems have been developed. These systems are usually seen as an inherently linked set of characteristics. However, a detailed analysis shows independent concepts that can be used in other systems.In this work, we perform this analysis on the main representatives of deduplication systems. We embed the results in a model, which shows two yet unexplored combinations of characteristics. In addition, the model enables a comprehensive evaluation of the representatives and the two new systems. We perform this evaluation based on real world data sets.","","","","","","","EuroSys '15","11","","","","","","","","","","","","","","","","","All Papers/K/Kaiser et al. 2015 - Deriving and comparing deduplication techniques using a model-based classification.pdf","","","",""
"Conference paper","Mandal S,Kuenning G,Ok D,Shastry V,Shilane P,Zhen S,Tarasov V,Zadok E","","Using hints to improve inline block-layer deduplication","","","","","14th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 16)","","","2016","","","315-322","All","Deduplication Systems","","","","","","","","2016","","","","","","","https://www.usenix.org/conference/fast16/technical-sessions/presentation/mandal;https://www.usenix.org/system/files/conference/fast16/fast16-papers-mandal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Mandal et al. 2016 - Using hints to improve inline block-layer deduplication.pdf","","","",""
"Conference paper","Zhang W,Yang T,Narayanasamy G,Tang H","","Low-cost data deduplication for virtual machine backup in cloud storage","","","","","5th ${$USENIX$}$ Workshop on Hot Topics in Storage and File Systems (HotStorage 13)","","","2013","","","","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Routing with Chunks","","","","","","","","2013","","","","","","","https://www.usenix.org/conference/hotstorage13/workshop-program/presentation/zhang_wei;https://www.usenix.org/system/files/conference/hotstorage13/hotstorage13-zhang-deduplication.pdf","","","","","","","Seperation of duplicate detection and data backup.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2013 - Low-cost data deduplication for virtual machine backup in cloud storage.pdf","","","",""
"Conference paper","van Renen A,Vogel L,Leis V,Neumann T,Kemper A","","Persistent Memory I/O Primitives","","","","","Proceedings of the 15th International Workshop on Data Management on New Hardware","","","2019","","","1-7","All","","","Article 12","Association for Computing Machinery","New York, NY, USA","","Amsterdam, Netherlands","","2019-07-01","","2021-06-27","9781450368018","","","","https://doi.org/10.1145/3329785.3329930;http://dx.doi.org/10.1145/3329785.3329930;https://dl.acm.org/doi/abs/10.1145/3329785.3329930?casa_token=dM8jgy2BswMAAAAA:c4T1wpB9FVrV20NTMEWsAx0ClcZZFFy5LFmwMVaaA-cVBuK3YFHbxDpM0a2jdmnpVUsPT1gpfi2GTQ;https://dl.acm.org/doi/pdf/10.1145/3329785.3329930?casa_token=3KLptQOOIjUAAAAA:lWlIz_udps9d7O-OnsvBPo2ZWfUEmqhnrhz_mJundemdXGJxV8UWjlISRxTZXoIQ_RREtPygeVVQhA","10.1145/3329785.3329930","","","","I/O latency and throughput is one of the major performance bottlenecks for disk-based database systems. Upcoming persistent memory (PMem) technologies, like Intel's Optane DC Persistent Memory Modules, promise to bridge the gap between NAND-based flash (SSD) and DRAM, and thus eliminate the I/O bottleneck. In this paper, we provide one of the first performance evaluations of PMem in terms of bandwidth and latency. Based on the results, we develop guidelines for efficient PMem usage and two essential I/O primitives tuned for PMem: log writing and block flushing.","","","","","","","DaMoN'19","12","","","","","","","","","","","","","","","","","All Papers/V/van Renen et al. 2019 - Persistent Memory I - O Primitives.pdf","","","",""
"Journal article","Kim T,Kim J,Lee S","","FineDedup: A Fine-grained Deduplication Technique for Extending Lifetime of Flash-based SSDs","JOURNAL OF SEMICONDUCTOR TECHNOLOGY AND SCIENCE","JOURNAL OF SEMICONDUCTOR TECHNOLOGY AND SCIENCE","","","","","","2017","17","5","648-659","All","","","","","","","","","2017-10","","2021-06-25","","","1598-1657","","https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE07252826;http://dx.doi.org/10.5573/JSTS.2017.17.5.648","10.5573/JSTS.2017.17.5.648","","","","Data deduplication is an effective solution in improving the lifetime of flash-based solid-state drives (SSDs) by preventing redundant data from being written to flash memory. Existing deduplication techniques for SSDs, however, fail to fully eliminate potential redundant data because of their coarsegrained granularity. In this paper, a fine-grained deduplication technique for SSDs, called FineDedup, is proposed to improve the likelihood of eliminating redundant data. FineDedup also resolves technical difficulties caused by its finer granularity, i.e., increased memory requirement and read response time. The results show that FineDedup reduces the amount of written data by up to 24% over existing techniques with negligible.","NAND flash memory;solid state disks;data deduplication;lifespan;reliability","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Picoli IL,Bonnet P,Tözün P","","LSM Management on Computational Storage","","","","","Proceedings of the 15th International Workshop on Data Management on New Hardware","","","2019","","","1-3","All","","","Article 17","Association for Computing Machinery","New York, NY, USA","","Amsterdam, Netherlands","","2019-07-01","","2021-06-25","9781450368018","","","","https://doi.org/10.1145/3329785.3329927;http://dx.doi.org/10.1145/3329785.3329927;https://dl.acm.org/doi/abs/10.1145/3329785.3329927?casa_token=div4PcXIpJoAAAAA:j2clIoTaBpoDvxszrPspFKIRSlgOf95XN5hMKTirm_OT4GnQNNqkUT1pjW-85QBOLeP8NLhtZJtnAw;https://dl.acm.org/doi/pdf/10.1145/3329785.3329927?casa_token=0YqACR1cLdQAAAAA:4aG1WjFNSucA5zyHH1lKn4c1p9hNHK0xNt_WTOQtL22co3jc_srJEveSaAEDB5aIjd9fnVN1MjVnEQ","10.1145/3329785.3329927","","","","LSM-trees have emerged as the write-optimized index of choice for key-value stores and relational database systems. LSM-trees typically rely on a storage manager on top of a file system for storing data on Solid-State Drives (SSDs). The I/O path thus comprises four layers, each independently managing similar indirection, journaling, and garbage collection mechanisms. Such overhead is increasingly problematic. First, the advent of microsecond-scale SSDs makes it necessary to streamline the I/O software stack. Second, the increasing performance gap between storage and CPU makes it necessary to reduce CPU storage overhead. A solution is to collapse LSM, file system, and SSD management layers into a single software layer embedded on computational storage. Specific commercial solutions are already available. In this short paper, we describe the design space for LSM management on computational storage.","","","","","","","DaMoN'19","17","","","","","","","","","","","","","","","","","All Papers/P/Picoli et al. 2019 - LSM Management on Computational Storage.pdf","","","",""
"Website","Haas G,Haubenschild M,Leis V","","Exploiting directly-attached NVMe arrays in DBMS","","","","","","","","","","","","All","","","","","","","","","","","2021-06-25","","","","","http://cidrdb.org/cidr2020/papers/p16-haas-cidr20.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Haas et al. - Exploiting directly-attached NVMe arrays in DBMS.pdf","","","",""
"Conference paper","Lersch L,Oukid I,Schreter I,Lehner W","","Rethinking DRAM Caching for LSMs in an NVRAM Environment","","","","","Advances in Databases and Information Systems","","","2017","","","326-340","All","","","","Springer International Publishing","","","","","2017","","","","","","","http://dx.doi.org/10.1007/978-3-319-66917-5_22;https://link.springer.com/chapter/10.1007/978-3-319-66917-5_22","10.1007/978-3-319-66917-5_22","","","","The rise of NVRAM technologies promises to change the way we think about system architectures. In order to fully exploit its advantages, it is required to develop systems specially tailored for NVRAM devices. Not only this imposes great challenges, but also developing full system architectures from scratch is undesirable in many scenarios due to prohibitive development costs. Instead, we analyze in this paper the behavior of an existing log-structured persistent key-value store, namely LevelDB, when run on top of an emulated NVRAM device. We investigate initial opportunities for improvement when adapting a system tailored for HDD/SSDs to run on top of an NVRAM environment. Furthermore, we analyze the behavior of the DRAM caching components of LevelDB and whether more suitable caching policies are required.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lersch et al. 2017 - Rethinking DRAM Caching for LSMs in an NVRAM Environment.pdf","","","",""
"Conference paper","Kim H,Yeom HY,Son Y","","An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging","","","","","2020 IEEE 13th International Conference on Cloud Computing (CLOUD)","","","2020","","","405-413","All;Grouped by Publication/IEEE Trans Cloud Computing;Grouped by Publication/IEEE Cloud Conference","","","","","","","","","2020-10","","","","","2159-6190","","http://dx.doi.org/10.1109/CLOUD49709.2020.00062;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9284224","10.1109/CLOUD49709.2020.00062","","","","Many cloud services perform periodic database backup to keep the data safe from failures such as sudden system crashes. In the database system, two techniques are widely used for data backup and recovery: a physical backup and a logical backup. The physical backup uses raw data by copying the files in the database, whereas the logical backup extracts data from the database and dumps it into separated files as a sequence of query statements. Both techniques support a full backup strategy that contains data of the entire database and incremental backup strategy that contains changed data since a previous backup. However, both strategies require additional I/O operations to perform the backup and need a long time to restore a backup. In this paper, we propose an efficient backup and recovery scheme by exploiting write-ahead logging (WAL) in database systems. In the proposed scheme, for backup, we devise a backup system to use log data generated by the existing WAL to eliminate the additional I/O operations. To restore a backup, we utilize and optimize the existing crash recovery procedure of WAL to reduce recovery time. For example, we divide the recovery range and applying the backup data for each range independently via multiple threads. We implement our scheme in MySQL, a popular database management system. The experimental result demonstrates that the proposed scheme provides instant backup while reducing recovery time compared with the existing schemes.","Cloud computing;Instruction sets;Conferences;Database systems;Computer crashes;Data mining;Database Management System;Data Management;Database Backup and Recovery;Write-Ahead Logging","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2020 - An Efficient Database Backup and Recovery Scheme using Write-Ahead Logging.pdf","","","",""
"Journal article","Koo D,Hur J","","Privacy-preserving deduplication of encrypted data with dynamic ownership management in fog computing","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2018","78","","739-752","All","","","","","","","","","2018-01-01","","","","","0167-739X","","https://www.sciencedirect.com/science/article/pii/S0167739X17301309;http://dx.doi.org/10.1016/j.future.2017.01.024","10.1016/j.future.2017.01.024","","","","The explosion in the volume of data generated by end-point devices, arising from IoT proliferation, has lead to the adoption of data outsourcing to dedicated data centers. However, centralized data centers such as cloud storage cannot afford to manage large stores of data in a timely manner. To allow low latency access to large amounts of data, a new computing paradigm, called fog computing, has been introduced. In a fog computing environment, privacy issues surrounding outsourced data become more critical due to its complicated innards of the system. In addition, efficient resource management is another important criterion considering the application of pay-per-use in commercial fog storage. As an extension of cloud storage, most fog storage service providers will choose to adopt data deduplication techniques to minimize resource dissipation. At the same time, data owners may update or remove outsourced data stored in the remote storage to reduce expenses. In this paper, we propose the first privacy-preserving deduplication protocol capable of efficient ownership management in fog computing. It achieves fine-grained access control by introducing user-level key management and update mechanisms. Data-invariant user-level private keys enable data owners to maintain a constant number of keys regardless of the number of outsourced data files. The update of user-level public keys for valid data owners at the remote storage dramatically reduces communication overhead. Security and performance analyses demonstrate the efficiency of the proposed scheme in terms of communication and key management in fog storage.","Data deduplication; Fog computing; Data privacy; Data ownership management; Efficiency","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Koo and Hur 2018 - Privacy-preserving deduplication of encrypted data with dynamic ownership management in fog computing.pdf","","","",""
"Journal article","Lu Y,Qi Y,Qi S,Zhang F,Wei W,Yang X,Zhang J,Dong X","","Secure Deduplication-based Storage Systems with Resistance to Side-Channel Attacks via Fog Computing","IEEE Sens. J.","IEEE sensors journal","","","","","","2021","","","1-1","All;Grouped by Publication/IEEE Sensors Journal","Ownership Management","","","","","","","","2021","","","","","1530-437X","1558-1748","http://dx.doi.org/10.1109/JSEN.2021.3052782;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9328522","10.1109/JSEN.2021.3052782","","","","Data deduplication technique could greatly save the storage overhead of the cloud by eliminating duplicated data and retaining one copy. In order to ensure the privacy of the data against an untrusted cloud, many cryptographic schemes have been proposed to make deduplication feasible in ciphertext. A typical scheme is Message-Locked Encryption (MLE), which takes cryptographic hash values of messages as encryption keys. However, MLE is vulnerable to side-channel attacks. Current solutions trying to mitigate these attacks raise either expensive overhead or security drawbacks. In this paper, we propose a secure data deduplication system against an untrusted cloud with resistance to two typical side-channel attacks, namely probe attack and key-cache attack. Our system uses fog computing to devise two new techniques to solve the two side-channel attacks with new security and efficiency trade-offs. The analysis and evaluation show that our system achieves better trade-offs compared with previous works.","Maximum likelihood estimation;Cloud computing;Encryption;Cryptography;Side-channel attacks;Probes;Sensors;Message-Locked Encryption;Deduplication;Fog Computing;Side-Channel Attack","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lu et al. 2021 - Secure Deduplication-based Storage Systems with Resistance to Side-Channel Attacks via Fog Computing.pdf","","","",""
"Book chapter","Zhang Y,Xu C,Shen XS","Zhang Y,Xu C,Shen XS","Secure Deduplication","","","Data Security in Cloud Storage","","","","","2020","","","55-86","All","","","","Springer Singapore","Singapore","","","","2020","","","9789811543746","","","","https://doi.org/10.1007/978-981-15-4374-6_4;http://dx.doi.org/10.1007/978-981-15-4374-6_4;https://link.springer.com/chapter/10.1007/978-981-15-4374-6_4","10.1007/978-981-15-4374-6_4","","","","This chapter introduces the secure data deduplication technique for cloud storage systems. First, the basic paradigms, principles, and classification of the secure data deduplication technique are presented. Then, the secure deduplication over outsourced data (which are in the plaintext form) is reviewed, where the threats and countermeasures are introduced and analyzed. Next, the secure deduplication over outsourced encrypted data is introduced, and a comprehensive survey on encrypted deduplication systems is provided. Finally, the latest advances in the secure encrypted data deduplication technique are studied, which shows the potentials and benefits of applying the encrypted deduplication technique to eHealth systems.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore-ieee-org.ezproxy.lib.bbk.ac.uk/stamp/stamp.jsp?tp=&arnumber=7897080","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Meister D,Kaiser J,Brinkmann A,Cortes T,Kuhn M,Kunkel J","","A study on data deduplication in HPC storage systems","","","","","2012 International Conference for High Performance Computing, Networking, Storage and Analysis","","","2012","","","","All","Datasets","","","IEEE","","2012 SC - International Conference for High Performance Computing, Networking, Storage and Analysis","Salt Lake City, UT","2012/11/10-2012/11/16","2012-11","","","9781467308052","9781467308069","","","http://dx.doi.org/10.1109/sc.2012.14","10.1109/sc.2012.14","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Meister et al. 2012 - A study on data deduplication in HPC storage systems.pdf","","","",""
"Miscellaneous","","","Request Merging Based Cross-User Deduplication for Cloud Storage with Resistance Against Appending Chunks Attack.pdf","","","","","","","","","","","","All","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Request Merging Based Cross-User Dedu... - Request Merging Based Cross-U ... ion for Cloud Storage with Resistance Against Appending Chunks Attack.pdf","","","",""
"Journal article","Koushik CSN,Choubey SB,Choubey A,Sinha GR","","Study of data deduplication for file chunking approaches","Data Deduplication Approaches: Concepts, Strategies, and Challenges","","","","","","","2020","","","111","All","","","","Academic Press","","","","","2020","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Koushik et al. 2020 - Study of data deduplication for file chunking approaches.pdf","","","",""
"Journal article","You LL,Pollack KT,Long DD,Gopinath K","","PRESIDIO: A Framework for Efficient Archival Data Storage","ACM Trans. Storage","","","","","","","2011","7","2","1-60","All;Grouped by Publication/ACM TOS","Similarity/Resemblance","","","Association for Computing Machinery","New York, NY, USA","","","","2011-07-01","","","","","1553-3077","","https://doi.org/10.1145/1970348.1970351;http://dx.doi.org/10.1145/1970348.1970351;https://dl.acm.org/doi/abs/10.1145/1970348.1970351?casa_token=nqtzQmRNkW0AAAAA:KjEkQDSQM26_MA5F3HtzZTJi5hNn9qniLE8iCar3Lk4aH4aGRrM7amiH4-LntsTPRZRojfbrz58jxg;https://dl.acm.org/doi/pdf/10.1145/1970348.1970351?casa_token=mZRhTk4KmsUAAAAA:Y7d9Kk8hwZeNwuiTJW_BZ2him61l6ArBF5wXO4rhS50O2Jzl8elQ7LOg5gkYysta5nJCy_Yn95DQMw","10.1145/1970348.1970351","","","","The ever-increasing volume of archival data that needs to be reliably retained for long periods of time and the decreasing costs of disk storage, memory, and processing have motivated the design of low-cost, high-efficiency disk-based storage systems. However, managed disk storage is still expensive. To further lower the cost, redundancy can be eliminated with the use of interfile and intrafile data compression. However, it is not clear what the optimal strategy for compressing data is, given the diverse collections of data.To create a scalable archival storage system that efficiently stores diverse data, we present PRESIDIO, a framework that selects from different space-reduction efficent storage methods (ESMs) to detect similarity and reduce or eliminate redundancy when storing objects. In addition, the framework uses a virtualized content addressable store (VCAS) that hides from the user the complexity of knowing which space-efficient techniques are used, including chunk-based deduplication or delta compression. Storing and retrieving objects are polymorphic operations independent of their content-based address. A new technique, harmonic super-fingerprinting, is also used for obtaining successively more accurate (but also more costly) measures of similarity to identify the existing objects in a very large data set that are most similar to an incoming new object.The PRESIDIO design, when reported earlier, had comprehensively introduced for the first time the notion of deduplication, which is now being offered as a service in storage systems by major vendors. As an aid to the design of such systems, we evaluate and present various parameters that affect the efficiency of a storage system using empirical data.","CAS, Archival storage systems, progressive compression, data compression, content-addressable storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/You et al. 2011 - PRESIDIO - A Framework for Efficient Archival Data Storage.pdf","","","6","July 2011"
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore-ieee-org.ezproxy.lib.bbk.ac.uk/stamp/stamp.jsp?tp=&arnumber=8994084","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Lu M,Constantinescu C,Sarkar P","","Content Sharing Graphs for Deduplication-Enabled Storage Systems","Algorithms","Algorithms","","","","","","2012","5","2","236-260","All","","","","Molecular Diversity Preservation International","","","","","2012-04-10","","2021-02-24","","","","","https://www.mdpi.com/1999-4893/5/2/236;http://dx.doi.org/10.3390/a5020236","10.3390/a5020236","","","","Deduplication in storage systems has gained momentum recently for its capability in reducing data footprint. However, deduplication introduces challenges to storage management as storage objects (e.g., files) are no longer independent from each other due to content sharing between these storage objects. In this paper, we present a graph-based framework to address the challenges of storage management due to deduplication. Specifically, we model content sharing among storage objects by content sharing graphs (CSG), and apply graph-based algorithms to two real-world storage management use cases for deduplication-enabled storage systems. First, a quasi-linear algorithm was developed to partition deduplication domains with a minimal amount of deduplication loss (i.e., data replicated across partitioned domains) in commercial deduplication-enabled storage systems, whereas in general the partitioning problem is NP-complete. For a real-world trace of 3 TB data with 978 GB of removable duplicates, the proposed algorithm can partition the data into 15 balanced partitions with only 54 GB of deduplication loss, that is, a 5% deduplication loss. Second, a quick and accurate method to query the deduplicated size for a subset of objects in deduplicated storage systems was developed. For the same trace of 3 TB data, the optimized graph-based algorithm can complete the query in 2.6 s, which is less than 1% of that of the traditional algorithm based on the deduplication metadata.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/L/Lu et al. 2012 - Content Sharing Graphs for Deduplication-Enabled Storage Systems.pdf","","","",""
"Journal article","Yao WB,Ye PD,Li XY,Chang JK","","Deduplication algorithm based on condensed nearest neighbor rule for deduplication metadata","Journal on Communications","","","","","","","","36","8","1","All","","","","","","","","","","","","","","","","http://www.infocomm-journal.com/txxb/EN/10.11959/j.issn.1000-436x.2015226;http://www.infocomm-journal.com/txxb/EN/article/downloadArticleFile.do?attachType=PDF&id=153908","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yao et al. - Deduplication algorithm based on condensed nearest neighbor rule for deduplication metadata.pdf","","","",""
"Journal article","","","","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://ieeexplore-ieee-org.ezproxy.lib.bbk.ac.uk/stamp/stamp.jsp?tp=&arnumber=4483332","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Balachandran S,Constantinescu C","","Sequence of Hashes Compression in Data De-duplication","","","","","Data Compression Conference (dcc 2008)","","","2008","","","505-505","All","","","","","","","","","2008-03","","","","","2375-0359","","http://dx.doi.org/10.1109/DCC.2008.80;https://ieeexplore.ieee.org/abstract/document/4483332/","10.1109/DCC.2008.80","","","","Data de-duplication is a simple compression method, popular in storage archival and backup that consists in partitioning large data objects (files) into smaller parts (named chunks), and replacing the chunks for the purpose of communication or storage by their ID, generally a cryptographic hash like SHA-1 of the chunk data [A. Muthitacharoen et al., 2001], [D.R. Bobbarjung et al., 2006]. The compression ratio achieved by de-duplication can be improved by (1) increasing the likelihood of matching the new chunks against the dictionary (archived) chunks and/or (2) compressing the list of hashes (indexes, of 20 bytes each). Using smaller chunk sizes increases the chance of matching but many more hashes will be generated. The chunks repository is a hash table where each entry stores the SHA-1 value of the chunk and the chunk data. In addition, with each newly created entry we store a chronological pointer linking it with the next new entry. When the hashes produced by the chunker follow the chronological pointers we encode them as a sequence of hashes by specifying the first hash in the sequence and the length of the sequence or when the same hash is generated repeatedly we encode it as a run of hashes by specifying its value and the number of repeated occurrences. The usefulness of the chronological pointers is derived from the insight that when archiving successive versions of a file or set of files, large contiguous areas remain unchanged between these versions and the chronological pointers are predictors of this contiguity. If the contiguity is broken there is a small loss in the hash sequence compression.","cryptography;data compression;data de-duplication;storage archival;cryptographic hash;chunk repository;hash table;chunk data;chronological pointer linking;hash sequence compression ratio;Data compression;Cryptography;Dictionaries;Joining processes;Operating systems;Data De-duplication;cryptographic hashes compression","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Balachandran and Constantinescu 2008 - Sequence of Hashes Compression in Data De-duplication.pdf","","","",""
"Conference paper","Lillibridge M,Eshghi K,Bhagwat D","","Improving restore speed for backup systems that use inline chunk-based deduplication","","","","","11th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 13)","","","2013","","","183-197","All;Thesis","Restore optimization ","","","","","","","","2013","","","","","","","https://www.usenix.org/conference/fast13/technical-sessions/presentation/lillibridge;https://www.usenix.org/system/files/conference/fast13/fast13-final124.pdf","","","","","","","Speed Factor: 1/mean containers read per MB of data restored.","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lillibridge et al. 2013 - Improving restore speed for backup systems that use inline chunk-based deduplication.pdf","","","",""
"Conference paper","Mao B,Jiang H,Wu S,Tian L","","POD: Performance Oriented I/O Deduplication for Primary Storage Systems in the Cloud","","","","","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","","2014","","","767-776","All","","","","","","","","","2014-05","","","","","1530-2075","","http://dx.doi.org/10.1109/IPDPS.2014.84;https://ieeexplore.ieee.org/abstract/document/6877308/?casa_token=ktSCOs4qocUAAAAA:v4U3wdB_-9Ag0u8oyC3lzyrBaY_3u4KjLHB_TdoT2qnPka6JJRMn07Efxy0fJ_bz9Bpg6hjhtJXA;https://ieeexplore.ieee.org/iel7/6875427/6877223/06877308.pdf?casa_token=0gHksFHRs5UAAAAA:FwfKfL0RgU7_oP7XigSV_NYLMcIz6D1Aj7xkNYxJ-eDlwwgeS2WegvZrYynAYR_30VwO3dCuXHrW","10.1109/IPDPS.2014.84","","","","Recent studies have shown that moderate to high data redundancy clearly exists in primary storage systems in the Cloud. Our experimental studies reveal that data redundancy exhibits a much higher level of intensity on the I/O path than that on disks due to the relatively high temporal access locality associated with small I/O requests to redundant data. On the other hand, we also observe that directly applying data deduplication to primary storage systems in the Cloud will likely cause space contention in memory and data fragmentation on disks. Based on these observations, we propose a Performance-Oriented I/O Deduplication approach, called POD, rather than a capacity-oriented I/O deduplication approach, represented by iDedup, to improve the I/O performance of primary storage systems in the Cloud without sacrificing capacity savings of the latter. The salient feature of POD is its focus on not only the capacity-sensitive large writes and files, as in iDedup, but also the performance-sensitive while capacity-insensitive small writes and files. The experiments conducted on our lightweight prototype implementation of POD show that POD significantly outperforms iDedup in the I/O performance measure by up to 87.9% with an average of 58.8%. Moreover, our evaluation results also show that POD achieves comparable or better capacity savings than iDedup.","cloud computing;data handling;redundancy;storage management;POD;primary storage systems;cloud computing;high data redundancy;I/O path intensity;high temporal access locality;data deduplication;space contention;data fragmentation;performance-oriented I/O deduplication approach;capacity-oriented I/O deduplication approach;iDedup;Indexes;Redundancy;Educational institutions;Memory management;Prototypes;Cache storage;Monitoring;I/O Deduplication;Data Redundancy;Primary Storage;I/O Performance;Storage Capacity","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Lai R,Hua Y,Feng D,Xia W,Fu M,Yang Y","","A Near-Exact Defragmentation Scheme to Improve Restore Performance for Cloud Backup Systems","","","","","Algorithms and Architectures for Parallel Processing","","","2014","","","457-471","All","Restore optimization ","","","Springer International Publishing","","","","","2014","","","","","","","http://dx.doi.org/10.1007/978-3-319-11197-1_35;https://link.springer.com/chapter/10.1007/978-3-319-11197-1_35","10.1007/978-3-319-11197-1_35","","","","Cloud backup systems leverage data deduplication to remove duplicate chunks that are shared by many versions. The duplicate chunks are replaced with the references to old chunks via deduplication, instead of being uploaded to the cloud. The consecutive chunks in backup streams are actually stored dispersedly in several segments (the storage unit in the cloud), which results in fragmentation for restore. The segments that are referred will be downloaded from the cloud when the users want to restore the chunks of the latest version, and some chunks that are not referred will be downloaded together, thus jeopardizing the restore performance. In order to address this problem, we propose a near-exact defragmentation scheme, called NED, for deduplication based cloud backups. The idea behind NED is to compute the ratio of the length of chunks referred by current data stream in a segment to the segment length. If the ratio is smaller than a threshold, the chunks in the data stream that refer to the segment will be labeled as fragments and written to new segments. By efficiently identifying fragmented chunks, NED significantly reduces the number of segments for restore with slight decrease of deduplication ratio. Experiment results based on real-world datasets demonstrate that NED effectively improves the restore performance by 6%~105% at the cost of 0.1%~6.5% decrease in terms of deduplication ratio.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lai et al. 2014 - A Near-Exact Defragmentation Scheme to Improve Restore Performance for Cloud Backup Systems.pdf","","","",""
"Journal article","Khan A,Hamandawana P,Kim Y","","A Content Fingerprint-Based Cluster-Wide Inline Deduplication for Shared-Nothing Storage Systems","IEEE Access","","","","","","","2020","8","","209163-209180","All","Shared Nothing","","","","","","","","2020","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2020.3039056;https://ieeexplore.ieee.org/abstract/document/9262924/;https://ieeexplore.ieee.org/iel7/6287639/8948470/09262924.pdf","10.1109/ACCESS.2020.3039056","","","","Deduplication has been principally employed in distributed storage systems to improve storage space efficiency. Traditional deduplication research ignores the design specifications of shared-nothing distributed storage systems such as no central metadata bottleneck, scalability, and storage rebalancing. Likewise, inline deduplication integration poses serious threats to storage system read/write performance, consistency, and scalability. Mainly, this is due to ineffective and error-prone deduplication metadata, duplicate lookup I/O redirection, and placement of content fingerprints and data chunks. Further, transaction failures after deduplication integration often render inconsistencies in data chunks, deduplication metadata, and garbage data chunks. results in rendering inconsistencies in data chunks, deduplication metadata, and garbage data chunks. In this paper, we propose Grate, a high-performance inline cluster-wide data deduplication, complying with the design constraints of shared-nothing storage systems. In particular, Grate eliminates duplicate copies across the cluster for high storage space efficiency without jeopardizing performance. We employ a distributed deduplication metadata shard, which promises high-performance deduplication metadata and duplicate fingerprint lookup I/Os without introducing a single point of failure. The placement of data and deduplication metadata is made cluster-wide based on the content fingerprint of chunks. We decouple the deduplication metadata shard from read I/O path and replace it with a read manifestation object to further speedup read performance. To guarantee deduplication-enabled transaction consistency and efficient garbage identification, we design a flag-based asynchronous consistency scheme, capable of repairing the missing data chunks on duplicate arrival. We design and implement Grate in Ceph. The evaluation shows an average of 18% performance bandwidth improvement over the content addressable deduplication approach at smaller chunk sizes, i.e., less than 128KB while maintaining high storage space savings.","memory architecture;meta data;parallel architectures;storage management;transaction processing;content fingerprint-based cluster-wide inline deduplication;shared-nothing storage systems;distributed storage systems;central metadata bottleneck;storage rebalancing;inline deduplication integration;error-prone deduplication metadata;garbage data chunks;high-performance inline cluster-wide data deduplication;high storage space efficiency;distributed deduplication metadata;high-performance deduplication metadata;deduplication-enabled transaction consistency;missing data chunks;content addressable deduplication approach;storage space savings;Grate;duplicate fingerprint lookup I-O;Metadata;Servers;Scalability;Fingerprint recognition;Distributed databases;Indexes;Heuristic algorithms;Parallel and distributed storage systems;shared-nothing architecture;data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Khan et al. 2020 - A Content Fingerprint-Based Cluster-Wide Inline Deduplication for Shared-Nothing Storage Systems.pdf","","","",""
"Conference paper","Lee YS,Kim KM,Lee JH,Choi JH,Chung SW","","A High-Performance Processing-in-Memory Accelerator for Inline Data Deduplication","","","","","2019 IEEE 37th International Conference on Computer Design (ICCD)","","","2019","","","515-523","All","","","","","","","","","2019-11","","","","","2576-6996","","http://dx.doi.org/10.1109/ICCD46524.2019.00077;https://ieeexplore.ieee.org/abstract/document/8988619/?casa_token=R6yhMd-rKX8AAAAA:WVFB_R73jaKphwdvlWWt6B5_IdG0Q-582ME5DSzAbRBjCn87RcgIpq7Q69XOlr2vRA35LNXTtP6a;https://ieeexplore.ieee.org/iel7/8970097/8988587/08988619.pdf?casa_token=eeBTn3f0GMAAAAAA:M6ctY62cw4RAsLqj_uAcQ9Y40oCLieSc49iPopKlMuu9QnyhJy0Qc2lquC7AC5_WLzTwDjLOWilf","10.1109/ICCD46524.2019.00077","","","","In data centers, inline data deduplication which eliminates redundant data on the fly, is crucial to significantly reduce storage cost. However, it causes substantial performance and energy overhead due to a large number of memory accesses in the conventional GPU. In this paper, we propose a highperformance processing-in-memory accelerator for inline data deduplication, called Deduplication Unit (DU) to reduce the latency and power consumption. We place the DUs in a base die or core dies of a 3D stacked memory to improve performance. Our simulation results show that the DUs in the base die reduce the latency and processing unit power consumption by 17.3% and 45.5%, on average, respectively, compared to the conventional GPU. In addition, in our thermal simulation, peak temperature of the DU is still lower than the threshold temperature.","data handling;parallel processing;power aware computing;storage management;high-performance processing-in-memory accelerator;inline data deduplication;data centers;3D stacked memory;Deduplication Unit;Fingerprint recognition;Graphics processing units;Memory management;Indexing;Hardware;Data centers;Writing;Processing in memory;inline data deduplication;accelerator;3D stacked memory","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lee et al. 2019 - A High-Performance Processing-in-Memory Accelerator for Inline Data Deduplication.pdf","","","",""
"Conference paper","Du J,Deng J,Qi H,Di X,Jiang Z","","A Cross-Domain Secure Deduplication Scheme Based on Threshold Blind Signature","","","","","Mobile Wireless Middleware, Operating Systems and Applications","","","2020","","","78-91","All","Secure Dedup","","","Springer International Publishing","","","","","2020","","","","","","","http://dx.doi.org/10.1007/978-3-030-62205-3_7;https://link.springer.com/chapter/10.1007/978-3-030-62205-3_7","10.1007/978-3-030-62205-3_7","","","","In the cloud storage environment, client-side deduplication can perform file repetitive detection locally. However, client-side deduplication still faces many security challenges. First, if the file hash value is used as evidence for repetitive detection, the attacker is likely to obtain the entire file information through the hash value of the file. Secondly, in order to protect data privacy, convergence encryption is widely used in the data deduplication scheme. Since the data itself is predictable, convergence encryption is still vulnerable to brute force attacks. In order to solve the above problems, this paper proposes to construct a secure deduplication scheme by using the threshold blind signature method. The generation of the convergence key is coordinated by multiple key servers, ensuring the confidentiality of the convergence key and effectively solving the violent dictionary attack problem. At the same time, since the key center is introduced to centrally manage the keys, the interaction between the key servers is reduced, and the key generation efficiency is improved. In addition, since the key server in this paper can be distributed in multiple independent network domains and interact with the key center through the Internet, the problem of cross-domain deduplication is solved. The experimental results show that the performance of this scheme is greatly improved in terms of system initialization and key generation.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Li J,Lee PP,Tan C,Qin C,Zhang X","","Information Leakage in Encrypted Deduplication via Frequency Analysis: Attacks and Defenses","ACM Trans. Storage","","","","","","","2020","16","1","1-30","All;Grouped by Publication/ACM TOS;Thesis","Side Channels;Frequency","","","Association for Computing Machinery","New York, NY, USA","","","","2020-03-29","","","","","1553-3077","","https://doi.org/10.1145/3365840;http://dx.doi.org/10.1145/3365840;https://dl.acm.org/doi/abs/10.1145/3365840?casa_token=DbBn_KJ-vogAAAAA:ZjhRF67BL80TyXwgv1D3IGST4Kdv9ayDCnE8MaU8wt3FR97EQVgB_KElOoAWdslhdlBAVAlmASmI;https://dl.acm.org/doi/pdf/10.1145/3365840?casa_token=CJProeEbOv0AAAAA:SyCEChq-o2xuZKoIdcKFRRtvsKoqmiFcEv5Zu_HQsIZGIrUHsjmmKoLaVlHcpmupSXiE0BdJRlLa;https://dl.acm.org/doi/pdf/10.1145/3365840","10.1145/3365840","","","","Encrypted deduplication combines encryption and deduplication to simultaneously achieve both data security and storage efficiency. State-of-the-art encrypted deduplication systems mainly build on deterministic encryption to preserve deduplication effectiveness. However, such deterministic encryption reveals the underlying frequency distribution of the original plaintext chunks. This allows an adversary to launch frequency analysis against the ciphertext chunks and infer the content of the original plaintext chunks. In this article, we study how frequency analysis affects information leakage in encrypted deduplication, from both attack and defense perspectives. Specifically, we target backup workloads and propose a new inference attack that exploits chunk locality to increase the coverage of inferred chunks. We further combine the new inference attack with the knowledge of chunk sizes and show its attack effectiveness against variable-size chunks. We conduct trace-driven evaluation on both real-world and synthetic datasets and show that our proposed attacks infer a significant fraction of plaintext chunks under backup workloads. To defend against frequency analysis, we present two defense approaches, namely MinHash encryption and scrambling. Our trace-driven evaluation shows that our combined MinHash encryption and scrambling scheme effectively mitigates the severity of the inference attacks, while maintaining high storage efficiency and incurring limited metadata access overhead.","encrypted deduplication, Frequency analysis, cloud storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2020 - Information Leakage in Encrypted Deduplication via Frequency Analysis - Attacks and Defenses.pdf","","","4","February 2020"
"Conference paper","Xu G,Tang B,Lu H,Yu Q,Sung CW","","LIPA: A Learning-based Indexing and Prefetching Approach for Data Deduplication","","","","","2019 35th Symposium on Mass Storage Systems and Technologies (MSST)","","","2019","","","299-310","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;""Segment"" term sources;SCAIL Bibliography;Grouped by Publication/MSST;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","ML","","","","","","","","2019-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2019.00010;https://ieeexplore.ieee.org/abstract/document/8890070/?casa_token=A1TPruKT2pAAAAAA:ugpjmnHQycRQLOseCXpPEhEE3FvInV0CK1dUAOR9fsavzHaH80PvZGWwelRfqJOHVVx-UAxx7pjh;https://ieeexplore.ieee.org/iel7/8882740/8890059/08890070.pdf?casa_token=qfakcIZfsUUAAAAA:tYb341UfYhXNL0yEeLrZx6KD79NTz4hrjN5VnJ6BQV21_8yqssIAndjtYvZ1RwoYKqoXEMx2CczZ","10.1109/MSST.2019.00010","","","","In this paper, we present a learning based data deduplication algorithm, called LIPA, which uses the reinforcement learning framework to build an adaptive indexing structure. It is rather different from previous inline chunk-based deduplication methods to solve the chunk-lookup disk bottleneck problem for large-scale backup. In previous methods, a full chunk index or a sampled chunk index often is often required to identify duplicate chunks, which is a critical stage for data deduplication. The full chunk index is hard to fit in RAM and the sampled chunk index directly affects the deduplication ratio dependent on the sampling ratio. Our learning based method only requires little memory overheads to store the index but achieves the same or even better deduplication ratio than previous methods. In our method, after the data stream is broken into relatively large segments, one or more representative chunk fingerprints are chosen as the feature of a segment. An incoming segment may share the same feature with previous segments. Thus we use a key-value structure to record the relationship between features and segments: a feature maps to a fixed number of segments. We train the similarities of these segments to a feature represented as scores by the reinforcement learning method. For an incoming segment, our method adaptively prefetches a segment and the successive ones into cache by using multi-armed bandits model. Our experimental results show that our method significantly reduces memory overheads and achieves effective deduplication.","data compression;data structures;disc storage;learning (artificial intelligence);duplicate chunks;sampled chunk index;deduplication ratio;sampling ratio;data stream;representative chunk fingerprints;incoming segment;reinforcement learning method;learning-based indexing;prefetching approach;learning based data deduplication algorithm;LIPA;adaptive indexing structure;chunk-lookup disk bottleneck problem;inline chunk-based deduplication methods;Prefetching;Reinforcement learning;Indexing;Memory management;Random access memory;Distributed databases;Deduplication;Reinforcement learning;Data prefetching;Chunk index","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xu et al. 2019 - LIPA - A Learning-based Indexing and Prefetching Approach for Data Deduplication.pdf","","","",""
"Conference paper","Chernov I,Ivashko E,Rumiantsev A,Ponomarev V,Shabaev A","","Survey on Deduplication Techniques in Flash-Based Storage","","","","","2018 22nd Conference of Open Innovations Association (FRUCT)","","","2018","","","25-33","All","SSD","","","","","","","","2018-05","","","","","2305-7254","","http://dx.doi.org/10.23919/FRUCT.2018.8468295;https://ieeexplore.ieee.org/abstract/document/8468295/?casa_token=Ca0ojuUsVJ4AAAAA:WDTY2KpHIk_b0aR9zoQYYGOdvrfWfSc-EzwP47_M06L_XRWw4kkX_85o6_esyV_0eB6cQV1-TRvU;https://ieeexplore.ieee.org/iel7/8457083/8468260/08468295.pdf?casa_token=7cjygrxBa2IAAAAA:XLfjbvk8kPWDv4Gu8WnIvgn6OQTfDIkREtUkyboNgCwXtf7hwbpWHv_gYLb1Bq2GvBIHpC6Ojm_7","10.23919/FRUCT.2018.8468295","","","","Data deduplication importance is growing with the growth of data volumes. The domain of data deduplication is in active development. Recently it was influenced by appearance of Solid State Drive. This new type of disk has significant differences from random access memory and hard disk drives and is widely used now. In this paper we propose a novel taxonomy which reflects the main issues related to deduplication in Solid State Drive. We present a survey on deduplication techniques focusing on flash-based storage. We also describe several Open Source tools implementing data deduplication and briefly describe open research problems related to data deduplication in flash-based storage systems.","disc drives;flash memories;hard discs;public domain software;random-access storage;data deduplication technique;random access memory;Open Source tools;flash-based storage systems;hard disk drives;Solid State Drive;data volumes;Taxonomy;Redundancy;Performance evaluation;Containers;Tools;Distributed databases","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Chernov IA,Ivashko E,Kositsyn D,Ponomarev V,Rumyantsev A,Shabaev A","","Flash-Based Storage Deduplication Techniques: A Survey","IJERTCS","International Journal of Embedded and Real-Time Communication Systems (IJERTCS)","","","","","","2019","10","3","32-48","All","SSD","","","IGI Global","","","","","2019-07-01","","2021-02-09","","","","","https://www.igi-global.com/article/flash-based-storage-deduplication-techniques/231459;http://dx.doi.org/10.4018/IJERTCS.2019070103","10.4018/IJERTCS.2019070103","","","","Exponential growth of the amount of data stored worldwide together with high level of data redundancy motivates the active development of data deduplication techniques. The overall increasing popularity of solid-state drives (SSDs) as primary storage devices forces the adaptation of deduplication te...","","","","","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Ajdari M,Park P,Kim J,Kwon D,Kim J","","CIDR: A Cost-Effective In-Line Data Reduction System for Terabit-Per-Second Scale SSD Arrays","","","","","2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)","","","2019","","","28-41","All","SSD","","","","","","","","2019-02","","","","","2378-203X","","http://dx.doi.org/10.1109/HPCA.2019.00025;https://ieeexplore.ieee.org/abstract/document/8675249/?casa_token=JYAzXZhnjREAAAAA:QJcaqQquAmbTJXSY2uF-rfS4mTLTFZKDGyLGUuxX8SJ1Oaij7eVWDW9B9VXtMDHpoJA36bIZF_Uc;https://ieeexplore.ieee.org/iel7/8666628/8675182/08675249.pdf?casa_token=-Z8OV0M6FVgAAAAA:VfOdQFZWn-vFs-fN4aSdQM_zlHrwC0nfV_QnDhI7IHQAOE7uEYaYE52pZ3CJEcBpYSt5XYWB_vFs","10.1109/HPCA.2019.00025","","","","An SSD array, a storage system consisting of multiple SSDs per node, has become a design choice to implement a fast primary storage system, and modern storage architects now aim to achieve terabit-per-second scale performance with the next-generation SSD array. To reduce the storage cost and improve the device endurability, such SSD array must employ data reduction schemes (i.e., deduplication, compression), which provide high data reduction capability at minimum costs. However, existing data reduction schemes do not scale with the fast increasing performance of an SSD array, due to inhibitive amount of CPU resources (e.g., in software-based schemes) or low data reduction ratio (e.g., in SSD device wide deduplication) or being cost ineffective to address workload changes in datacenters (e.g., in ASIC-based acceleration). In this paper, we propose CIDR, a novel FPGA-based, cost-effective data reduction system for an SSD array to achieve the terabit-per-second scale storage performance. Our key ideas are as follows. First, we decouple data reduction-related computing tasks from the unscalable host CPUs by offloading them to a scalable array of FPGA boards. Second, we employ a centralized, node-wide metadata management scheme to achieve an SSD array-wide, high data reduction. Third, our FPGA-based reconfiguration adapts to different workload patterns by dynamically balancing the amount of software and hardware tasks running on CPUs and FPGAs, respectively. For evaluation, we built our example CIDR prototype achieving up to 12.8 GB/s (0.1 Tbps) on one FPGA. CIDR outperforms the baseline for a write-only workload by up to 2.47x and a mixed read-write workload by an expected 3.2x, respectively. We showed CIDR's scalability to achieve Tbps-scale performance by measuring a two-FPGA CIDR and projecting the performance impacts for more FPGAs.","data reduction;field programmable gate arrays;integrated circuit design;meta data;solid state drives;storage management chips;data reduction schemes;terabit-per-second scale storage performance;node-wide metadata management scheme;mixed read-write workload;FPGA-based reconfiguration;FPGA-based cost-effective data reduction system;next generation SSD array;primary storage system;terabit-per-second scale SSD arrays;cost-effective inline data reduction system;data reduction-related computing tasks;Arrays;Servers;Throughput;Field programmable gate arrays;Nonvolatile memory;Hardware;Acceleration;deduplication;compression;FPGA;SSD array","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Oh M,Park S,Yoon J,Kim S,Lee K,Weil S,Yeom HY,Jung M","","Design of Global Data Deduplication for a Scale-Out Distributed Storage System","","","","","2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)","","","2018","","","1063-1073","All","","","","","","","","","2018-07","","","","","2575-8411","","http://dx.doi.org/10.1109/ICDCS.2018.00106;https://ieeexplore.ieee.org/abstract/document/8416369/?casa_token=eRuwKuTIsBcAAAAA:BEr8EgLB1LxF8uWmQDXRP-ljOFakiTyqdab3uE7GFQIpUYe3LKwJILJhHsZr1HW3t3a04Rg1_cA5;https://ieeexplore.ieee.org/iel7/8415923/8416263/08416369.pdf?casa_token=kIx3Jwee0kkAAAAA:jOKICv7LvE7O6wK80GYGMkoWcQ-Dt6B15xOgBIFJKyHwnqyOAzQg5jxNikJZUGp9yPifxDWvw-o7","10.1109/ICDCS.2018.00106","","","","Scale-out distributed storage systems can uphold balanced data growth in terms of capacity and performance on an on-demand basis. However, it is a challenge to store and manage large sets of contents being generated by the explosion of data. One of the promising solutions to mitigate big data issues is data deduplication, which removes redundant data across many nodes of the storage system. Nevertheless, it is non-trivial to apply a conventional deduplication design to the scale-out storage due to the following root causes. First, chunk-lookup for deduplication is not as scalable and extendable as the underlying storage system supports. Second, managing the metadata associated to deduplication requires a huge amount of design and implementation modifications of the existing distributed storage system. Lastly, the data processing and additional I/O traffic imposed by deduplication can significantly degrade performance of the scale-out storage. To address these challenges, we propose a new deduplication method, which is highly scalable and compatible with the existing scale-out storage. Specifically, our deduplication method employs a double hashing algorithm that leverages hashes used by the underlying scale-out storage, which addresses the limits of current fingerprint hashing. In addition, our design integrates the meta-information of file system and deduplication into a single object, and it controls the deduplication ratio at online by being aware of system demands based on post-processing. We implemented the proposed deduplication method on an open source scale-out storage. The experimental results show that our design can save more than 90% of the total amount of storage space, under the execution of diverse standard storage workloads, while offering the same or similar performance, compared to the conventional scale-out storage.","Big Data;meta data;storage management;conventional deduplication design;data processing;deduplication method;file system;deduplication ratio;open source scale-out storage;storage space;diverse standard storage workloads;conventional scale-out storage;global data deduplication;scale-out distributed storage system;data growth;big data issues;redundant data;distributed storage system;metadata management;file system meta-information;Indexes;Distributed databases;Metadata;Scalability;Servers;Degradation;Data structures;Scale out Distributed Storage System;Deduplication;decentralized system","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Ma J,Wang G,Liu X","","DedupeSwift: Object-Oriented Storage System Based on Data Deduplication","","","","","2016 IEEE Trustcom/BigDataSE/ISPA","","","2016","","","1069-1076","All","","","","","","","","","2016-08","","","","","2324-9013","","http://dx.doi.org/10.1109/TrustCom.2016.0177;https://ieeexplore.ieee.org/abstract/document/7847060/?casa_token=IWq9MmjEGU4AAAAA:qv0vPbvdM6_8pbKsiN2q5wVXh3-C4QWPp681NCQGemgfLjf6yRMoIaz3WbDKtcEQ-2w4iLfefVwD;https://ieeexplore.ieee.org/iel7/7845250/7846883/07847060.pdf?casa_token=8aDuzlOipFMAAAAA:IJNfK0O80blDS4rAALdT8ny58w98Rno4MlDS_tggzcnt_j9bNhesvMwRJI12iCtpo-3z_8AdM6HM","10.1109/TrustCom.2016.0177","","","","Recent years have witnessed the explosion of the data universe. Facing the rapid growth of the data size, cloud storage is proposed as an approach to provide cost-efficient and reliable data storage service. As data size grows, data centers providing cloud storage service need more storage resources to meet the ever-increasing requirements. Data deduplication is a technology aiming to remove redundant data blocks. It has been used to reduce the storage footprint of backup and archival systems. In this paper, we propose DedupeSwift, which is based on OpenStack Swift, an open-source object-oriented storage software widely used in public and private clouds. Data deduplication is introduced to reduce the storage overhead. To deal with the performance overhead brought by deduplication, a lazy method is introduced to reduce the disk I/O bottleneck. Compression and caching are also used in the system to improve the read performance. Experimental results show that our proposed DedupeSwift can reduce the storage overhead by 65.24% and 89.84% on the two data sets with favorable upload and download throughput.","cache storage;cloud computing;computer centres;object-oriented methods;public domain software;data size;data storage service;data centers;cloud storage service;data storage resources;data deduplication;redundant data block removal;DedupeSwift;OpenStack Swift;open-source object-oriented storage software;public clouds;private clouds;disk I/O bottleneck;data compression;caching;Servers;Containers;Cloud computing;Reliability;Fingerprint recognition;Authorization;Indexes","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Wu J,Hua Y,Zuo P,Sun Y","","A cost-efficient rewriting scheme to improve restore performance in deduplication systems","","","","","Proc. MSST","","","2017","","","","All;Grouped by Publication/MSST","Restore optimization ","","","","","","","","2017","","","","","","","https://storageconference.us/2017/Papers/RestorePerformanceInDeduplicationSystems.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2017 - A cost-efficient rewriting scheme to improve restore performance in deduplication systems.pdf","","","",""
"Conference paper","Won Y,Ban J,Min J,Hur J,Oh S,Lee J","","Efficient index lookup for De-duplication backup system","","","","","2008 IEEE International Symposium on Modeling, Analysis and Simulation of Computers and Telecommunication Systems","","","2008","","","","All","Fingerprint-Indexing","","","IEEE","","Telecommunication Systems (MASCOTS)","Baltimore, MD, USA","2008/9/8-2008/9/10","2008-09","","","9781424428175","","","","http://dx.doi.org/10.1109/mascot.2008.4770594","10.1109/mascot.2008.4770594","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Won et al. 2008 - Efficient index lookup for De-duplication backup system.pdf","","","",""
"Conference paper","Song L,Deng Y,Xie J","","Exploiting fingerprint prefetching to improve the performance of data deduplication","","","","","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","","","2013","","","","All","Fingerprint-Indexing","","","IEEE","","2013 IEEE International Conference on High Performance Computing and Communications (HPCC) & 2013 IEEE International Conference on Embedded and Ubiquitous Computing (EUC)","Zhangjiajie, China","2013/11/13-2013/11/15","2013-11","","","9780769550886","","","","http://dx.doi.org/10.1109/hpcc.and.euc.2013.122","10.1109/hpcc.and.euc.2013.122","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Song et al. 2013 - Exploiting fingerprint prefetching to improve the performance of data deduplication.pdf","","","",""
"Conference paper","Kaczmarczyk M,Dubnicki C","","Reducing fragmentation impact with forward knowledge in backup systems with deduplication","","","","","Proceedings of the 8th ACM International Systems and Storage Conference","","","2015","","","","All","Restore optimization ","","","ACM","New York, NY, USA","SYSTOR 2015: International Conference on Systems and Storage","Haifa Israel","2015/5/26-2015/5/28","2015-05-26","","","9781450336079","","","","http://dx.doi.org/10.1145/2757667.2757678","10.1145/2757667.2757678","","","","","","","http://www.acm.org/publications/policies/copyright_policy#Background","9LivesData, LLC","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaczmarczyk and Dubnicki 2015 - Reducing fragmentation impact with forward knowledge in backup systems with deduplication.pdf","","","",""
"Conference paper","Kaczmarczyk M,Barczynski M,Kilian W,Dubnicki C","","Reducing impact of data fragmentation caused by in-line deduplication","","","","","Proceedings of the 5th Annual International Systems and Storage Conference on - SYSTOR '12","","","2012","","","","All","Restore optimization ","","","ACM Press","New York, New York, USA","the 5th Annual International Systems and Storage Conference","Haifa, Israel","2012/6/4-2012/6/6","2012","","","9781450314480","","","","http://dx.doi.org/10.1145/2367589.2367600","10.1145/2367589.2367600","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaczmarczyk et al. 2012 - Reducing impact of data fragmentation caused by in-line deduplication.pdf","","","",""
"Book","Rosenblum M","","The Design and Implementation of a Log-structured file system","","","","","","","","2012","","","","All","","","","Springer US","","","","","2012-10-04","","","9781461359333","","","","https://play.google.com/store/books/details?id=vdNSngEACAAJ;https://books.google.com/books/about/The_Design_and_Implementation_of_a_Log_s.html?hl=&id=vdNSngEACAAJ","","","","","Computersystemsresearch is heavilyinfluencedby changesincomputertechnol­ ogy. As technology changes alterthe characteristics ofthe underlying hardware com­ ponents of the system, the algorithms used to manage the system need to be re­ examinedand newtechniques need to bedeveloped. Technological influencesare par­ ticularly evident in the design of storage management systems such as disk storage managers and file systems. The influences have been so pronounced that techniques developed as recently as ten years ago are being made obsolete. The basic problem for disk storage managers is the unbalanced scaling of hard­ warecomponenttechnologies. Disk storage managerdesign depends on the technolo­ gy for processors, main memory, and magnetic disks. During the 1980s, processors and main memories benefited from the rapid improvements in semiconductortechnol­ ogy and improved by several orders ofmagnitude in performance and capacity. This improvement has not been matched by disk technology, which is bounded by the me­ chanics ofrotating magnetic media. Magnetic disks ofthe 1980s have improved by a factor of 10in capacity butonly a factor of2 in performance. This unbalanced scaling ofthe hardware components challenges the disk storage manager to compensate for the slower disks and allow performance to scale with the processor and main memory technology. Unless the performance of file systems can be improved over that of the disks, I/O-bound applications will be unable to use the rapid improvements in processor speeds to improve performance for computer users. Disk storage managers must break this bottleneck and decouple application perfor­ mance from the disk.","","","","","en","","","","","","","132","","","","","","","","","","","","","All Papers/R/Rosenblum 2012 - The Design and Implementation of a Log-structured file system.pdf","","","",""
"Journal article","Wu X,Gao J,Ji G,Wu T,Tian Y,Al-Nabhan N","","A feature-based intelligent deduplication compression system with extreme resemblance detection","Conn. Sci.","Connection science","","","","","","2020","","","1-29","All;Thesis;p-scailbib","Similarity/Resemblance","","","Taylor & Francis","","","","","2020-12-21","","","","","0954-0091","","https://doi.org/10.1080/09540091.2020.1862058;http://dx.doi.org/10.1080/09540091.2020.1862058;https://www.tandfonline.com/doi/abs/10.1080/09540091.2020.1862058","10.1080/09540091.2020.1862058","","","","ABSTRACT With the fast development of various computing paradigms, the amount of data is rapidly increasing that brings the huge storage overhead. However, the existing data deduplication techniques do not make full use of similarity detection to improve the storage efficiency and data transmission rate. In this paper, we study the problem of utilising the duplicate and resemblance detection techniques to further compress data. We first present a framework of FIDCS-ERD, a feature-based intelligent deduplication compression system with extreme resemblance detection. We also introduce the main components and the detailed workflow of our compression system. We propose a content-defined chunking algorithm for duplicate detection and a Bloom filter-based resemblance detection algorithm. FIDCS-ERD implements the intelligent file chunking and the fast duplicate and resemblance detection. By extensive experiments over the real datasets, we demonstrate that FIDCS-ERD has better compression effect and more accurate resemblance detection compared to the existing approaches.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2020 - A feature-based intelligent deduplication compression system with extreme resemblance detection.pdf","","","",""
"Conference paper","Fan H,Xu G,Zhang Y,Yuan L,Xue Y","","CSF: An Efficient Parallel Deduplication Algorithm by Clustering Scattered Fingerprints","","","","","2019 IEEE Intl Conf on Parallel Distributed Processing with Applications, Big Data Cloud Computing, Sustainable Computing Communications, Social Computing Networking (ISPA/BDCloud/SocialCom/SustainCom)","","","2019","","","602-607","All;Thesis","Fingerprint-Indexing;sorted deduplication;FSLHomes","","","","","","","","2019-12","","","","","","","http://dx.doi.org/10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00091;https://ieeexplore.ieee.org/abstract/document/9047466/","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00091","","","","Deduplication is one of the most effective and efficient techniques to save memory space. It is widely used in data centers and cloud storage systems. Multi-stream concurrency is expected to increase the throughput of deduplication. However, multiple data streams hurt the locality of accessed data and weaken the benefit of data concurrency, which forms a challenge for data deduplication. Usually, the ordered index can reshape the locality of data streams, which can improve the cache hit rate during deduplication. In this paper, we first propose an efficient parallel deduplication algorithm by clustering scattered fingerprints, called CSF, to exploit the data locality as much as possible. It tries to improve the utilization rate of the fingerprint page by the clustered fingerprints. Moreover, it retains the scattered fingerprint to next round fingerprint comparison by re-using the fingerprints on the same page. Thus the number of the fingerprint pages to read is reduced. We further optimize the proposed algorithm by a scheduling strategy, which effectively schedules the task of part streams ahead while ensuring the overall performance. Finally, we evaluated the performance of our algorithm with various data sets in experiments. The experimental results show that our proposed algorithm achieves better performance than the state-of-the-art method.","cache storage;cloud computing;parallel algorithms;pattern clustering;scheduling;storage management;data concurrency;data deduplication;cache hit rate;scattered fingerprint;data locality;fingerprint page;clustered fingerprints;fingerprint comparison;data sets;data centers;cloud storage systems;multistream concurrency;multiple data streams;accessed data;parallel deduplication algorithm;CSF;Indexes;Clustering algorithms;Task analysis;Servers;Fingerprint recognition;Throughput;Distributed databases;Deduplication;Storage Systems;Parallel Algorithms;Data Streams","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fan et al. 2019 - CSF - An Efficient Parallel Deduplication Algorithm by Clustering Scattered Fingerprints.pdf","","","",""
"Preprint","Wu H,Wang C,Fu Y,Sakr S,Zhu L,Lu K","","HPDedup: A Hybrid Prioritized Data Deduplication Mechanism for Primary Storage in the Cloud","","","","","","arXiv [cs.DC]","","2017","","","","All","","","","","","","","","2017-02-27","","","","","","","http://arxiv.org/abs/1702.08153","","","1702.08153","","Eliminating duplicate data in primary storage of clouds increases the cost-efficiency of cloud service providers as well as reduces the cost of users for using cloud services. Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post-processing deduplication running in system idle time to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services or applications for the following two reasons: Firstly, the temporal locality of duplicate data writes may not exist in some primary storage workloads thus inline caching often fails to achieve good deduplication ratio. Secondly, the post-processing deduplication allows duplicate data to be written into disks, therefore does not provide the benefit of I/O deduplication and requires high peak storage capacity. This paper presents HPDedup, a Hybrid Prioritized data Deduplication mechanism to deal with the storage system shared by applications running in co-located virtual machines or containers by fusing an inline and a post-processing process for exact deduplication. In the inline deduplication phase, HPDedup gives a fingerprint caching mechanism that estimates the temporal locality of duplicates in data streams from different VMs or applications and prioritizes the cache allocation for these streams based on the estimation. HPDedup also allows different deduplication threshold for streams based on their spatial locality to reduce the disk fragmentation. The post-processing phase removes duplicates whose fingerprints are not able to be cached due to the weak temporal locality from disks. Our experimental results show that HPDedup clearly outperforms the state-of-the-art primary storage deduplication techniques in terms of inline cache efficiency and primary deduplication efficiency.","","","","","","","","","arXiv","1702.08153","cs.DC","","","","","","","","","","","","","","All Papers/W/Wu et al. 2017 - HPDedup - A Hybrid Prioritized Data Deduplication Mechanism for Primary Storage in the Cloud.pdf","","","",""
"Journal article","O’Neil P,Cheng E,Gawlick D,O’Neil E","","The log-structured merge-tree (LSM-tree)","Acta Inform.","Acta Informatica","","","","","","1996","33","4","351-385","All;Thesis;p-scailbib","","","","","","","","","1996-06-01","","","","","0001-5903","1432-0525","https://doi.org/10.1007/s002360050048;http://dx.doi.org/10.1007/s002360050048;https://link.springer.com/article/10.1007/s002360050048;http://www.inf.ufpr.br/eduardo/ensino/ci809/papers/lsmtree.pdf","10.1007/s002360050048","","","","High-performance transaction system applications typically insert rows in a History table to provide an activity trace; at the same time the transaction system generates log records for purposes of system recovery. Both types of generated information can benefit from efficient indexing. An example in a well-known setting is the TPC-A benchmark application, modified to support efficient queries on the history for account activity for specific accounts. This requires an index by account-id on the fast-growing History table. Unfortunately, standard disk-based index structures such as the B-tree will effectively double the I/O cost of the transaction to maintain an index such as this in real time, increasing the total system cost up to fifty percent. Clearly a method for maintaining a real-time index at low cost is desirable. The log-structured mergetree (LSM-tree) is a disk-based data structure designed to provide low-cost indexing for a file experiencing a high rate of record inserts (and deletes) over an extended period. The LSM-tree uses an algorithm that defers and batches index changes, cascading the changes from a memory-based component through one or more disk components in an efficient manner reminiscent of merge sort. During this process all index values are continuously accessible to retrievals (aside from very short locking periods), either through the memory component or one of the disk components. The algorithm has greatly reduced disk arm movements compared to a traditional access methods such as B-trees, and will improve cost-performance in domains where disk arm costs for inserts with traditional access methods overwhelm storage media costs. The LSM-tree approach also generalizes to operations other than insert and delete. However, indexed finds requiring immediate response will lose I/O efficiency in some cases, so the LSM-tree is most useful in applications where index inserts are more common than finds that retrieve the entries. This seems to be a common property for history tables and log files, for example. The conclusions of Sect. 6 compare the hybrid use of memory and disk components in the LSM-tree access method with the commonly understood advantage of the hybrid method to buffer disk pages in memory.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/O/O’Neil et al. 1996 - The log-structured merge-tree (LSM-tree).pdf","","","",""
"Conference paper","Kaiser J,Süß T,Nagel L,Brinkmann A","","Sorted deduplication: How to process thousands of backup streams","","","","","2016 32nd Symposium on Mass Storage Systems and Technologies (MSST)","","","2016","","","1-14","All;SCAIL Bibliography;Grouped by Publication/MSST;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","sorted deduplication","","","","","","","","2016-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2016.7897082;https://ieeexplore.ieee.org/abstract/document/7897082/;https://storageconference.us/2016/Papers/SortedDeduplication.pdf","10.1109/MSST.2016.7897082","","","","The requirements of deduplication systems have changed in the last years. Early deduplication systems had to process dozens to hundreds of backup streams at the same time while today they are able to process hundreds to thousands of them. Traditional approaches rely on stream-locality, which supports parallelism, but which easily leads to many non-contiguous disk accesses, as each stream competes with all other streams for the available resources. This paper presents a new exact deduplication approach designed for processing thousands of backup streams at the same time on the same fingerprint index. The underlying approach destroys the traditionally exploited temporal chunk locality and creates a new one by sorting fingerprints. The sorting leads to perfectly sequential disk access patterns on the backup servers, while only slightly increasing the load on the clients. In our experiments, the new approach generates up to 113 times less I/Os than the exact Data Domain deduplication file system and up to 12 times less I/Os than the approximate Sparse Indexing, while consuming less memory at the same time.","back-up procedures;disc storage;sorting;sorted deduplication;backup streams;deduplication systems;noncontiguous disk accesses;temporal chunk locality;sorting fingerprints;backup servers;data domain deduplication file system;sparse indexing approximation;Servers;Fingerprint recognition","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaiser et al. 2016 - Sorted deduplication - How to process thousands of backup streams.pdf","","","",""
"Conference paper","Cao Z,Liu S,Wu F,Wang G,Li B,Du DH","","Sliding look-back window assisted data chunk rewriting for improving deduplication restore performance","","","","","17th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 19)","","","2019","","","129-142","All;Grouped by Publication/FAST Papers","Restore optimization ","","","usenix.org","","","","","2019","","","","","","","https://www.usenix.org/conference/fast19/presentation/cao;https://www.usenix.org/system/files/fast19-cao.pdf","","","","","Data deduplication is an effective way of improving storage space utilization. The data generated by deduplication is persistently stored in data chunks or data containers (a container consisting of a few hundreds or thousands of data chunks). The data restore …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cao et al. 2019 - Sliding look-back window assisted data chunk rewriting for improving deduplication restore performance.pdf","","","",""
"Journal article","Zhou Y,Deng Y,Yang LT,Yang R,Si L","","LDFS: A Low Latency In-Line Data Deduplication File System","IEEE Access","","","","","","","2018","6","","15743-15753","All","","","","ieeexplore.ieee.org","","","","","2018","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2018.2800763;https://ieeexplore.ieee.org/abstract/document/8278179/;https://ieeexplore.ieee.org/iel7/6287639/6514899/08278179.pdf","10.1109/ACCESS.2018.2800763","","","","Due to the rapid proliferation of sensors and intelligent devices, the cyber-physical-social computing and networking (CPSCN) is emerging as a new computing paradigm. Massive data have been generated in the CPSCN environment. The traditional data deduplication is not able to handle the CPSCN environment due to the involved long latency. This paper presents a low latency in-line data deduplication file system (LDFS). The LDFS decouples the unique data block and fingerprint index by writing the address of data blocks to the corresponding file recipe and fingerprint index, thus avoiding accessing fingerprint index on the path of the read operation. For every unique data block, the LDFS assigns a globally unique ID, and thus, the LDFS only requires one disk access to obtain the corresponding data block reference count using the global ID. In order to guarantee the write performance, the LDFS employs finer granularity lock to optimize the block flushing strategy of write buffer. Experimental results demonstrate that the LDFS significantly enhances the read and write performance on the critical path in contrast to the traditional deduplication file system LessFS. Meanwhile, the LDFS achieves almost the same deduplication ratio (40.8) as that of LessFS.","Indexes;Fingerprint recognition;Metadata;Data mining;Containers;Systems architecture;File systems;Massive data;inline data deduplication;file system;low latency;disk bottleneck","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2018 - LDFS - A Low Latency In-Line Data Deduplication File System.pdf","","","",""
"Conference paper","Fu M,Feng D,Hua Y,He X,Chen Z,Xia W,Huang F,Liu Q","","Accelerating restore and garbage collection in deduplication-based backup systems via exploiting historical information","","","","","2014 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 14)","","","2014","","","181-192","All","Restore optimization ","","","","","","","","2014","","","","","","","https://www.usenix.org/conference/atc14/technical-sessions/presentation/fu_min;https://www.usenix.org/system/files/conference/atc14/atc14-paper-fu_min.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2014 - Accelerating restore and garbage collection in deduplication-based backup systems via exploiting historical information.pdf","","","",""
"Conference paper","Chen H,Liao L,Jin H,Wu J","","The dynamic cuckoo filter","","","","","2017 IEEE 25th International Conference on Network Protocols (ICNP)","","","2017","","","1-10","All","","","","","","","","","2017-10","","","","","","","http://dx.doi.org/10.1109/ICNP.2017.8117563;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8117563","10.1109/ICNP.2017.8117563","","","","The emergence of large-scale dynamic sets in real applications creates stringent requirements for approximate set representation structures: 1) the capacity of the set representation structures should support flexibly extending or reducing to cope with dynamically changing of set size; 2) the set representation structures should support reliable delete operation. Existing techniques for approximate set representation, e.g., the cuckoo filter, the Bloom filter and its variants cannot meet both the requirements of a dynamic set. To solve the problem, in this paper we propose the dynamic cuckoo filter (DCF) to support reliable delete operation and elastic capacity for dynamic set representation and membership testing. Two factors contribute to the efficiency of the DCF design. First, the data structure of a DCF is extendable, making the representation of a dynamic set space efficient. Second, a DCF utilizes a monopolistic fingerprint for representing an item and guarantees reliable delete operation. Experiment results show that compared to the existing state-of-the-art designs, DCF achieves 75% reduction in memory cost, 50% improvement in construction speed, and 80% improvement in speed of membership query. We implement a prototype file backup system and use DCF for data deduplication. Comprehensive experiment results demonstrate the efficiency of our DCF design compared to existing schemes.","data structures;dynamic cuckoo filter;large-scale dynamic sets;approximate set representation structures;Bloom filter;dynamic set representation;DCF design;data structure;reliable delete operation;membership testing;monopolistic fingerprint;prototype file backup system;data deduplication;Testing;Reliability;Prototypes;Encoding;Arrays;Upper bound;Dynamic set representation;set membership testing;cuckoo filter","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chen et al. 2017 - The dynamic cuckoo filter.pdf","","","",""
"Journal article","Tütüncü RH,Koenig M","","Robust Asset Allocation","Ann. Oper. Res.","Annals of Operations Research","","","","","","2004","132","1","157-187","All","","","","Springer","","","","","2004-11-01","","","","","0254-5330","1572-9338","https://doi.org/10.1023/B:ANOR.0000045281.41041.ed;http://dx.doi.org/10.1023/B:ANOR.0000045281.41041.ed;https://link.springer.com/article/10.1023/B:ANOR.0000045281.41041.ed;http://www.math.cmu.edu/~reha/Pss/raarev.pdf","10.1023/B:ANOR.0000045281.41041.ed","","","","This article addresses the problem of finding an optimal allocation of funds among different asset classes in a robust manner when the estimates of the structure of returns are unreliable. Instead of point estimates used in classical mean-variance optimization, moments of returns are described using uncertainty sets that contain all, or most, of their possible realizations. The approach presented here takes a conservative viewpoint and identifies asset mixes that have the best worst-case behavior. Techniques for generating uncertainty sets from historical data are discussed and numerical results that illustrate the stability of robust optimal asset mixes are reported.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tütüncü and Koenig 2004 - Robust Asset Allocation.pdf","","","",""
"Journal article","Shapiro LD","","Join processing in database systems with large main memories","ACM Trans. Database Syst.","","","","","","","1986","11","3","239-264","All","","","","Association for Computing Machinery","New York, NY, USA","","","","1986-08-01","","","","","0362-5915","","https://doi.org/10.1145/6314.6315;http://dx.doi.org/10.1145/6314.6315;https://dl.acm.org/doi/abs/10.1145/6314.6315?casa_token=HJN1ig__KlcAAAAA:U8WfUjb42U1OzQIBhvaxE-7xdh9lv3t6OhTGL7KO72ghrhPDhn8hiToTHYWmNUKOr6QSBEWhaEmI;https://dl.acm.org/doi/pdf/10.1145/6314.6315?casa_token=fcQgGsUGUJQAAAAA:DdeYNQSvw5eQQRvW3t-10k4wcki6ZcDeci8PB7FMKKX6XvtIR3d0j4s9UnFpN3sEvqkACIQ40ZJn","10.1145/6314.6315","","","","We study algorithms for computing the equijoin of two relations in a system with a standard architecture hut with large amounts of main memory. Our algorithms are especially efficient when the main memory available is a significant fraction of the size of one of the relations to he joined; but they can be applied whenever there is memory equal to approximately the square root of the size of one relation. We present a new algorithm which is a hybrid of two hash-based algorithms and which dominates the other algorithms we present, including sort-merge. Even in a virtual memory environment, the hybrid algorithm dominates all the others we study.Finally, we describe how three popular tools to increase the efficiency of joins, namely filters, Babb arrays, and semijoins, can he grafted onto any of our algorithms.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shapiro 1986 - Join processing in database systems with large main memories.pdf","","","","Sept. 1986"
"Miscellaneous","Zhang Y,Fu M,Wu X,Wang F,Wang Q,Wang C,Dong X,Han H","","Improving Restore Performance of Packed Datasets in Deduplication Systems via Reducing Persistent Fragmented Chunks","IEEE Transactions on Parallel and Distributed Systems","","","","","","","2020","31","7","1651-1664","All","Restore optimization ","","","","","","","","2020","","","","","","","http://dx.doi.org/10.1109/tpds.2020.2972898","10.1109/tpds.2020.2972898","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2020 - Improving Restore Performance of Packed Datasets in Deduplication Systems via Reducing Persistent Fragmented Chunks.pdf","","","",""
"Journal article","Wu H,Wang C,Fu Y,Sakr S,Lu K,Zhu L","","A Differentiated Caching Mechanism to Enable Primary Storage Deduplication in Clouds","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2018","29","6","1202-1216","All;Grouped by Publication/IEEE Trans Parallel Distrib Syst","","","","","","","","","2018-06","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2018.2790946;https://ieeexplore.ieee.org/abstract/document/8249847/?casa_token=3xlpPSmS-LEAAAAA:30r0s7vYiayQE9JaWgODWkOC6CY3qJadQkns84_XXHQ3Fwc7Emgq15sVcCQts3aSjYjnaq7R1keOHg;https://ieeexplore.ieee.org/iel7/71/4359390/08249847.pdf?casa_token=6Ztz0kg6_TAAAAAA:lPSG7f1DoEkOi9dcACu6P__jPIw_xyDJE6xW1BUQwTRhWIpD44c_deTV46iUDizC3bqGqZ4vzhjfmQ","10.1109/TPDS.2018.2790946","","","","Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post-processing deduplication to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services for the following two reasons: First, the temporal locality of duplicate data writes varies among primary storage workloads, which makes it challenging to efficiently allocate the inline cache space and achieve a good deduplication ratio. Second, the post-processing deduplication does not eliminate duplicate I/O operations that write to the same logical block address as it is performed after duplicate blocks have been written. A hybrid deduplication mechanism is promising to deal with these problems. Inline fingerprint caching is essential to achieving efficient hybrid deduplication. In this paper, we present a detailed analysis of the limitations of using existing caching algorithms in primary deduplication in the cloud. We reveal that existing caching algorithms either perform poorly or incur significant memory overhead in fingerprint cache management. To address this, we propose a novel fingerprint caching mechanism that estimates the temporal locality of duplicates in different data streams and prioritizes the cache allocation based on the estimation. We integrate the caching mechanism and build a hybrid deduplication system. Our experimental results show that the proposed mechanism provides significant improvement for both deduplication ratio and overhead reduction.","cache storage;cloud computing;input-output programs;storage management;differentiated caching mechanism;post-processing deduplication;cloud servers;temporal locality;duplicate I/O operations;logical block address;inline fingerprint caching;fingerprint cache management;hybrid deduplication system;primary storage deduplication;cache allocation;Cloud computing;Estimation;Servers;Algorithm design and analysis;Electronic mail;Resource management;Containers;Data deduplication;cache management;ghost cache;primary storage;cloud services","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2018 - A Differentiated Caching Mechanism to Enable Primary Storage Deduplication in Clouds.pdf","","","",""
"Conference paper","Douglis F,Duggal A,Shilane P,Wong T,Yan S,Botelho F","","The logic of physical garbage collection in deduplicating storage","","","","","15th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 17)","","","2017","","","29-44","All","","","","","","","","","2017","","","","","","","https://www.usenix.org/conference/fast17/technical-sessions/presentation/douglis;https://www.usenix.org/system/files/conference/fast17/fast17-douglis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Douglis et al. 2017 - The logic of physical garbage collection in deduplicating storage.pdf","","","",""
"Conference paper","Tan Y,Jiang H,Feng D,Tian L,Yan Z,Zhou G","","SAM: A Semantic-Aware Multi-tiered Source De-duplication Framework for Cloud Backup","","","","","2010 39th International Conference on Parallel Processing","","","2010","","","614-623","All","","","","","","","","","2010-09","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPP.2010.69;https://ieeexplore.ieee.org/abstract/document/5599246/?casa_token=7W7PjYEMdjwAAAAA:8QgKzo4hlWvWFVEA9WiOwOZKvcPn9nzrUQjjHzCc6WBtU_47JTXWUo4nxDWS_SyE4rgWPyz_B_LA2w;https://ieeexplore.ieee.org/iel5/5598250/5599149/05599246.pdf?casa_token=BxWZLb-fOosAAAAA:EwAg2-jTJDau7v6ew_3jgZo51GfdP9RIRgE8xY3h2w8svMGzXoO_nFP5mY3VNOJwLYOF0jPeFkdQMQ","10.1109/ICPP.2010.69","","","","Existing de-duplication solutions in cloud backup environment either obtain high compression ratios at the cost of heavy de-duplication overheads in terms of increased latency and reduced throughput, or maintain small de-duplication overheads at the cost of low compression ratios causing high data transmission costs, which results in a large backup window. In this paper, we present SAM, a Semantic-Aware Multitiered source de-duplication framework that first combines the global file-level de-duplication and local chunk-level deduplication, and further exploits file semantics in each stage in the framework, to obtain an optimal tradeoff between the deduplication efficiency and de-duplication overhead and finally achieve a shorter backup window than existing approaches. Our experimental results with real world datasets show that SAM not only has a higher de-duplication efficiency/overhead ratio than existing solutions, but also shortens the backup window by an average of 38.7%.","client-server systems;data compression;Internet;semantic aware multitiered source deduplication framework;cloud backup environment;compression ratio;high data transmission cost;global file level deduplication;local chunk level deduplication;Servers;Semantics;Redundancy;Clouds;Data communication;Image coding;Indexes;Cloud Backup;Backup Window;Data Deduplication;File Semantics","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan et al. 2010 - SAM - A Semantic-Aware Multi-tiered Source De-duplication Framework for Cloud Backup.pdf","","","",""
"Conference paper","Sun W,Zhang N,Lou W,Hou YT","","Tapping the Potential: Secure Chunk-based Deduplication of Encrypted Data for Cloud Backup","","","","","2018 IEEE Conference on Communications and Network Security (CNS)","","","2018","","","1-9","All","","","","","","","","","2018-05","","","","","","","http://dx.doi.org/10.1109/CNS.2018.8433173;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8433173","10.1109/CNS.2018.8433173","","","","We, in this work, investigate the problem of designing a secure chunk-based deduplication scheme in the enterprise backup storage setting. Most of the existing works focus on realizing file-level encrypted data deduplication or key/metadata management. Little attention is drawn to the practical chunk-level deduplication system. In particular, we identify that the information contained in a small-sized chunk is more susceptible to the brute-force attack compared with file-based deduplication. We propose a randomized oblivious key generation mechanism based on the inner workings of the backup service. In contrast with the current work that compromising one client will eventually expose all the clients' storage, our scheme offers a counter-intuitive property of achieving security against multiclient compromise with minimal deduplication performance loss. In addition, we enforce a per-backup rate-limiting policy to slow down the online brute-force attack. We show that the proposed scheme is provably secure in the malicious model. We also calibrate the system design by taking into account the practical deduplication requirements to accomplish a comparable plaintext deduplication performance. Our experiment on the real-world dataset shows its efficiency, effectiveness, and practicality.","back-up procedures;cloud computing;cryptography;meta data;random functions;storage management;cloud backup;secure chunk-based deduplication scheme;enterprise backup storage setting;file-level encrypted data deduplication;file-based deduplication;randomized oblivious key generation mechanism;backup service;per-backup rate-limiting policy;online brute-force attack;data encryption;chunk-level deduplication system;plaintext deduplication performance;metadata management;Encryption;Servers;Containers;Cloud computing;Protocols","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Xing YX,Xiao N,Liu F,Sun Z,He WH","","AR-dedupe: An efficient deduplication approach for cluster deduplication system","J. Shanghai Jiatong Univ.","Journal of Shanghai Jiaotong University","","","","","","2015","20","1","76-81","All","","","","","","","","","2015-02-01","","","","","1007-1172","1995-8188","https://doi.org/10.1007/s12204-015-1591-1;http://dx.doi.org/10.1007/s12204-015-1591-1;https://link.springer.com/content/pdf/10.1007/s12204-015-1591-1.pdf","10.1007/s12204-015-1591-1","","","","As data are growing rapidly in data centers, inline cluster deduplication technique has been widely used to improve storage efficiency and data reliability. However, there are some challenges faced by the cluster deduplication system: the decreasing data deduplication rate with the increasing deduplication server nodes, high communication overhead for data routing, and load balance to improve the throughput of the system. In this paper, we propose a well-performed cluster deduplication system called AR-Dedupe. The experimental results of two real datasets demonstrate that AR-Dedupe can achieve a high data deduplication rate with a low communication overhead and keep the system load balancing well at the same time through a new data routing algorithm. In addition, we utilize application-aware mechanism to speed up the index of handprints in the routing server which has a 30% performance improvement.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Sun Z,Kuenning G,Mandal S,Shilane P,Tarasov V,Xiao N,Zadok KE","","A long-term user-centric analysis of deduplication patterns","2016 32nd Symposium on Mass Storage Systems and Technologies (MSST)","","","","","","","2016","","","","All;SCAIL Bibliography;Grouped by Publication/MSST;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","","","","","","","","","2016","","","","","","","http://dx.doi.org/10.1109/msst.2016.7897080","10.1109/msst.2016.7897080","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sun et al. 2016 - A long-term user-centric analysis of deduplication patterns.pdf","","","",""
"Conference paper","Mulazzani M,Schrittwieser S,Leithner M,Huber M,Weippl ER","","Dark Clouds on the Horizon: Using Cloud Storage as Attack Vector and Online Slack Space","","","","","USENIX security symposium","","","2011","","","65-76","All;Grouped by Publication/Usenix Security;Thesis","Side Channels;PoW;Sec Techniques/Protocols","San Francisco, CA, USA","","","","","","","2011","","","","","","","https://static.usenix.org/events/sec11/tech/full_papers/Mulazzani6-24-11.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Mulazzani et al. 2011 - Dark Clouds on the Horizon - Using Cloud Storage as Attack Vector and Online Slack Space.pdf","","","",""
"Journal article","Yang T,Feng D,Chou W,Liu J","","A study on disk index design for large scale de–duplication storage systems","Int. J. Comput. Sci. Eng.","International Journal of Computational Science and Engineering","","","","","","2015","10","1-2","171-180","ILL Request;All","","","","Inderscience Publishers","","","","","2015-01-01","","","","","1742-7185","","https://www.inderscienceonline.com/doi/abs/10.1504/IJCSE.2015.067074;http://dx.doi.org/10.1504/IJCSE.2015.067074","10.1504/IJCSE.2015.067074","","","","Chunk?based de?duplication storage, which aims to optimise the storage or bandwidth usage by eliminating the duplicate chunks in the inter?file level, has been attended broadly both in academia and industry recently. For a petabyte?scale de?duplication storage system, the metadata storage especially the disk index, which establishes a mapping between the fingerprints and corresponding chunks in the system, can reach terabyte?scale size. In this paper, we propose a disk?resident hash table to implement the disk index, and theoretically study yet extensively experiment the probability of hash table overflow. These studies help us design a space?efficient disk index which not only reduces metadata storage but also improves access performance.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Yang Q,Jin R,Zhao M","","Smartdedup: optimizing deduplication for resource-constrained devices","","","","","2019 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 19)","","","2019","","","633-646","Out-of-line deduplication;All","","","","","","","","","2019","","","","","","","https://www.usenix.org/conference/atc19/presentation/yang-qirui;https://www.usenix.org/system/files/atc19-yang-qirui.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2019 - Smartdedup - optimizing deduplication for resource-constrained devices.pdf","","","",""
"Conference paper","Broder AZ","","On the resemblance and containment of documents","","","","","Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171)","","","1997","","","21-29","All;Thesis","","IEEE","","","","","","","1997","","","","","","","https://ieeexplore.ieee.org/abstract/document/666900/;https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.779&rep=rep1&type=pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Broder 1997 - On the resemblance and containment of documents.pdf","","","",""
"Conference paper","Kiss SZ,Hosszu É,Tapolcai J,Rónyai L,Rottenstreich O","","Bloom Filter with a False Positive Free Zone","","","","","IEEE INFOCOM 2018 - IEEE Conference on Computer Communications","","","2018","","","1412-1420","All","","","","","","","","","2018-04","","","","","","","http://dx.doi.org/10.1109/INFOCOM.2018.8486415;https://ieeexplore.ieee.org/abstract/document/8486415/?casa_token=uhhLKC94v5gAAAAA:cs-NDX5fMvA_x4suzokNt-tZ_wKpq1I69qCqZlaQIxbW7ldVjlEm9l5b_d9GHQCZci_VnSmg6XSTGg;https://ieeexplore.ieee.org/iel7/8464035/8485803/08486415.pdf?casa_token=F5VxFn2zsGsAAAAA:iBl375M0tAV7azKfRJk3l6OiWAs5p45gtKulJ4wwLgxp557XyGExJHDOvfu23IaNNmKb5rkC_Hburw;http://lendulet.tmit.bme.hu/lendulet_website/wp-content/papercite-data/pdf/kiss2018bloom.pdf","10.1109/INFOCOM.2018.8486415","","","","Bloom filters and their variants are widely used as space efficient probabilistic data structures for representing set systems and are very popular in networking applications. They support fast element insertion and deletion, along with membership queries with the drawback of false positives. Bloom filters can be designed to match the false positive rates that are acceptable for the application domain. However, in many applications a common engineering solution is to set the false positive rate very small, and ignore the existence of the very unlikely false positive answers. This paper is devoted to close the gap between the two design concepts of unlikely and not having false positives. We propose a data structure, called EGH filter, that supports the Bloom filter operations and besides it can guarantee false positive free operations for a finite universe and a restricted number of elements stored in the filter. We refer to the limited universe and filter size as the false positive free zone of the filter. We describe necessary conditions for the false positive free zone of a filter and generalize the filter to support listing of the elements. We evaluate the performance of the filter in comparison with the traditional Bloom filters. Our data structure is based on recently developed combinatorial group testing techniques.","data structures;Bloom filter operations;false positive free operations;filter size;false positive free zone;false positive rate;probabilistic data structures;EGH filter;combinatorial group testing techniques;Testing;Probabilistic logic;Probes;Conferences;Arrays;Focusing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kiss et al. 2018 - Bloom Filter with a False Positive Free Zone.pdf","","","",""
"Conference paper","Won Y,Kim R,Ban J,Hur J,Oh S,Lee J","","PRUN : Eliminating Information Redundancy for Large Scale Data Backup System","","","","","2008 International Conference on Computational Sciences and Its Applications","","","2008","","","139-144","All","Chunking","","","","","","","","2008-06","","","","","","","http://dx.doi.org/10.1109/ICCSA.2008.46;https://ieeexplore.ieee.org/abstract/document/4561214/;http://www.esos.hanyang.ac.kr/files/publication/conferences/international/PRUN%20-%20Eliminating%20Information%20Redundancy%20for%20Large%20Scale%20Data%20Backup%20System.pdf","10.1109/ICCSA.2008.46","","","","In this work, we develop novel backup system, PRUN, for massive scale data storage. PRUN aims at improving the backup latency and storage overhead of backup via effectively eliminating information redundancy in the files. PRUN eliminates intra-file and inter-file information redundancy. PRUN consists of client module and server module. PRUN consists of three key technical ingredients: redundancy detection, fingerprint manager, and chunk manager. File chunking for redundancy detection is the most time consuming task in backup. For efficient file chunking, we develop incremental modulo-K algorithm which enables us to improve the file chunking time significantly. We perform various experiment to measure the overhead of each tasks in backup operation and to examine the efficiency of redundancy elimination. Incremental modulo-K reduces the file chunking latency by approximately 60%. Redundancy elimination scheme can reduce the storage requirement of backup by 80% when we backup different minor versions of Linux 2.6 kernel source.","data handling;information redundancy;large scale data backup system;massive scale data storage;backup latency;interfile information redundancy;redundancy detection;fingerprint manager;chunk manager;incremental modulo-K algorithm;file chunking latency;Linux 2.6 kernel source;Large-scale systems;File systems;Fingerprint recognition;Video compression;Delay;Law;Legal factors;Bandwidth;Partitioning algorithms;Application software;backup;de-duplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Won et al. 2008 - PRUN - Eliminating Information Redundancy for Large Scale Data Backup System.pdf","","","",""
"Miscellaneous","Thwel TT,Thein NL","","An Efficient Indexing Mechanism for Data Deduplication","2009 International Conference on the Current Trends in Information Technology (CTIT)","","","","","","","2009","","","","All","","","","","","","","","2009","","","","","","","http://dx.doi.org/10.1109/ctit.2009.5423123","10.1109/ctit.2009.5423123","","","","","","No B+Tree algorithm in the paper","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Thwel and Thein 2009 - An Efficient Indexing Mechanism for Data Deduplication.pdf","","","",""
"Journal article","Bobbarjung DR,Jagannathan S,Dubnicki C","","Improving duplicate elimination in storage systems","ACM Trans. Storage","","","","","","","2006","2","4","424-448","""Super-chunk"" term sources;All;SCAIL Bibliography;Grouped by Publication/ACM TOS;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Chunking;Super-chunking","","","Association for Computing Machinery","New York, NY, USA","","","","2006-11-01","","","","","1553-3077","","https://doi.org/10.1145/1210596.1210599;http://dx.doi.org/10.1145/1210596.1210599","10.1145/1210596.1210599","","","","Minimizing the amount of data that must be stored and managed is a key goal for any storage architecture that purports to be scalable. One way to achieve this goal is to avoid maintaining duplicate copies of the same data. Eliminating redundant data at the source by not writing data which has already been stored not only reduces storage overheads, but can also improve bandwidth utilization. For these reasons, in the face of today's exponentially growing data volumes, redundant data elimination techniques have assumed critical significance in the design of modern storage systems.Intelligent object partitioning techniques identify data that is new when objects are updated, and transfer only these chunks to a storage server. In this article, we propose a new object partitioning technique, called fingerdiff, that improves upon existing schemes in several important respects. Most notably, fingerdiff dynamically chooses a partitioning strategy for a data object based on its similarities with previously stored objects in order to improve storage and bandwidth utilization. We present a detailed evaluation of fingerdiff, and other existing object partitioning schemes, using a set of real-world workloads. We show that for these workloads, the duplicate elimination strategies employed by fingerdiff improve storage utilization on average by 25%, and bandwidth utilization on average by 40% over comparable techniques.","Storage management, Rabin's fingerprints, content-based addressing, duplicate elimination","Fingerdiff, and defines superchunks","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bobbarjung et al. 2006 - Improving duplicate elimination in storage systems.pdf;All Papers/B/Bobbarjung et al. 2006 - Improving duplicate elimination in storage systems.pdf;All Papers/B/Bobbarjung et al. 2006 - Improving duplicate elimination in storage systems.pdf","","","","November 2006"
"Conference paper","Wang G,Zhao Y,Xie X,Liu L","","Research on a Clustering Data De-Duplication Mechanism Based on Bloom Filter","","","","","2010 International Conference on Multimedia Technology","","","2010","","","1-5","All","","","","","","","","","2010-10","","","","","","","http://dx.doi.org/10.1109/ICMULT.2010.5630395;https://ieeexplore.ieee.org/abstract/document/5630395/?casa_token=Xot0vm4c4DYAAAAA:hkbviZOWevtUe9BEl9cdFmLOM4in7v-lU33ReZuumBas7rZCcXIxIlVNVrw_AZcVGLkqRpGYhO-mJg;https://ieeexplore.ieee.org/iel5/5628463/5629505/05630395.pdf?casa_token=QSgBGLCzYaUAAAAA:Ryf-lBA0vQm4vSUaJG999hPdGhS1PodL3jWtj8Bu96mUCpvXzzAzahjdjRWutQh-qTkRWntr3xVwng","10.1109/ICMULT.2010.5630395","","","","Recently, data de-duplication, the hot emerging technology, has received a broad attention from both academia and industry. Some researches focus on the approach by which to reduce more redundant data. And the others investigate how to do de-duplication at high speed. In this paper, we aim at reducing the time and space requirement for data de-duplication. We describe a clustering architecture with multiple nodes and all nodes can do the chunk-level data de-duplication in parallel. Thus the performance will be improved noticeably. At the same time, this paper proposes a new technique called ""Fingerprint Summary"". Each node keeps a compact summary of the chunks' fingerprints of every other node in its memory. When checking for duplicate chunks, each node queries its local chunk hash database and then the Fingerprint Summary if necessary to eliminate inter-node redundant chunks. So we can reduce the storage capacity requirement largely.","data compression;data handling;filtering theory;pattern clustering;storage management;Bloom filter;clustering data deduplication mechanism;time space requirement reduction;clustering architecture;chunk level data deduplication;fingerprint summary;chunk hash database;internode redundant chunk;Fingerprint recognition;Databases;Arrays;Redundancy;Space technology;Protocols","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2010 - Research on a Clustering Data De-Duplication Mechanism Based on Bloom Filter.pdf","","","",""
"Journal article","Almeida PS,Baquero C,Preguiça N,Hutchison D","","Scalable Bloom Filters","Inf. Process. Lett.","Information processing letters","","","","","","2007","101","6","255-261","All","","","","","","","","","2007-03-31","","","","","0020-0190","","http://www.sciencedirect.com/science/article/pii/S0020019006003127;http://dx.doi.org/10.1016/j.ipl.2006.10.007;https://www.sciencedirect.com/science/article/pii/S0020019006003127;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.725.390&rep=rep1&type=pdf;https://gsd.di.uminho.pt/members/cbm/ps/dbloom.pdf","10.1016/j.ipl.2006.10.007","","","","Bloom filters provide space-efficient storage of sets at the cost of a probability of false positives on membership queries. The size of the filter must be defined a priori based on the number of elements to store and the desired false positive probability, being impossible to store extra elements without increasing the false positive probability. This leads typically to a conservative assumption regarding maximum set size, possibly by orders of magnitude, and a consequent space waste. This paper proposes Scalable Bloom Filters, a variant of Bloom filters that can adapt dynamically to the number of elements stored, while assuring a maximum false positive probability.","Data structures; Bloom filters; Distributed systems; Randomized algorithms","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Almeida et al. 2007 - Scalable Bloom Filters.pdf","","","",""
"Conference paper","Botelho FC,Shilane P,Garg N,Hsu W","","Memory efficient sanitization of a deduplicated storage system","","","","","Presented as part of the 11th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 13)","","","2013","","","81-94","Locality Based (per Ma 2017);All","","","","usenix.org","","","","","2013","","","","","","","https://www.usenix.org/conference/fast13/technical-sessions/presentation/botelho;https://www.usenix.org/system/files/conference/fast13/fast13-final100_0.pdf","","","","","Sanitization is the process of securely erasing sensitive data from a storage system, effectively restoring the system to a state as if the sensitive data had never been stored. Depending on the threat model, sanitization could require erasing all unreferenced blocks …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Botelho et al. 2013 - Memory efficient sanitization of a deduplicated storage system.pdf","","","",""
"Conference paper","Srinivasan K,Bisson T,Goodson GR,Voruganti K","","iDedup: latency-aware, inline data deduplication for primary storage","","","","","Fast","","","2012","12","","1-14","Locality Based (per Ma 2017);All","","","","","","","","","2012","","","","","","","https://static.usenix.org/events/fast/tech/full_papers/Srinivasan2-10-12.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Srinivasan et al. 2012 - iDedup - latency-aware, inline data deduplication for primary storage.pdf","","","",""
"Journal article","Li J,Li YK,Chen X,Lee PP,Lou W","","A Hybrid Cloud Approach for Secure Authorized Deduplication","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2015","26","5","1206-1216","All;Grouped by Publication/IEEE Trans Parallel Distrib Syst","","","","","","","","","2015-05","","","","","1045-9219","1558-2183","http://dx.doi.org/10.1109/TPDS.2014.2318320;https://ieeexplore.ieee.org/abstract/document/6802424/?casa_token=TgyHD5e-JKoAAAAA:sJy-iwS-UG-oyzDctewcA5nXBtaiXqihvhxk2-l0WalFQWqtxRUB5pGYnXXxXs49-mUj_jUJIpyuRg;https://ieeexplore.ieee.org/iel7/71/4359390/06802424.pdf?casa_token=viHSIHAGmMgAAAAA:dXObBdhJJhsao5_qanYG9Jkv67BE3uw30qPXO3C_V1aTID4Qbx3W7ezsxqoebmo2IxJe731pDZgWgQ","10.1109/TPDS.2014.2318320","","","","Data deduplication is one of important data compression techniques for eliminating duplicate copies of repeating data, and has been widely used in cloud storage to reduce the amount of storage space and save bandwidth. To protect the confidentiality of sensitive data while supporting deduplication, the convergent encryption technique has been proposed to encrypt the data before outsourcing. To better protect data security, this paper makes the first attempt to formally address the problem of authorized data deduplication. Different from traditional deduplication systems, the differential privileges of users are further considered in duplicate check besides the data itself. We also present several new deduplication constructions supporting authorized duplicate check in a hybrid cloud architecture. Security analysis demonstrates that our scheme is secure in terms of the definitions specified in the proposed security model. As a proof of concept, we implement a prototype of our proposed authorized duplicate check scheme and conduct testbed experiments using our prototype. We show that our proposed authorized duplicate check scheme incurs minimal overhead compared to normal operations.","authorisation;cloud computing;cryptography;data compression;hybrid cloud approach;secure authorized data deduplication;data compression techniques;cloud storage;sensitive data confidentiality;convergent encryption technique;data security;authorized duplicate check scheme;Cloud computing;Encryption;Servers;Educational institutions;Protocols;Deduplication;authorized duplicate check;confidentiality;hybrid cloud","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2015 - A Hybrid Cloud Approach for Secure Authorized Deduplication.pdf","","","",""
"Journal article","Ma J,Stones RJ,Ma Y,Wang J,Ren J,Wang G,Liu X","","Lazy Exact Deduplication","ACM Trans. Storage","","","","","","","2017","13","2","1-26","Locality Based (per Ma 2017);All;Parallel Schemes;Grouped by Publication/ACM TOS","Exact Deduplication","","","Association for Computing Machinery","New York, NY, USA","","","","2017-06-10","","","","","1553-3077","","https://doi.org/10.1145/3078837;http://dx.doi.org/10.1145/3078837;https://dl.acm.org/doi/abs/10.1145/3078837?casa_token=hED2cwwSVYEAAAAA:MapXk0EPQvaaIBObEsEVxSfC2nPDg3iPvum_FNUE2TG8zVi7HXqmK9wGbjn7YOxVIlLHs6M5Emg4Ew;https://dl.acm.org/doi/pdf/10.1145/3078837?casa_token=GPyalHSrpfoAAAAA:j8nSCX8wZ1qiI4tDQjwxNniotfbp8ouxv7kWchsmZ1Hhe72FHmMSdwMCTAJiNpfYP2lui0opsgr48Q","10.1145/3078837","","","","Deduplication aims to reduce duplicate data in storage systems by removing redundant copies of data blocks, which are compared to one another using fingerprints. However, repeated on-disk fingerprint lookups lead to high disk traffic, which results in a bottleneck.In this article, we propose a “lazy” data deduplication method, which buffers incoming fingerprints that are used to perform on-disk lookups in batches, with the aim of improving subsequent prefetching. In deduplication in general, prefetching is used to improve the cache hit rate by exploiting locality within the incoming fingerprint stream. For lazy deduplication, we design a buffering strategy that preserves locality in order to facilitate prefetching. Furthermore, as the proportion of deduplication time spent on I/O decreases, the proportion spent on fingerprint calculation and chunking increases. Thus, we also utilize parallel approaches (utilizing multiple CPU cores and a graphics processing unit) to further improve the overall performance.Experimental results indicate that the lazy method improves fingerprint identification performance by over 50% compared with an “eager” method with the same data layout. The GPU improves the hash calculation by a factor of 4.6 and multithreaded chunking by a factor of 4.16. Deduplication performance can be improved by over 45% on SSD and 80% on HDD in the last round on the real datasets.","Deduplication, GPU, disk I/O, lazy method","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Ma et al. 2017 - Lazy Exact Deduplication.pdf","","","11","June 2017"
"Conference paper","Ng CH,Lee PP","","RevDedup: a reverse deduplication storage system optimized for reads to latest backups","","","","","Proceedings of the 4th Asia-Pacific Workshop on Systems","","","2013","","","1-7","All;p-scailbib","","","Article 15","Association for Computing Machinery","New York, NY, USA","","Singapore, Singapore","","2013-07-29","","2020-11-07","9781450323161","","","","https://doi.org/10.1145/2500727.2500731;http://dx.doi.org/10.1145/2500727.2500731;https://dl.acm.org/doi/abs/10.1145/2500727.2500731?casa_token=wTCx6HiP5ZQAAAAA:5caOL5tfNRah4Jz5xJqPc2s-VIUuQcCEdvficTPZnAqr5-0evICAG1JOLkI8C3IgoXR6cF6ODz0rgQ;https://dl.acm.org/doi/pdf/10.1145/2500727.2500731?casa_token=SD5AjZ3EotsAAAAA:jUUEMqPREOoCBWVpiTCLSX-XAtfJLIYDhyZfQ1Z6AiYwYgVJmM4TyCPt8ziv8FNpFa_PYlbBbBIk2Q","10.1145/2500727.2500731","","","","Deduplication is known to effectively eliminate duplicates, yet it introduces fragmentation that degrades read performance. We propose RevDedup, a deduplication system that optimizes reads to the latest backups of virtual machine (VM) images using reverse deduplication. In contrast with conventional deduplication that removes duplicates from new data, RevDedup removes duplicates from old data, thereby shifting fragmentation to old data while keeping the layout of new data as sequential as possible. We evaluate our RevDedup prototype using a 12-week span of real-world VM image snapshots of 160 users. We show that RevDedup achieves high deduplication efficiency, high backup throughput, and high read throughput.","","","","","","","APSys '13","15","","","","","","","","","","","","","","","","","All Papers/N/Ng and Lee 2013 - RevDedup - a reverse deduplication storage system optimized for reads to latest backups.pdf","","","",""
"Journal article","Li YK,Xu M,Ng CH,Lee PP","","Efficient Hybrid Inline and Out-of-Line Deduplication for Backup Storage","ACM Trans. Storage","","","","","","","2015","11","1","1-21","Out-of-line deduplication;All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2015-12-29","","","","","1553-3077","","https://doi.org/10.1145/2641572;http://dx.doi.org/10.1145/2641572;https://dl.acm.org/doi/abs/10.1145/2641572?casa_token=WSTr8YLqGtgAAAAA:88849IhepNX4sTkvtYpZ6QlWQBfbtiVxjRmAt-kdTm4neDAk3F7kGg_BpfqqOqxPvlb5WMgBpFIvWw;https://dl.acm.org/doi/pdf/10.1145/2641572?casa_token=8XqwG-eMmOQAAAAA:KvhL7mtXB2nhLFBBna46kh__gSvB7i-psu7Ek0uiv9LE0K48GAzIzVOSkGuH6JqWXuqROqZqKk5zAQ","10.1145/2641572","","","","Backup storage systems often remove redundancy across backups via inline deduplication, which works by referring duplicate chunks of the latest backup to those of existing backups. However, inline deduplication degrades restore performance of the latest backup due to fragmentation, and complicates deletion of expired backups due to the sharing of data chunks. While out-of-line deduplication addresses the problems by forward-pointing existing duplicate chunks to those of the latest backup, it introduces additional I/Os of writing and removing duplicate chunks.We design and implement RevDedup, an efficient hybrid inline and out-of-line deduplication system for backup storage. It applies coarse-grained inline deduplication to remove duplicates of the latest backup, and then fine-grained out-of-line reverse deduplication to remove duplicates from older backups. Our reverse deduplication design limits the I/O overhead and prepares for efficient deletion of expired backups. Through extensive testbed experiments using synthetic and real-world datasets, we show that RevDedup can bring high performance to the backup, restore, and deletion operations, while maintaining high storage efficiency comparable to conventional inline deduplication.","backup storage, Deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2015 - Efficient Hybrid Inline and Out-of-Line Deduplication for Backup Storage.pdf","","","2","February 2015"
"Conference paper","Zhike Zhang,Zejun Jiang,Zhiqiang Liu,Chengzhang Peng","","LHS: A novel method of information retrieval avoiding an index using linear hashing with key groups in deduplication","","","","","2012 International Conference on Machine Learning and Cybernetics","","","2012","4","","1312-1318","All","","","","","","","","","2012-07","","","","","2160-1348","","http://dx.doi.org/10.1109/ICMLC.2012.6359555;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6359555","10.1109/ICMLC.2012.6359555","","","","Indexing of RAM is important to information retrieval. In deduplication systems, we need to use methods of information retrieval to find duplicate data chunks quickly. Chunk-lookup disk bottleneck problem is one of the most important problems in the information retrieval of deduplication systems. Previous methods can reduce RAM usage of index a lot to avoid reading index from disk for every chunk search. However, these methods still need several TB of RAM to hold the index for dozens of PB of storage space utilization. We design Linear Hashing with Key Groups(LHs), a variation of Linear Hashing, to organize and address bins. Based on LHs, we propose a novel method of information retrieval in deduplication, which can avoid an index in RAM by utilizing LHs to compute the address of a bin. A bin contains the chunk IDs of the similar files to a file. Then, we do not need to maintain an index in RAM to do the same thing. Our method does not decrease the deduplication efficiency compared with Extreme Binning, when it needs one disk read for every file. For every file, our method firstly computes the bin address of this file using LHs, loads the bin and then deduplicates the file against the loaded bin. Experimental results show that, while our method does not need an index in RAM, the deduplication efficiency of our method is slightly better than that of Extreme Binning.","database indexing;information retrieval;random-access storage;search problems;storage management;LHS;information retrieval;linear hashing;key groups;RAM indexing;deduplication systems;duplicate data chunks;chunk-lookup disk bottleneck problem;RAM usage;chunk search;storage space utilization;deduplication efficiency;extreme binning;Indexes;Abstracts;Random access memory;Deduplication;chunk-lookup disk bottleneck problem;index;linear hashing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhike Zhang et al. 2012 - LHS - A novel method of information retrieval avoiding an index using linear hashing with key groups in deduplication.pdf","","","",""
"Conference paper","Jain N,Dahlin M,Tewari R","","Taper: Tiered approach for eliminating redundancy in replica synchronization","","","","","Proceedings of the 4th conference on USENIX Conference on File and Storage Technologies-Volume 4","","","2005","","","21-21","All;Grouped by Publication/FAST Papers","","USENIX Association","","","","","","","2005","","","","","","","https://www.usenix.org/event/fast05/tech/full_papers/jain/jain_html","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jain et al. 2005 - Taper - Tiered approach for eliminating redundancy in replica synchronization.pdf","","","",""
"Conference paper","Wang C,Qin Z,Yang L,Wang J","","A Fast Duplicate Chunk Identifying Method Based on Hierarchical Indexing Structure","","","","","2012 International Conference on Industrial Control and Electronics Engineering","","","2012","","","624-627","All","","","","","","","","","2012-08","","","","","","","http://dx.doi.org/10.1109/ICICEE.2012.169","10.1109/ICICEE.2012.169","","","","To solve the disk bottleneck problem of deduplication system without depending on the data locality, a fast duplicate chunk identifying method based on hierarchical indexing structure is proposed. In this method, the traditional flat indexing structure is vertically divided into two layers, and only a handful of the most representative indices selected according to the Broder's theorem are kept in the RAM. The experiment results on real data, which are lack of locality, indicate that the deduplication performance of this method can reach 87.05% of the optimal value with a far less RAM requirement than the current methods.","indexing;random-access storage;duplicate chunk identifying method;hierarchical indexing structure;disk bottleneck problem;deduplication system;flat indexing structure;representative indices;Broder theorem;RAM;Indexing;Random access memory;Throughput;Feature extraction;Writing;Educational institutions;deduplication;disk bottleneck;data locality;hierarchical indexing structure","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2012 - A Fast Duplicate Chunk Identifying Method Based on Hierarchical Indexing Structure.pdf","","","",""
"Conference paper","Tseng C,Ciou J,Liu T","","A Cluster-Based Data De-duplication Technology","","","","","2014 Second International Symposium on Computing and Networking","","","2014","","","226-230","All","","","","","","","","","2014-12","","","","","2379-1896","","http://dx.doi.org/10.1109/CANDAR.2014.22","10.1109/CANDAR.2014.22","","","","Data deduplication technology usually identifies redundant data quickly and correctly by using bloom filter technology. A bloom filter can determine whether there is redundant data. However, there are the presences of false positives. In order to avoid false positives, we need to compare a new chunk with chunks that have been stored. In order to reduce the time to exclude the bloom filter false positives, current research uses many small size index tables to store chunk ID. However, the target chunk ID only stores in one index table. Searching for the target chunk ID at another index table uselessly took a great deal of time. In this paper, we cluster the stored chunks to reduce the time of excluding the false positive problem induced by bloom filter.","data structures;pattern clustering;cluster-based data deduplication technology;redundant data identification;bloom filter technology;chunk ID;index table;Indexes;Arrays;Multimedia communication;Linux;Kernel;Cloud computing;System performance;Bloom filter;cluster;data deduplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tseng et al. 2014 - A Cluster-Based Data De-duplication Technology.pdf","","","",""
"Journal article","Zhang X,Zhu G,Wang E,Fowler S,Dong X","","Data De-Duplication with Adaptive Chunking and Accelerated Modification Identifying","Comput. Inform.","Computing and Informatics","","","","","","2016","35","3","586-614","All","","","","","","","","","2016-02-11","","2020-10-30","","","1335-9150","","http://www.cai.sk/ojs/index.php/cai/article/viewArticle/1687;http://www.cai.sk/ojs/index.php/cai/article/download/1687/768","","","","","The data de-duplication system not only pursues the high de-duplication rate, which refers to the aggregate reduction in storage requirements gained from de-duplication, but also the de-duplication speed. To solve the problem of random parameter-setting brought by Content Defined Chunking (CDC), a self-adaptive data chunking algorithm is proposed. The algorithm improves the de-duplication rate by conducting pre-processing de-duplication to the samples of the classified files and then selecting the appropriate algorithm parameters. Meanwhile, FastCDC, a kind of content-based fast data chunking algorithm, is adopted to solve the problem of low de-duplication speed of CDC. By introducing de-duplication factor and acceleration factor, FastCDC can significantly boost de-duplication speed while not sacrificing the de-duplication rate through adjusting these two parameters. The experimental results demonstrate that our proposed method can improve the de-duplication rate by about 5 %, while FastCDC can obtain the increase of de-duplication speed by 50 % to 200 % only at the expense of less than 3 % de-duplication rate loss.","","Precursor to FastCDC","","","en","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2016 - Data De-Duplication with Adaptive Chunking and Accelerated Modification Identifying.pdf","","","",""
"Conference paper","Zhang J,Zhang S,Lu Y,Zhang X,Wu S","","Hierarchical Data Deduplication Technology Based on Bloom Filter Array","","","","","Proceedings of the International Conference on Information Engineering and Applications (IEA) 2012","","","2013","","","725-732","All","","","","Springer London","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-1-4471-4856-2_88;https://link.springer.com/chapter/10.1007/978-1-4471-4856-2_88;https://link.springer.com/content/pdf/10.1007%2F978-1-4471-4856-2_88.pdf","10.1007/978-1-4471-4856-2_88","","","","In recent years, the data deduplication technology has become a research hotspot. In order to reduce the time and storage space requirements of deduplication technology, we propose a hierarchical deduplication approach which is based on file-level and block-level to eliminate redundant data, and introduce bloom filter (BF) to leach fingerprint to accelerate the search process. In order to further reduce the false positive rate of BF, the concept of bloom filter array (BFA) is applied. The performance results show that this strategy can effectively alleviate the pressure of storage and network transmission, raise the rate of data to be deleted and ensure higher data deduplication speed.","","Hierarchy is File Dedup to Block Dedup. Uses bloom filter and introduces bloom filter array","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Ma J,Bi C,Bai Y,Zhang L","","UCDC: Unlimited Content-Defined Chunking, A File-Differing Method Apply to File-Synchronization among Multiple Hosts","","","","","2016 12th International Conference on Semantics, Knowledge and Grids (SKG)","","","2016","","","76-82","All","Chunking","","","","","","","","2016-08","","","","","","","http://dx.doi.org/10.1109/SKG.2016.019;https://ieeexplore.ieee.org/abstract/document/7815080/","10.1109/SKG.2016.019","","","","Nowadays, the data centric system has been playing an increasingly important role in blogs sharing, content delivery and news broadcasting, file-synchronization, and so on. Due to generated amount of data within the system, data backup and archiving has become a main challenging task. A main methods to solve the problem is Chunking based deduplication by eliminating redundant data and reducing the total storage space. In this paper, we summarized several ways of file-differing, and then designs a new Unlimited Content-Defined Chunking (UCDC) algorithm, which contains file-chunking, file-comparing and file-merging. We evaluate the effectiveness of the UCDC by simulation example that produces the description of file.","file organisation;Web sites;UCDC algorithm;unlimited content-defined chunking algorithm;file-differing method;file-synchronization;blogs sharing;content delivery;news broadcasting;Algorithm design and analysis;Computers;Blogs;Broadcasting;Semantics;Computer science;Information processing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Ma et al. 2016 - UCDC - Unlimited Content-Defined Chunking, A File-Differing Method Apply to File-Synchronization among Multiple Hosts.pdf","","","",""
"Conference paper","Yang T,Feng D,Liu J,Wan Y,Niu Z,Ke Y","","3DNBS: A Data De-duplication Disk-Based Network Backup System","","","","","2009 IEEE International Conference on Networking, Architecture, and Storage","","","2009","","","287-294","All","Deduplication Systems","","","","","","","","2009-07","","","","","","","http://dx.doi.org/10.1109/NAS.2009.56;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5197342","10.1109/NAS.2009.56","","","","Traditionally, backup and archiving have been performed on tapes. With the rapid advances in disk storage technology witnessed in recent years, it becomes practical to use disks other than tape libraries as backend storage device for a backup system. For such a disk-based system, storage space efficiency is essential. Since traditional backup method cannot eliminate redundancies during backup, a new data de-duplication backup technique should be developed to provide more efficient data storage at the system. This paper describes the design and performance evaluation of a data de-duplication disk-based network backup system,called 3DNBS. 3DNBS breaks files into variable sized chunks using content-defined chunking (CDC) for the purpose of duplication detection. Chunks are indexed and addressed by hashing their content, which leads to intrinsically single instance storage. Experimental results show that in comparison with traditional backup method such as Bacula, 3DNBS presents dramatic reduction in required storage space on various workloads. By eliminating duplicated data, 3DNBS also reduces the size of data to be transmitted, hence reducing time to perform backup in a bandwidth constraint environment.","disc storage;performance evaluation;3DNBS;data de-duplication disk;network backup system;disk storage technology;backend storage device;performance evaluation;content-defined chunking;variable sized chunks;single instance storage;Web server;Space technology;Fingerprint recognition;File servers;Computer architecture;Computer networks;Libraries;Memory;Bandwidth;Costs;backup;de-duplication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2009 - 3DNBS - A Data De-duplication Disk-Based Network Backup System.pdf","","","",""
"Journal article","Mi B,Li Y,Darong H,Wei T,Zou Q","","Secure Data De-Duplication Based on Threshold Blind Signature and Bloom Filter in Internet of Things","IEEE Access","","","","","","","2020","8","","167113-167122","All","","","","","","","","","2020","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2020.3023750;https://ieeexplore.ieee.org/abstract/document/9195458/;https://ieeexplore.ieee.org/iel7/6287639/8948470/09195458.pdf","10.1109/ACCESS.2020.3023750","","","","Within the cloud environment, the availability of storage, as well as bandwidth, can be effectively preserved in virtue of data de-duplication. However, refraining redundancy from additional storage or communication is not trivial due to security concerns. Though intensive researches have been addressed on a convergent cryptosystem for secure data de-duplication, the conflicts amongst functionality, confidentiality, and authority remain unbalanced. More concretely, although data are obfuscated under convergent encryption, a violent dictionary attack is still efficacious since the whole pseudorandom process relies heavily on plaintexts. As for data ownership, the download privilege, which depends on hash value, may also be infringed due to the same reason. To dispose of these problems, we presented a conspiracy-free data de-duplication protocol based on a threshold blind signature in this article. With the help of multiple key servers, the outsourced file and de-duplication label will be computationally indistinguishable from random strings. We used the Boom filter as a tool to implement a proof of ownership, ensuring that the ownership claims made by users are real. It effectively prevents the attacker from using the stolen tag to get the whole file to gain file access without authorization. The most significant innovation of this article is to use homomorphism computation to aggregate and generate partial signature tags, and to introduce a secret sharing mechanism based on The Chinese Remainder Theorem to hide signature keys, thus balancing the security concerns of cloud and client. Compared with existing schemes, both communication and computation performances are preferable in our protocol. As far as we know, our scheme is the only data de-duplication scheme that satisfies the semantic security of ciphertext and label.","Servers;Cloud computing;Data privacy;Encryption;Internet of Things;Cloud;secure data de-duplication;threshold blind signature;Bloom filter","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Mi et al. 2020 - Secure Data De-Duplication Based on Threshold Blind Signature and Bloom Filter in Internet of Things.pdf","","","",""
"Journal article","Nayak SK,Tripathy S","","SEDS: secure and efficient server-aided data deduplication scheme for cloud storage","Int. J. Inf. Secur.","International Journal of Information Security","","","","","","2020","19","2","229-240","All","","","","","","","","","2020-04-01","","","","","1615-5270","","https://doi.org/10.1007/s10207-019-00455-w;http://dx.doi.org/10.1007/s10207-019-00455-w;https://link.springer.com/article/10.1007/s10207-019-00455-w","10.1007/s10207-019-00455-w","","","","Data deduplication is frequently used by many cloud service providers (CSPs) for efficient storage management. This process facilitates CSPs not to store the duplicate copies of a file. However, as data privacy is critical, data owners encrypt their data before outsourcing. Convergent encryption techniques have been proposed to enable deduplication over outsourced encrypted data. In this paper, we propose a server-aided data deduplication scheme for cloud storage, which supports duplication check across key servers. The most attractive feature of this scheme is the generation of fixed-size ciphertext regardless of the number of key servers. We showed that the proposed scheme achieves better performance than previous efforts both theoretically and experimentally. Further, we proved the security of the proposed scheme.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Li C,Shilane P,Douglis F,Shim H,Smaldone S,Wallace G","","Nitro: A Capacity-Optimized ${$SSD$}$ Cache for Primary Storage","","","","","2014 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 14)","","","2014","","","501-512","All","SSD","","","","","","","","2014","","","","","","","https://www.usenix.org/conference/atc14/technical-sessions/presentation/li_cheng_1;https://www.usenix.org/system/files/conference/atc14/atc14-paper-li_cheng_nitro.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Nitro - A Capacity-Optimized ${$SSD$}$ Cache for Primary Storage.pdf","","","",""
"Miscellaneous","Koller R,Rangaswami R","","I/O Deduplication","ACM Transactions on Storage","","","","","","","2010","6","3","1-26","All","","","","","","","","","2010","","","","","","","http://dx.doi.org/10.1145/1837915.1837921","10.1145/1837915.1837921","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Koller and Rangaswami 2010 - I - O Deduplication.pdf","","","",""
"Conference paper","Wang Q,Li J,Xia W,Kruus E,Debnath B,Lee PP","","Austere Flash Caching with Deduplication and Compression","","","","","2020 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 20)","","","2020","","","713-726","All","","","","","","","","","2020","","","","","","","https://www.usenix.org/conference/atc20/presentation/wang-qiuping;https://www.usenix.org/system/files/atc20-wang-qiuping.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2020 - Austere Flash Caching with Deduplication and Compression.pdf","","","",""
"Conference paper","Rashid F,Miri A,Woungang I","","A secure data deduplication framework for cloud environments","","","","","2012 Tenth Annual International Conference on Privacy, Security and Trust","","","2012","","","81-87","All","","","","","","","","","2012-07","","","","","","","http://dx.doi.org/10.1109/PST.2012.6297923;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6297923","10.1109/PST.2012.6297923","","","","Cloud computing has empowered the individual user by providing seemingly unlimited storage space and availability and accessibility of data anytime and anywhere. Cloud service providers are able to maximize data storage space by incorporating data deduplication into cloud storage. Although data deduplication removes data redundancy and data replication, it also introduces major data privacy and security issues for the user. In this paper, a new privacy-preserving framework that addresses this issue is proposed. Our framework uses an efficient deduplication algorithm to divide a given file into smaller units. These units are then encrypted by the user using the combination of a secure hash function and a block encryption algorithm. An index tree of hash values of these units is also generated and encrypted using an asymmetric search encryption scheme by the user. This index tree will enable the cloud service provider to search through the index and return the requested units. We will show that our proposed framework will allow cloud service and storage providers to employ data deduplication techniques without giving them access to either the users' plaintexts or the users' decryption keys.","cloud computing;cryptography;data privacy;trees (mathematics);secure data deduplication;cloud computing;data storage space;cloud storage;data redundancy;data replication;data privacy;security issue;privacy-preserving framework;secure hash function;block encryption algorithm;index tree;asymmetric search encryption scheme;Encryption;Cloud computing;Servers;Data privacy;Indexing;Secure data deduplication;B+ Tree indexing;cryptographic Cloud;TTTD chunking algorithm;asymmetric searchable encryption scheme;convergent encryption scheme","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rashid et al. 2012 - A secure data deduplication framework for cloud environments.pdf","","","",""
"Conference paper","Eshghi K,Lillibridge M,Wilcock L,Belrose G,Hawkes R","","Jumbo Store: Providing Efficient Incremental Upload and Versioning for a Utility Rendering Service","","","","","FAST","","","2007","7","","123-138","Grouped by Publication/FAST Papers;All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","","","","","","","","","2007","","","","","","","https://static.usenix.org/events/fast07/tech/full_papers/eshghi/eshghi.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/E/Eshghi et al. 2007 - Jumbo Store - Providing Efficient Incremental Upload and Versioning for a Utility Rendering Service.pdf","","","",""
"Miscellaneous","Park N,Lilja DJ","","Characterizing datasets for data deduplication in backup applications","IEEE International Symposium on Workload Characterization (IISWC'10)","","","","","","","2010","","","","All","","","","","","","","","2010","","","","","","","http://dx.doi.org/10.1109/iiswc.2010.5650369","10.1109/iiswc.2010.5650369","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Park and Lilja 2010 - Characterizing datasets for data deduplication in backup applications.pdf","","","",""
"Conference paper","Xia W,Jiang H,Feng D,Tian L","","Combining Deduplication and Delta Compression to Achieve Low-Overhead Data Reduction on Backup Datasets","","","","","2014 Data Compression Conference","","","2014","","","203-212","All","","","","","","","","","2014-03","","","","","2375-0359","","http://dx.doi.org/10.1109/DCC.2014.38;https://ieeexplore.ieee.org/abstract/document/6824428/?casa_token=nsRRc-PPvXwAAAAA:IT6f4L1KlsBwAZFo2--QsVo07eSHl6fFqskdyikxZAXkmZdy41eaqwPTX39NwkSzj8QoAWXQ1Hh4DA;https://ieeexplore.ieee.org/iel7/6823819/6824399/06824428.pdf?casa_token=Oenp8YzTY6sAAAAA:wOXDGxyutiPiSRpZUG9FOqETh0zB96v058OAwnftMbp977Cbzcterh_p2UjmEJf5MjDgYk9AsfAlRQ","10.1109/DCC.2014.38","","","","Data reduction has become increasingly important in storage systems due to the explosive growth of digital data in the world that has ushered in the big data era. In this paper, we present DARE, a Deduplication-Aware Resemblance detection and Elimination scheme for compressing backup datasets that effectively combines data deduplication and delta compression to achieve high data reduction efficiency at low overhead. The main idea behind DARE is to employ a scheme, call Duplicate-Adjacency based Resemblance Detection (DupAdj), by considering any two data chunks to be similar (i.e., candidates for delta compression) if their respective adjacent data chunks are found to be duplicate in a deduplication system, and then further enhance the resemblance detection efficiency by an improved super-feature approach. Our experimental results based on real-world and synthetic backup datasets show that DARE achieves an additional data reduction by a factor of more than 2 (2X) on top of deduplication with very low overhead while nearly doubling the data restore performance of deduplication-only systems by supplementing delta compression to deduplication.","Big Data;data compression;data reduction;delta compression;low-overhead data reduction;backup datasets;storage systems;digital data;big data era;DARE;deduplication-aware resemblance detection and elimination scheme;data deduplication;data reduction efficiency;duplicate-adjacency based resemblance detection;data chunks;super-feature approach;data restore performance;deduplication-only systems;DupAdj;Feature extraction;Redundancy;Containers;Indexing;Educational institutions;Prototypes;Scalability;data reduction;delta compression;deduplication;backup storage system","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2014 - Combining Deduplication and Delta Compression to Achieve Low-Overhead Data Reduction on Backup Datasets.pdf","","","",""
"Conference paper","Bartus P,Arzuaga E","","GDedup: Distributed File System Level Deduplication for Genomic Big Data","","","","","2018 IEEE International Congress on Big Data (BigData Congress)","","","2018","","","120-127","All","Deduplication Systems","","","","","","","","2018-07","","","","","","","http://dx.doi.org/10.1109/BigDataCongress.2018.00023;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8457739","10.1109/BigDataCongress.2018.00023","","","","During the last years, the cost of sequencing has dropped, and the amount of generated genomic sequence data has skyrocketed. As a consequence, genomic sequence data have become more expensive to store than to generate. The storage needs for genomic sequence data are also following this trend. In order to solve these new storage needs, different compression algorithms have been used. Nevertheless, typical compression ratios for genomic data range between 3 and 10. In this paper, we propose the use of GDedup, a deduplication storage system for genomics data, in order to improve data storage capacity and efficiency in distributed file systems without compromising I/O performance. GDedup can be developed by modifying existing storage system environments such as the Hadoop Distributed File System. By taking advantage of deduplication technology, we can better manage the underlying redundancy in genomic sequence data and reduce the space needed to store these files in the file systems, thus allowing for more capacity per volume. We present a study on the relation between the amount of different types of mutations in genomic data such as point mutations, substitutions, inversions, and the effect of such in the deduplication ratio for a data set of vertebrate genomes in FASTA format. The experimental results show that the deduplication ratio values are superior to the actual compression ratio values for both (file read-decompress or write-compress) I/O patterns, highlighting the potential for this technology to be effectively adapted to improve storage management of genomics data.","Big Data;biology computing;data compression;distributed databases;genomics;storage management;Distributed File System level deduplication;genomic big data;generated genomic sequence data;deduplication storage system;data storage capacity;distributed file systems;Hadoop Distributed File System;data set;vertebrate genomes;storage system environments;compression algorithms;I/O performance;GDedup;FASTA format;compression ratio values;file read-decompress;write-compress;I/O patterns;storage management;Genomics;Bioinformatics;Distributed databases;DNA;File systems;Indexes;Deduplication, gene mutations, DNA, genomics sequence data","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bartus and Arzuaga 2018 - GDedup - Distributed File System Level Deduplication for Genomic Big Data.pdf","","","",""
"Conference paper","Krishnaprasad PK,Narayamparambil BA","","A Proposal for Improving Data Deduplication with Dual Side Fixed Size Chunking Algorithm","","","","","2013 Third International Conference on Advances in Computing and Communications","","","2013","","","13-16","All","","","","","","","","","2013-08","","","","","","","http://dx.doi.org/10.1109/ICACC.2013.10;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6686327","10.1109/ICACC.2013.10","","","","DeDuplication is the technique of data reduction by breaking streams of data down into very granular components, and storing only the first instance of data items on the destination media and all the other similar occurrences to an index. Hash values are computed to identify the similar data items. Fixed size chunking (FSC) is a DeDuplication algorithm which breaks the data into fixed size chunks or blocks from the beginning of the file. But the main disadvantage of this technique is that, if new chunks are added in front or in the middle of a file, remaining chunks will get shifted from its initial position. This will yields a new hash value to the resulting chunks and thereby less DeDuplication ratio. But we can overcome this drawback by calculating hash values of chunks from the beginning as well as from the end of file and storing both values to metadata table. A new algorithm 'Dual Side Fixed Size Chunking' is proposed to get the high DeDuplication ratio over existing FSC. Without using computationally expensive Variable size chunking or content defined chunking, this algorithm can be effectively used for video or audio files to achieve a better DeDuplication ratio. This data reduction will provide network bandwidth savings and the ability to store more data on a given amount of disk or cloud storage. Reduced storage requirements will result in lower storage management and energy costs.","data reduction;storage management;data deduplication algorithm;dual side fixed size chunking algorithm;data reduction technique;destination media;data items;FSC;metadata;audio files;video files;cloud storage;energy costs;storage management;Power capacitors;Algorithm design and analysis;Servers;Bandwidth;Educational institutions;Electronic mail;Cloud computing;DeDuplication;Fixed Size Chunking;Fixed Block Hashing;DeDuplication Algorithm;Cloud Storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Krishnaprasad and Narayamparambil 2013 - A Proposal for Improving Data Deduplication with Dual Side Fixed Size Chunking Algorithm.pdf","","","",""
"Conference paper","Zhen Sun,Xiao N,Liu F,Fu Y","","DS-Dedupe: A scalable, low network overhead data routing algorithm for inline cluster deduplication system","","","","","2014 International Conference on Computing, Networking and Communications (ICNC)","","","2014","","","895-899","""Super-chunk"" term sources;All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Super-chunking;Similarity/Resemblance;Routing with Superchunks","","","","","","","","2014-02","","","","","","","http://dx.doi.org/10.1109/ICCNC.2014.6785456;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6785456","10.1109/ICCNC.2014.6785456","","","","Inline cluster deduplication technique has been widely used in data centers to improve storage efficiency. Data routing algorithm has a crucial impact on the deduplication factor, throughput and scalability in a cluster deduplication system. In this paper, we propose a stateful data routing algorithm called DS-Dedupe. To make full use of similarity in data streams, DS-Dedupe builds up a super-chunk granularity similarity index in each client to trace the super-chunks that have been routed. Then we calculate a similarity coefficient according to the index to determine whether a new super-chunk should be assigned directly or by a consistent hash, thus strike a sensible tradeoff between deduplication factor and network overhead. Our experiments on two datasets demonstrate that DS-Dedupe achieves a high elimination ratio at a low communication overhead. Besides, as data routing is operated by client node, metadata server bottleneck can be avoided.","computer centres;network servers;telecommunication network routing;telecommunication transmission lines;DS-Dedupe;scalable data routing;low network overhead data routing;inline cluster deduplication;data centers;stateful data routing;data streams;super-chunk granularity similarity index;consistent hash;sensible tradeoff;deduplication factor;elimination ratio;low communication overhead;client node;metadata server bottleneck;Routing;Servers;Indexes;Algorithm design and analysis;Clustering algorithms;Throughput;Fingerprint recognition;data deduplication;data routing algorithm;scalability;network overhead","Based on Sigma-Dedupe","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhen Sun et al. 2014 - DS-Dedupe - A scalable, low network overhead data routing algorithm for inline cluster deduplication system.pdf","","","",""
"Book chapter","San Kong J,Kim MJ,Lee WY,Ko YW","","Two-Level Metadata Management for Data Deduplication System","","","IST","","","","","2013","23","","299-303","All","","","","ASTL","","","","","2013","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Lkhagvasuren I,So JM,Lee JG,Yoo C,Ko YW","","Byte-index Chunking algorithm for data deduplication system","International Journal of Security and its Applications","","","","","","","2013","7","5","415-424","All","","","","Science and Engineering Research Support Society","","","","","2013","","","","","","","https://pdfs.semanticscholar.org/0880/325749067aefa23dce9bf63c4e92c6478773.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lkhagvasuren et al. 2013 - Byte-index Chunking algorithm for data deduplication system.pdf","","","",""
"Conference paper","Tolia N,Kozuch M,Satyanarayanan M,Karp B,Bressoud TC,Perrig A","","Opportunistic Use of Content Addressable Storage for Distributed File Systems","","","","","USENIX Annual Technical Conference, General Track","","","2003","3","","127-140","All;Grouped by Publication/Usenix ATC","","","","","","","","","2003","","","","","","","https://www.usenix.org/legacy/events/usenix03/tech/full_papers/full_papers/tolia/tolia.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tolia et al. 2003 - Opportunistic Use of Content Addressable Storage for Distributed File Systems.pdf","","","",""
"Journal article","Glass G,Martynov M,Zhang Q,Ran EL,Li D","","Redundancy elimination by aggregation of multiple chunks","US Patent","United States Patent and Trademark Office","","","","","","2010","","","","All","","","","","","","","","2010-10-12","","2020-09-01","","","","","https://patents.google.com/patent/US7814284B1/en","","","","","A data redundancy elimination system. In particular implementations, a method includes storing in a memory one or more aggregation trees, each aggregation tree comprising one or more base chunk nodes and one or more super chunk nodes, wherein each base chunk node comprises a chunk signature and corresponding raw data, and wherein super chunk nodes correspond to child base chunk nodes and include a chunk signature; receiving a data block; dividing the data block into a plurality of base chunks, each base chunk having a degree value characterizing the occurrence probability of the base chunk; computing chunk signatures for the plurality of base chunks; applying a super chunk rule to contiguous sequences of base chunks of the plurality of base chunks to create one or more aggregation trees, wherein the super chunk rule aggregates base chunks based on the respective occurrence probabilities of the base chunks; identifying one or more nodes in the one or more created aggregation trees that match corresponding nodes of the aggregation trees in the memory; compressing the received data block based on the identified nodes; and conditionally adding the one or more created aggregation trees to the memory.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Glass et al. 2010 - Redundancy elimination by aggregation of multiple chunks.pdf","","","",""
"Conference paper","Manber U","","Finding Similar Files in a Large File System","","","","","Proceedings of the Winter 1994 USENIX Technical Conference","","","1994","94","","1-10","All;Thesis","Chunking","","","","","","","","1994","","","","","","","https://static.usenix.org/publications/library/proceedings/sf94/full_papers/manber.finding","","","","","","","Great description of Rabin fingerprinting.","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Manber 1994 - Finding Similar Files in a Large File System.pdf","","","",""
"Journal article","Sun Zjason,Kuenning G,Mandal S,Shilane P,Tarasov V,Xiao N,Zadok E","","Cluster and Single-Node Analysis of Long-Term Deduplication Patterns","ACM Trans. Storage","","","","","","","2018","14","2","1-27","All;""Super-chunk"" term sources;SCAIL Bibliography;Grouped by Publication/ACM TOS;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;FSL Traces","Super-chunking;Routing with Superchunks","","","Association for Computing Machinery","New York, NY, USA","","","","2018-05-11","","","","","1553-3077","","https://doi.org/10.1145/3183890;http://dx.doi.org/10.1145/3183890;https://dl.acm.org/doi/10.1145/3183890","10.1145/3183890","","","","Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this article, we first collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We then analyzed the dataset, examining a variety of essential characteristics across two dimensions: single-node deduplication and cluster deduplication. For single-node deduplication analysis, our primary focus was individual-user data. Despite apparently similar roles and behavior among all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. For cluster deduplication analysis, we implemented seven published data-routing algorithms and created a detailed comparison of their performance with respect to deduplication ratio, load distribution, and communication overhead. We found that per-file routing achieves a higher deduplication ratio than routing by super-chunk (multiple consecutive chunks), but it also leads to high data skew (imbalance of space usage across nodes). We also found that large chunking sizes are better for cluster deduplication, as they significantly reduce data-routing overhead, while their negative impact on deduplication ratios is small and acceptable. We draw interesting conclusions from both single-node and cluster deduplication analysis and make recommendations for future deduplication systems design.","User study, data routing algorithms, large data set","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sun et al. 2018 - Cluster and Single-Node Analysis of Long-Term Deduplication Patterns.pdf","","","13","May 2018"
"Journal article","Harnik D,Hershcovitch M,Shatsky Y,Epstein A,Kat R","","Sketching Volume Capacities in Deduplicated Storage","ACM Trans. Storage","","","","","","","2019","15","4","1-23","All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2019-12-18","","","","","1553-3077","","https://doi.org/10.1145/3369737;http://dx.doi.org/10.1145/3369737;https://dl.acm.org/doi/10.1145/3369737","10.1145/3369737","","","","The adoption of deduplication in storage systems has introduced significant new challenges for storage management. Specifically, the physical capacities associated with volumes are no longer readily available. In this work, we introduce a new approach to analyzing capacities in deduplicated storage environments. We provide sketch-based estimations of fundamental capacity measures required for managing a storage system: How much physical space would be reclaimed if a volume or group of volumes were to be removed from a system (the reclaimable capacity) and how much of the physical space should be attributed to each of the volumes in the system (the attributed capacity). Our methods also support capacity queries for volume groups across multiple storage systems, e.g., how much capacity would a volume group consume after being migrated to another storage system? We provide analytical accuracy guarantees for our estimations as well as empirical evaluations. Our technology is integrated into a prominent all-flash storage array and exhibits high performance even for very large systems. We also demonstrate how this method opens the door for performing placement decisions at the data-center level and obtaining insights on deduplication in the field.","estimation, Deduplication, capacity management","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Harnik et al. 2019 - Sketching Volume Capacities in Deduplicated Storage.pdf","","","24","February 2020"
"Miscellaneous","Li J,Qin C,Lee PP,Zhang X","","Information Leakage in Encrypted Deduplication via Frequency Analysis","2017 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)","","","","","","","2017","","","","All;Thesis;p-scailbib","Frequency","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1109/dsn.2017.28","10.1109/dsn.2017.28","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2017 - Information Leakage in Encrypted Deduplication via Frequency Analysis.pdf","","","",""
"Miscellaneous","Paulo J,Pereira J","","Distributed Exact Deduplication for Primary Storage Infrastructures","Distributed Applications and Interoperable Systems","","","","","","","2014","","","52-66","All","","","","","","","","","2014","","","","","","","http://dx.doi.org/10.1007/978-3-662-43352-2_5","10.1007/978-3-662-43352-2_5","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Paulo and Pereira 2014 - Distributed Exact Deduplication for Primary Storage Infrastructures.pdf","","","",""
"Miscellaneous","Fingler H,Ra MR,Panta R","","Scalable, Efficient, and Policy-Aware Deduplication for Primary Distributed Storage Systems","2019 31st International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)","","","","","","","2019","","","","All","","","","","","","","","2019","","","","","","","http://dx.doi.org/10.1109/sbac-pad.2019.00038","10.1109/sbac-pad.2019.00038","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Paulo J,Pereira J","","Efficient Deduplication in a Distributed Primary Storage Infrastructure","ACM Trans. Storage","","","","","","","2016","12","4","1-35","All;Grouped by Publication/ACM TOS","","","","Association for Computing Machinery","New York, NY, USA","","","","2016-05-20","","","","","1553-3077","","https://doi.org/10.1145/2876509;http://dx.doi.org/10.1145/2876509;https://dl.acm.org/doi/abs/10.1145/2876509?casa_token=53E3cZLpq4IAAAAA:3sws5P1wa0LIqW8-yPEZSJcFWaoqTC5bMm_aK_0FxJcpl2BYOxtkglqZ9qBWinGs49VSBoxTRCIehA;https://dl.acm.org/doi/pdf/10.1145/2876509?casa_token=GQs06itT19kAAAAA:QrQa7lZ-ZcZpVfqp4NsDujmjL7EwT_S4Lbev1fefZOxQ8PsNw4xTW_CDUNNLpdL91N0FPvPsxrt66g","10.1145/2876509","","","","A large amount of duplicate data typically exists across volumes of virtual machines in cloud computing infrastructures. Deduplication allows reclaiming these duplicates while improving the cost-effectiveness of large-scale multitenant infrastructures. However, traditional archival and backup deduplication systems impose prohibitive storage overhead for virtual machines hosting latency-sensitive applications. Primary deduplication systems reduce such penalty but rely on special cluster filesystems, centralized components, or restrictive workload assumptions. Also, some of these systems reduce storage overhead by confining deduplication to off-peak periods that may be scarce in a cloud environment.We present DEDIS, a dependable and fully decentralized system that performs cluster-wide off-line deduplication of virtual machines’ primary volumes. DEDIS works on top of any unsophisticated storage backend, centralized or distributed, as long as it exports a basic shared block device interface. Also, DEDIS does not rely on data locality assumptions and incorporates novel optimizations for reducing deduplication overhead and increasing its reliability.The evaluation of an open-source prototype shows that minimal I/O overhead is achievable even when deduplication and intensive storage I/O are executed simultaneously. Also, our design scales out and allows collocating DEDIS components and virtual machines in the same servers, thus, sparing the need of additional hardware.","Primary storage, deduplication, distributed systems","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Paulo and Pereira 2016 - Efficient Deduplication in a Distributed Primary Storage Infrastructure.pdf","","","20","August 2016"
"Conference paper","Nachman A,Yadgar G,Sheinvald S","","GoSeed: Generating an Optimal Seeding Plan for Deduplicated Storage","","","","","18th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 20)","","","2020","","","193-207","Grouped by Publication/FAST Papers;All","","","","","","","","","2020","","","","","","","https://www.usenix.org/conference/fast20/presentation/nachman;https://www.usenix.org/system/files/fast20-nachman.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Nachman et al. 2020 - GoSeed - Generating an Optimal Seeding Plan for Deduplicated Storage.pdf","","","",""
"Conference paper","Zhang P,Huang P,He X,Wang H,Yan L,Zhou K","","RMD: A Resemblance and Mergence Based Approach for High Performance Deduplication","","","","","2016 45th International Conference on Parallel Processing (ICPP)","","","2016","","","536-541","All;""Segment"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Chunking;Similarity/Resemblance","","","","","","","","2016-08","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPP.2016.68;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7573857","10.1109/ICPP.2016.68","","","","Data deduplication, a data redundancy elimination technique, has been employed in almost all kinds of application environments to reduce storage space. However, one of the main challenges facing deduplication technology is to provide a fast key-value fingerprint index for large datasets, as the index performance is critical to the overall deduplication performance. This paper proposes RMD, a resemblance and mergence based deduplication scheme, which aims to provide quick responses to fingerprint queries. The key idea of RMD is to leverage a bloom filter array and the data resemblance algorithm to dramatically reduce the query range for deduplication. Moreover, RMD utilizes mergence based approach to merge resemblance segments to relevant bins, and exploits frequency-based Fingerprint Retention Policy to reduce the bin capacity to improve query throughput and improve data deduplication ratio. Extensive experimental results with real-world datasets have shown that RMD is able to achieve pretty high query performance and outperforms several state-of-the-art deduplication schemes.","data structures;parallel processing;query processing;storage management;RMD;high performance deduplication;data redundancy elimination;storage space reduction;key-value fingerprint index;index performance;fingerprint queries;bloom filter array;data resemblance algorithm;frequency-based fingerprint retention policy;data deduplication ratio;bin capacity;Indexes;Radio frequency;Fingerprint recognition;Random access memory;Arrays;Organizations;Linux;deduplication;key value index;data resemblance;fingerprint retrieval;DBA;Bloom Filter Array","Segment size is 64 (presumably 64 chunk fingerprints)","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2016 - RMD - A Resemblance and Mergence Based Approach for High Performance Deduplication.pdf","","","",""
"Journal article","Meyer DT,Bolosky WJ","","A study of practical deduplication","ACM Transactions on Storage","","","","","","","2012","7","4","1-20","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;Grouped by Publication/ACM TOS","","","","","","","","","2012","","","","","","","http://dx.doi.org/10.1145/2078861.2078864","10.1145/2078861.2078864","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Meyer and Bolosky 2012 - A study of practical deduplication.pdf","","","",""
"Miscellaneous","Agrawal N,Bolosky WJ,Douceur JR,Lorch JR","","A five-year study of file-system metadata","ACM Transactions on Storage","","","","","","","2007","3","3","9","All","","","","","","","","","2007","","","","","","","http://dx.doi.org/10.1145/1288783.1288788","10.1145/1288783.1288788","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Agrawal et al. 2007 - A five-year study of file-system metadata.pdf","","","",""
"Conference paper","Rhea SC,Cox R,Pesterev A","","Fast, Inexpensive Content-Addressed Storage in Foundation","","","","","USENIX Annual Technical Conference","","","2008","","","143-156","All;Grouped by Publication/Usenix ATC","","","","","","","","","2008","","","","","","","https://www.usenix.org/legacy/events/usenix08/tech/full_papers/rhea/rhea.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rhea et al. 2008 - Fast, Inexpensive Content-Addressed Storage in Foundation.pdf","","","",""
"Conference paper","Blanc O,Constant M,Watrin P","","A Finite-State Super-Chunker","","","","","Implementation and Application of Automata","","","2007","","","306-308","All","","","","Springer Berlin Heidelberg","","","","","2007","","","","","","","http://dx.doi.org/10.1007/978-3-540-76336-9_29;https://link.springer.com/content/pdf/10.1007%2F978-3-540-76336-9_29.pdf","10.1007/978-3-540-76336-9_29","","","","Language is full of multiword unit expressions that form basic semantic units. The identification of these structures limits the combinatorial complexity induced by lexical ambiguity. In this paper, we detail an experiment that largely integrates these notions in a finite-state procedure of segmentation into super-chunks, preliminary to a parser. We show that the chunker, developped for French, reaches 92.9% precision and 98.7% recall.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Blanc et al. 2007 - A Finite-State Super-Chunker.pdf","","","",""
"Miscellaneous","Sathe SC,Dongre NM","","Block Level based Data Deduplication and Assured Deletion in Cloud","2018 International Conference on Smart Systems and Inventive Technology (ICSSIT)","","","","","","","2018","","","","All","","","","","","","","","2018","","","","","","","http://dx.doi.org/10.1109/icssit.2018.8748482","10.1109/icssit.2018.8748482","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Kumar KJ,Latesh Kumar KJ,Lawrance R","","Novel Approach: Deduplication for Backup Systems Using Data Block Size","Computational Intelligence in Data Mining - Volume 1","","","","","","","2015","","","365-373","All","Chunking","","","","","","","","2015","","","","","","","http://dx.doi.org/10.1007/978-81-322-2205-7_35","10.1007/978-81-322-2205-7_35","","","","","","Applies more to network deduplication. English is a challenge, but seems to use block size (or the lowest bit of block size) to partition blocks for faster lookup.","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kumar et al. 2015 - Novel Approach - Deduplication for Backup Systems Using Data Block Size.pdf","","","",""
"Conference paper","Constantinescu C,Pieper J,Li T","","Block Size Optimization in Deduplication Systems","","","","","2009 Data Compression Conference","","","2009","","","442-442","All;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Super-chunking","","","","","","","","2009-03","","","","","2375-0359","","http://dx.doi.org/10.1109/DCC.2009.51;https://ieeexplore.ieee.org/abstract/document/4976496/","10.1109/DCC.2009.51","","","","Data deduplication is a popular dictionary based compression method in storage archival and backup. The deduplication efficiency improves for smaller chunk sizes, however the files become highly fragmented requiring many disk accesses during reconstruction or chattiness in a client-server architecture. Within the sequence of chunks that an object (file) is decomposed into, sub-sequences of adjacent chunks tend to repeat. We exploit this insight to optimize the chunk sizes by joining repeated sub-sequences of small chunks into new super chunks with the constraint to achieve practically the same matching performance. We employ suffix arrays to find these repeating sub-sequences and to determine a new encoding that covers the original sequence. With super chunks we significantly reduce fragmentation, improving reconstruction time and the overall deduplication ratio by lowering the amount of metadata. As a result, fewer chunks are used to represent a file, reducing the number of disk accesses needed to reconstruct the file and requiring fewer entries in the chunk dictionary and fewer hashes to encode a file. To encode a sequence of chunks we proved two facts: (1) any subsequence that repeats is part of some super chunk (su- permaximal in Figure 1) - therefore the dictionary contains just supermaximals and non-repeats, and (2) maximals are the only repeats not covered (overlapped) by the containing supermaximals - so once we discover the maximals with the suffix array, and encode them, there is no need for maintaining auxiliary data structures like bit masks, to guarantee the coverage (encoding) of the entire object. Our experimental evaluation (Figure 2) yields a reduction in fragmentation between 89%-97% and a reduction of dictionary metadata (number of entries) between 80%-97%, without increasing the total amount of storage required for unique chunks.","client-server systems;data compression;data structures;dictionaries;information retrieval systems;optimisation;deduplication systems;block size optimization;data deduplication;dictionary based compression method;storage archival;disk accesses;client-server architecture;auxiliary data structures;Dictionaries;Samarium;Encoding;Data compression;Optimization methods;Constraint optimization;Data structures;Data deduplication;Information archiving;Storage backup;File chunking","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Constantinescu et al. 2009 - Block Size Optimization in Deduplication Systems.pdf","","","",""
"Conference paper","Armknecht F,Bohli JM,Karame GO,Youssef F","","Transparent Data Deduplication in the Cloud","","","","","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security","","","2015","","","886-900","All","","","","Association for Computing Machinery","New York, NY, USA","","Denver, Colorado, USA","","2015-10-12","","2020-07-05","9781450338325","","","","https://doi.org/10.1145/2810103.2813630;http://dx.doi.org/10.1145/2810103.2813630;https://dl.acm.org/doi/10.1145/2810103.2813630","10.1145/2810103.2813630","","","","","secure data deduplication, transparent attestation of deduplication, cloud security","","","","","","CCS '15","","","","","","","","","","","","","","","","","","All Papers/A/Armknecht et al. 2015 - Transparent Data Deduplication in the Cloud.pdf","","","",""
"Conference paper","Shah P,So W","","Lamassu: Storage-efficient host-side encryption","","","","","2015 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 15)","","","2015","","","333-345","All","Metadata","","","","","","","","2015","","","","","","","https://www.usenix.org/conference/atc15/technical-session/presentation/shah;https://www.usenix.org/system/files/conference/atc15/atc15-paper-shah.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shah and So 2015 - Lamassu - Storage-efficient host-side encryption.pdf","","","",""
"Patent","","","System for backing up files from disk volumes on multiple nodes of a computer network","","","","","","","","1998","","","","All","Secure Dedup;CE/MLE","","","","","","","","1998-07-07","","","","","","","http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&Sect2=HITOFF&d=PALL&p=1&u=%2Fnetahtml%2FPTO%2Fsrchnum.htm&r=1&f=G&l=50&s1=5778395.PN.&OS=PN/5778395&RS=PN/5778395","","","","","A system for backing up files from disk volumes on multiple nodes of a computer network to a common random-access backup storage means. As part of the backup process, duplicate files (or portions of files) may be identified across nodes, so that only a single copy of the contents of the duplicate files (or portions thereof) is stored in the backup storage means. For each backup operation after the initial backup on a particular volume, only those files which have changed since the previous backup are actually read from the volume and stored on the backup storage means. In addition, differences between a file and its version in the previous backup may be computed so that only the changes to the file need to be written on the backup storage means. All of these enhancements significantly reduce both the amount of storage and the amount of network bandwidth required for performing the backup. Even when the backup data is stored on a shared-file server, data privacy can be maintained by encrypting each file using a key generated from a fingerprint of the file contents, so that only users who have a copy of the file are able to produce the encryption key and access the file contents. To view or restore files from a backup, a user may mount the backup set as a disk volume with a directory structure identical to that of the entire original disk volume at the time of the backup.","","","","","","","","","","","","","","","","Whiting,Douglas L. D,Tom","STAC Inc.","United States Patent","5,778,395","1995-10-23","Patent","","","","","","","",""
"Journal article","Flajolet P,Nigel Martin G","","Probabilistic counting algorithms for data base applications","J. Comput. System Sci.","Journal of Computer and System Sciences","","","","","","1985","31","2","182-209","All","","","","","","","","","1985-10-01","","","","","0022-0000","","http://www.sciencedirect.com/science/article/pii/0022000085900418;http://dx.doi.org/10.1016/0022-0000(85)90041-8;https://www.sciencedirect.com/science/article/pii/0022000085900418;https://www.sciencedirect.com/science/article/pii/0022000085900418/pdf?md5=4e045817344b56cd28fb30b1f234e056&pid=1-s2.0-0022000085900418-main.pdf&_valck=1","10.1016/0022-0000(85)90041-8","","","","This paper introduces a class of probabilistic counting algorithms with which one can estimate the number of distinct elements in a large collection of data (typically a large file stored on disk) in a single pass using only a small additional storage (typically less than a hundred binary words) and only a few operations per element scanned. The algorithms are based on statistical observations made on bits of hashed values of records. They are by construction totally insensitive to the replicative structure of elements in the file; they can be used in the context of distributed systems without any degradation of performances and prove especially useful in the context of data bases query optimisation.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Flajolet and Nigel Martin 1985 - Probabilistic counting algorithms for data base applications.pdf","","","",""
"Conference paper","Lin X,Douglis F,Li J,Li X,Ricci R,Smaldone S,Wallace G","","Metadata considered harmful to deduplication","","","","","7th Usenix Workshop on Hot Topics in Storage and File Systems (HotStorage 15)","","","2015","","","","Superchunking Paper Sources/CDC/Chunking/Metadata;All;FSL Traces;Thesis","Metadata","","","","","","","","2015","","","","","","","https://www.usenix.org/conference/hotstorage15/workshop-program/presentation/lin;https://www.usenix.org/system/files/conference/hotstorage15/hotstorage15-lin.pdf","","","","","","","The ""Metadata"" referred to here is file metadata in the OS, not deduplication metadata.","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2015 - Metadata considered harmful to deduplication.pdf","","","",""
"Miscellaneous","Malhotra J,Bakal J","","A survey and comparative study of data deduplication techniques","2015 International Conference on Pervasive Computing (ICPC)","","","","","","","2015","","","","All","Survey","","","","","","","","2015","","","","","","","http://dx.doi.org/10.1109/pervasive.2015.7087116","10.1109/pervasive.2015.7087116","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Malhotra and Bakal 2015 - A survey and comparative study of data deduplication techniques.pdf","","","",""
"Journal article","","","Data Structures and Algorithms: Table of Contents","","","","","","","","","","","","All","","","","","","","","","","","","","","","","https://doc.lagout.org/Alfred%20V.%20Aho%20-%20Data%20Structures%20and%20Algorithms.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Data Structures and Algorithms - Table... - Data Structures and Algorithms - Table of Contents.pdf","","","",""
"Journal article","Qin C,Li J,Lee PP","","The Design and Implementation of a Rekeying-Aware Encrypted Deduplication Storage System","ACM Transactions on Storage","","","","","","","2017","13","1","","All","Chunking;Fingerprint-Indexing;Similarity/Resemblance","","","","","","","","2017-02-25","","","","","","","https://doi.org/10.1145/3032966;https://dl.acm.org/doi/abs/10.1145/3032966;https://dl.acm.org/doi/pdf/10.1145/3032966;https://dl.acm.org/doi/pdf/10.1145/3032966?download=true","","","","","Rekeying refers to an operation of replacing an existing key with a new key for encryption. It renews security protection to protect against key compromise and enable dynamic access control in cryptographic storage. However, it is non-trivial to realize efficient rekeying in encrypted deduplication storage systems, which use deterministic content-derived encryption keys to allow deduplication on ciphertexts. We design and implement a rekeying-aware encrypted deduplication (REED) storage system. REED builds on a deterministic version of all-or-nothing transform, such that it enables secure and lightweight rekeying, while preserving the deduplication capability. We propose two REED encryption schemes that trade between performance and security and extend REED for dynamic access control. We implement a REED prototype with various performance optimization techniques and demonstrate how we can exploit similarity to mitigate key generation overhead. Our trace-driven testbed evaluation shows that our REED prototype maintains high performance and storage efficiency.","deduplication, Encryption, cloud storage, rekeying","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Q/Qin et al. 2017 - The Design and Implementation of a Rekeying-Aware Encrypted Deduplication Storage System.pdf","","","",""
"Journal article","Yang,Lu,Shao,Tang,Ghorbani","","Achieving Efficient Secure Deduplication with User-Defined Access Control in Cloud","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","2020","","","1-1","All","Secure Dedup","","","","","","","","2020-04-01","","","","","","","http://dx.doi.org/10.1109/TDSC.2020.2987793;https://www.computer.org/csdl/journal/tq/5555/01/09069266/1j4GC3DLyCI","10.1109/TDSC.2020.2987793","","","","Cloud storage as one of the most important services of cloud computing facilitates cloud users to outsource their data to the cloud for storage and share them with authorized users. In cloud storage, secure deduplication has been investigated as it can eliminate the redundancy over the encrypted data to reduce storage space and communication overhead. Regarding the security and privacy, many existing secure deduplication schemes generally focus on achieving the following properties: data confidentiality, tag consistency, access control and resistance to brute force attacks. However, as far as we know, none of them can achieve these four requirements at the same time. To overcome this shortcoming, in this paper, we propose an efficient secure deduplication scheme that supports user-defined access control. Specifically, by allowing only the cloud service provider to authorize data access on behalf of data owners, our scheme can maximally eliminate duplicates without violating the security and privacy of cloud users. Detailed security analysis shows that our authorized secure deduplication scheme achieves data confidentiality and tag consistency while resisting brute-force attacks. Furthermore, extensive simulations demonstrate that our scheme outperforms the existing competing schemes, in terms of computational, communication and storage overheads as well as the effectiveness of deduplication.","","Preprint found 1st May 2020. PDF not yet available.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2020 - Achieving Efficient Secure Deduplication with User-Defined Access Control in Cloud.pdf","","","",""
"Journal article","Xia W,Zou X,Jiang H,Zhou Y,Liu C,Feng D,Hua Y,Hu Y,Zhang Y","","The Design of Fast Content-Defined Chunking for Data Deduplication Based Storage Systems","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2020","31","9","2017-2031","All;Grouped by Publication/IEEE Trans Parallel Distrib Syst","Chunking","","","","","","","","2020-09","","","","","1045-9219","","http://dx.doi.org/10.1109/TPDS.2020.2984632;https://ieeexplore.ieee.org/abstract/document/9055082/","10.1109/TPDS.2020.2984632","","","","Content-Defined Chunking (CDC) has been playing a key role in data deduplication systems recently due to its high redundancy detection ability. However, existing CDC-based approaches introduce heavy CPU overhead because they declare the chunk cut-points by computing and judging the rolling hashes of the data stream byte by byte. In this article, we propose FastCDC, a Fast and efficient Content-Defined Chunking approach, for data deduplication-based storage systems. The key idea behind FastCDC is the combined use of five key techniques, namely, gear based fast rolling hash, simplifying and enhancing the Gear hash judgment, skipping sub-minimum chunk cut-points, normalizing the chunk-size distribution in a small specified region to address the problem of the decreased deduplication ratio stemming from the cut-point skipping, and last but not least, rolling two bytes each time to further speed up CDC. Our evaluation results show that, by using a combination of the five techniques, FastCDC is 3-12X faster than the state-of-the-art CDC approaches, while achieving nearly the same and even higher deduplication ratio as the classic Rabin-based CDC. In addition, our study on the deduplication throughput of FastCDC-based Destor (an open source deduplication project) indicates that FastCDC helps achieve 1.2-3.0X higher throughput than Destor based on state-of-the-art chunkers.","Microsoft Windows;Gears;Power capacitors;Redundancy;Acceleration;Throughput;Distributed databases;Data deduplication;content-defined chunking;storage system;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2020 - The Design of Fast Content-Defined Chunking for Data Deduplication Based Storage Systems.pdf","","","",""
"Conference paper","Wallace G,Douglis F,Qian H,Shilane P,Smaldone S,Chamness M,Hsu W","","Characteristics of backup workloads in production systems","","","","","FAST","","","2012","12","","4-4","All;Grouped by Publication/FAST Papers;Fingerdiff Refs;Thesis","Datasets","","","","","","","","2012","","","","","","","https://www.usenix.org/legacy/events/fast12/tech/slides/Wallace.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wallace et al. 2012 - Characteristics of backup workloads in production systems.pdf;All Papers/W/Wallace et al. 2012 - Characteristics of backup workloads in production systems.pdf","","","",""
"Miscellaneous","Wei J,Zhou K,Tian L,Wang H,Feng D","","A Fast Dual-level Fingerprinting Scheme for Data Deduplication","International Journal of Digital Content Technology and its Applications","","","","","","","2012","6","1","271-282","All","Chunking","","","","","","","","2012","","","","","","","http://dx.doi.org/10.4156/jdcta.vol6.issue1.33","10.4156/jdcta.vol6.issue1.33","","","","","","Uses multicore fingerprinting at file level and chunk level at the same time.","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Luo S,Hou M","","A novel chunk coalescing algorithm for data deduplication in cloud storage","","","","","2013 IEEE Jordan Conference on Applied Electrical Engineering and Computing Technologies (AEECT)","","","2013","","","1-5","All;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Chunking;Super-chunking","","","","","","","","2013-12","","","","","","","http://dx.doi.org/10.1109/AEECT.2013.6716447;https://ieeexplore.ieee.org/abstract/document/6716447/","10.1109/AEECT.2013.6716447","","","","Data deduplication has been widely attracted by researchers in cloud storage. Existing works usually adopt variable-size chunking and merge some subchunks into superchunk so as to improve storage and bandwidth utilization, but the overheads of chunk coalescing process are not considered in these algorithms. Due to this situation, we propose a novel chunk coalescing algorithm in this paper, which concerns both the maximum and minimum number of subchunks, that should be coalesced into superchunks. Experimental results show that our algorithm indeed reduces the overheads of chunk coalescing process and accelerates the whole procedure during the data deduplication.","cloud computing;storage management;chunk coalescing algorithm;data deduplication;cloud storage;variable-size chunking;superchunk;storage utilization;bandwidth utilization;chunk coalescing process;Algorithm design and analysis;Fingerprint recognition;Partitioning algorithms;Computers;Cloud computing;Educational institutions;Conferences;Data deduplication;Chunk coalescing;Cloud storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Luo and Hou 2013 - A novel chunk coalescing algorithm for data deduplication in cloud storage.pdf","","","",""
"Journal article","Guo C,Jiang X,Choo KK,Jie Y","","R-Dedup: Secure client-side deduplication for encrypted data without involving a third-party entity","Journal of Network and Computer Applications","","","","","","","2020","162","","102664","All;Thesis","Secure Dedup;client-side deduplication","","","","","","","","2020-07-15","","","","","1084-8045","","https://www.sciencedirect.com/science/article/pii/S1084804520301387;http://dx.doi.org/10.1016/j.jnca.2020.102664;http://www.sciencedirect.com/science/article/pii/S1084804520301387;https://www.sciencedirect.com/science/article/pii/S1084804520301387?casa_token=1ei4LL4LPjQAAAAA:6fQhfQ6tJ8Zxg_owRkiiQdjOmdIxHNGzYQnbjAApexvmNv1dXnOf3VGtdK4hH5igXwOIGkxXacA;https://pdf.sciencedirectassets.com/272436/AIP/1-s2.0-S1084804520301387/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJ%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIBk5rvvd0QeFN0vFC2P9WjglmEPtT1BSIXZXDTA3vKfMAiEA1HERYmPlAR7OBDX1B%2BDmxsZBC%2FhFsl9qcaW2LWqM4M0qvQMIuP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARADGgwwNTkwMDM1NDY4NjUiDF6eujmHeJB%2FiXxEfiqRA%2F0DPBwswxidTICY3JO1NIv9XZLe%2BjectA4CKV%2FC5Fbl%2FLkUiVLd3dFqhld%2FSf5V4OEitYxuOWgRlHLlRL4TNjinOzM5fWzaJwZnFhz1j7SQPPifxdy%2Fq76AB80%2FoVaSNSsUvtGa27jJ13EK4y%2Bt4DMk1Ky1KJ8llt0LP2VobhPvewWmIGEviVrmZcvHTBQnNGNP7rzTYzY8riBNJor%2BnhXf%2FkxAq6W%2Bf1XqDEy80zzCQJBWj0cttpzrwv%2BoXeIbpbeOBBXBDRzRBE3R3o3eonetShLZ9lI7jCOKiVQ0l62uPr336M8827zmZ1ua1Sx009onP62QD65JgL7PtvJ3G6uitwite0uFU0WZbVIlFRbKVrVFdZqyHSIEJcbajxB%2FRNdFSIN%2B9JaaIM0%2BNmH6kD7HsT8ewpR5rPQlSRxyCtk4hsvJXSt37FuWXPSo34LGzR0%2BSneuObbJQDwAlkIv6bXY4Ai3Yod%2FA5kGVrpiCJqTre71ZQHaw4pTTJlLpaS3NCkt1MIFC%2FoZbHu%2BTwikxzzxMJuY5fQFOusBXYpkoe2jBwGoMSyDqoSei5JE3cxHKoDBAlCwU1FrlN95GBxV6%2F6Bip3SazwHGChuQnfPGnG9sKEfgYVnbdxJyQtyzQeU%2Bfn0m2FXAGPU56LU1DW6wUfh3%2BCjaQj4pcZT%2FBmzWzBZHTsqelln%2Fq9Ebw%2FaLvIxMGv8i7qKmkwqycCNxbquCcq6VrHAk9F2fPNNn%2B6LR%2FPmH064sRzP8opICyPEldAJlwb31fQWPy2cTg9pP6CvhTfqwtFbglItqK5sx8LjB0nouoxzqIO%2B5NT6lNh8XZkeeqK%2B8X4kT9we1shbBZVWL2GgpZIhlQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200417T071057Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTJVVUSYR%2F20200417%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=31c3c2869e34780cd987a28aa65bfea177dc61ab89870c91c66f9c73bb60bd7c&hash=d424ea0baee9611fc2d7eb1604beb208de6bb4667025ff358df6069c7878644a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1084804520301387&tid=spdf-f7068b42-04fc-429d-9422-7a5da196a599&sid=d107a1122a1c0644d5596d5627b1da5aa0fbgxrqb&type=client","10.1016/j.jnca.2020.102664","","","","Deduplication technology is used extensively in applications such as cloud computing services to optimize their storage performance (e.g., cloud service providers store only one copy of identical data). However, due to the use of encryption (to ensure the confidentiality of data stored in the cloud), it can be challenging for cloud service providers to perform deduplication (e.g., due to the use of different keys on the same content). In this paper, we propose a randomized, secure, cross-user deduplication scheme (R-Dedup). The scheme does not involve any third-party entity (e.g., an additional cloud server) or require assistance from other users. In R-Dedup, the randomness feature means that users with identical copies of a file share the same random value via ElGamal encryption. The security analysis and experimental results demonstrate that R-Dedup is lightweight, and achieves both data privacy and data integrity.","Deduplication; Encrypted data; Semantic security; Brute-force attack","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Guo et al. 2020 - R-Dedup - Secure client-side deduplication for encrypted data without involving a third-party entity.pdf;All Papers/G/Guo et al. 2020 - R-Dedup - Secure client-side deduplication for encrypted data without involving a third-party entity.pdf","","","",""
"Journal article","Singhal S,Sharma P,Aggarwal RK,Passricha V","","A Global Survey on Data Deduplication","IJGHPC","International Journal of Grid and High Performance Computing (IJGHPC)","","","","","","2018","10","4","43-66","All","Survey","","","IGI Global","","","","","2018-10-01","","2020-03-16","","","","","https://www.igi-global.com/article/a-global-survey-on-data-deduplication/210174;http://dx.doi.org/10.4018/IJGHPC.2018100103;https://dl.acm.org/doi/abs/10.4018/IJGHPC.2018100103","10.4018/IJGHPC.2018100103","","","","This article describes how data deduplication efficiently eliminates the redundant data by selecting and storing only single instance of it and becoming popular in storage systems. Digital data is growing much faster than storage volumes, which shows the importance of data deduplication among scient...","","","","","en","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Lin B,Li S,Liao X,Liu X,Zhang J,Jia Z","","CareDedup: Cache-Aware Deduplication for Reading Performance Optimization in Primary Storage","","","","","2016 IEEE First International Conference on Data Science in Cyberspace (DSC)","","","2016","","","1-10","All","Chunking","","","ieeexplore.ieee.org","","","","","2016-06","","","","","","","http://dx.doi.org/10.1109/DSC.2016.56;https://ieeexplore.ieee.org/abstract/document/7866101/;https://dl.acm.org/doi/pdf/10.1145/2541614.2541615?casa_token=-cci49KheYYAAAAA:E5Bbwitax2rn6YAoXhn0XdxF365lZFpS3S6brjG2LmXg9mioyIO5AZ9CnY033Ugr-rZ3yDgdOcih-A","10.1109/DSC.2016.56","","","","Deduplication technology has been increasingly used to reduce the primary storage cost. In practice, it often causes additional on-disk fragmentation that impairs the reading performance. Existing deduplication algorithms mainly focus on the static data layout design so that the random I/O requests are largely avoided and the harmful effect can be alleviated. However, our trace-driven emulations show that, deduplication does not always impair the reading. It offers unique new opportunities for reading performance optimization by more possible cache hits. Motivated by this, we propose a novel cache-aware deduplication scheme CareDedup to well leverage the new opportunities. Based on a uniform locality assessment algorithm design, CareDedup selects the most profitable duplicated blocks to deduplicate for maximizing the reading performance. Our experimental evaluation using real-world traces shows that compared with the sequence-based deduplication algorithms, the duplicate elimination ratio and the reading performance (latency) can be both improved simultaneously. Given a desired duplicate elimination ratio, CareDedup can consistently outperforms sequence-based method by further reducing the reading latency by 2-5%.","cache storage;data handling;optimisation;storage management;CareDedup;cache-aware deduplication;reading performance optimization;primary storage;primary storage cost;on-disk fragmentation;random I/O requests;static data layout design;trace-driven emulations;sequence-based deduplication algorithms;duplicate elimination ratio;Optimization;Emulation;Servers;Layout;Algorithm design and analysis;Measurement;Electronic mail;Deduplication;Fragmentation;Cache","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lin et al. 2016 - CareDedup - Cache-Aware Deduplication for Reading Performance Optimization in Primary Storage.pdf","","","",""
"Conference paper","Salada J,Barreto J","","TurboSockets: Democratizing Distributed Deduplication","","","","","2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications","","","2013","","","1291-1298","All","","","","ieeexplore.ieee.org","","","","","2013-07","","","","","2324-9013","","http://dx.doi.org/10.1109/TrustCom.2013.154;https://ieeexplore.ieee.org/abstract/document/6680976/;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.642.7593&rep=rep1&type=pdf","10.1109/TrustCom.2013.154","","","","Distributed deduplication is one of today's most prominent techniques for efficient data transfer of data across a network. However, leveraging a distributed application with distributed deduplication capabilities is a complex challenge, not accessible to the average programmer. This paper advocates that the time has come to devise general-purpose middleware abstractions that can democratize the use of state-of-art distributed deduplication techniques, even by programmers with no know-how on the field. We propose TurboSockets, the first middleware abstraction that aims at such a goal. The TurboSockets middleware enables unskilled programmers to establish a communication channel between two remote processes and, through that channel, exchange data streams whose content is deduplicated by stateof-the-art algorithms. Turbosockets hides all the complexity associated with the deduplication protocol away from the programmer, closely resembling traditional inter-process communication APIs. Using a full-fledged prototype of the TurboSockets middleware, experimental results with real workloads confirm gains in performance and transferred volumes for a wide range of real workloads and scenarios.","application program interfaces;electronic data interchange;middleware;TurboSockets middleware;distributed deduplication democratization;data transfer;general-purpose middleware abstractions;data stream exchange;deduplication protocol;interprocess communication API;Sockets;Receivers;Protocols;Redundancy;Distributed databases;Middleware;Bandwidth;distributed deduplication;data redundancy;sockets;middleware","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Salada and Barreto 2013 - TurboSockets - Democratizing Distributed Deduplication.pdf","","","",""
"Journal article","Zhou B,Wen JT","","Improving Metadata Caching Efficiency for Data Deduplication via In-RAM Metadata Utilization","J. Comput. Sci. Technol.","Journal of computer science and technology","","","","","","2016","31","4","805-819","All","Metadata","","","Springer","","","","","2016-07-01","","","","","1000-9000","1860-4749","https://doi.org/10.1007/s11390-016-1664-0;http://dx.doi.org/10.1007/s11390-016-1664-0;https://link.springer.com/article/10.1007/s11390-016-1664-0;http://search.proquest.com/openview/25809ab68d1b29595896544b6275bbc4/1.pdf?pq-origsite=gscholar&cbl=326258&casa_token=QcFj9dO3Q6oAAAAA:HurOUwk59PEhmoyuJHcuF-wMz1y6PksfB59hb_FjaY3UDr7MnN4wLM1bHkjkzlmXv974qb22Fg","10.1007/s11390-016-1664-0","","","","We describe a data deduplication system for backup storage of PC disk images, named in-RAM metadata utilizing deduplication (IR-MUD). In-RAM hash granularity adaptation and miniLZO based data compression are firstly proposed to reduce the in-RAM metadata size and thereby reduce the space overheads required by the in-RAM metadata caches. Secondly, an in-RAM metadata write cache, as opposed to the traditional metadata read cache, is proposed for further reducing metadata-related disk I/O operations and improving deduplication throughput. During deduplication, the metadata write cache is managed following the LRU caching policy. For each manifest that is hit in the metadata write cache, an expensive manifest reloading operation from the disk is avoided. After deduplication, all the manifests in the metadata write cache are cleared and stored on the disk. Our experimental results using 1.5 TB real-world disk image dataset show that 1) IR-MUD achieved about 95% size reduction for the deduplication metadata, with a small time overhead introduced, 2) when the metadata write cache was not utilized, with the same RAM space size for the metadata read cache, IR-MUD achieved a 400% higher RAM hit ratio and a 50% higher deduplication throughput, as compared with the classic Sparse Indexing deduplication system where no metadata utilization approaches are utilized, and 3) when the metadata write cache was utilized and enough RAM space was available, IR-MUD achieved a 500% higher RAM hit ratio compared with Sparse Indexing and a 70% higher deduplication throughput compared with IR-MUD with only a single metadata read cache. The in-RAM metadata harnessing and metadata write caching approaches of IR-MUD can be applied in most parallel deduplication systems for improving metadata caching efficiency.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou and Wen 2016 - Improving Metadata Caching Efficiency for Data Deduplication via In-RAM Metadata Utilization.pdf","","","",""
"Journal article","Kaczmarczyk M,Dubnicki C","","Reducing fragmentation impact with forward knowledge in backup systems with deduplication","Proceedings of the 8th ACM International","","","","","","","2015","","","","All","Restore optimization ","","","dl.acm.org","","","","","2015","","","","","","","https://dl.acm.org/doi/abs/10.1145/2757667.2757678?casa_token=TqpXFYTgl3AAAAAA:M1EhRqWisIIpiz8m1SnGd-6YhG1mDPKrODAnhUwVgsNqr1cTtBCzH3zLkGqehln_r7uixe0ohW-8_w;https://dl.acm.org/doi/pdf/10.1145/2757667.2757678?casa_token=mDlPXxBeR0UAAAAA:20U6YES_RrF-TTUo_tW6ooX18K0jYPzx-Z4pm-q8025n8vfYAkaH-bwd-0-e5c3xG_Vtq214soazNA","","","","","Deduplication of backups is very effective in saving storage, but may also cause significant restore slowdown. This problem is caused by data fragmentation, where logically continuous but duplicate data is not placed sequentially on the disk. Two types of fragmentation …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaczmarczyk and Dubnicki 2015 - Reducing fragmentation impact with forward knowledge in backup systems with deduplication.pdf","","","",""
"Conference paper","Zhang Z,Bhagwat D,Litwin W,Long D,Schwarz SJ","","Improved deduplication through parallel Binning","","","","","2012 IEEE 31st International Performance Computing and Communications Conference (IPCCC)","","","2012","","","130-141","All;Thesis","","","","","","","","","2012-12","","","","","1097-2641","","http://dx.doi.org/10.1109/PCCC.2012.6407746;https://ieeexplore.ieee.org/abstract/document/6407746/;ftp://ftp.cse.ucsc.edu/pub/darrell/zhang-ipccc12.pdf","10.1109/PCCC.2012.6407746","","","","Many modern storage systems use deduplication in order to compress data by avoiding storing the same data twice. Deduplication needs to use data stored in the past, but accessing information about all data stored can cause a severe bottleneck. Similarity based deduplication only accesses information on past data that is likely to be similar and thus more likely to yield good deduplication. We present an adaptive deduplication strategy that extends Extreme Binning and investigate theoretically and experimentally the effects of the additional bin accesses.","data compression;parallel processing;improved deduplication;parallel binning;data compression;adaptive deduplication strategy;extreme binning;Indexes;Random access memory;Feature extraction;Data structures;Probability;Companies;Search engines","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2012 - Improved deduplication through parallel Binning.pdf","","","",""
"Website","","","Hashing Concepts and the Java Programming Language: Library Hash Functions","","","","","","","","","","","","All","","","","","","","","","","","2020-03-05","","","","","http://www.serve.net/buz/hash.adt/java.002.html","","","","","","","Buzhash. Cited and used in Kumar Genetic Optimized Deduplication","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Kumar N,Antwal S,Samarthyam G,Jain SC","","Genetic optimized data deduplication for distributed big data storage systems","","","","","2017 4th International Conference on Signal Processing, Computing and Control (ISPCC)","","","2017","","","7-15","All","","","","","","","","","2017-09","","","","","","","http://dx.doi.org/10.1109/ISPCC.2017.8269581;https://ieeexplore.ieee.org/abstract/document/8269581/;https://ieeexplore.ieee.org/iel7/8258908/8269571/08269581.pdf","10.1109/ISPCC.2017.8269581","","","","Content-Defined Chunking (CDC) detect maximum redundancy in data deduplication systems in the past years. In this research work, we focus on optimizing the deduplication system by adjusting the pertinent factors in content defined chunking (CDC) to identify as the key ingredients by declaring chunk cut-points and efficient fingerprint lookup using bucket based index partitioning. For efficient chunking, we propose Genetic Evolution (GE) algorithm based approach which is optimized Two Thresholds Two Divisors (TTTD-P) CDC algorithm where we significantly reduce the number of computing operations by using single dynamic optimal parameter divisor D with optimal threshold value exploiting the multi-operations nature of TTTD. To reduce the chunk-size variance, TTTD algorithm introduces an additional backup divisor D' that has a higher probability of finding cut-points. However, adding an additional divisor decreases the chunking throughput, meaning that TTTD algorithm aggravates Rabin's CDC performance bottleneck. To this end, Asymmetric Extremum (AE) significantly improves chunking throughput while providing comparable deduplication efficiency by using the local extreme value in a variable-sized asymmetric window to overcome the Rabin, MAXP and TTTD boundaries-shift problem. FAST CDC in the year 2016 is about 10 times faster than unimodal Rabin CDC and about 3 times faster than Gear and Asymmetric Extremum (AE) CDC, while achieving nearby the same deduplication ratio (DR). Therefore, we propose GE based TTTD-P optimized chunking to maximize chunking throughput with increased DR; and bucket indexing approach reduces hash values judgement time to identify and declare redundant chunk about 16 times than unimodal baseline Rabin CDC, 5 times than AE CDC, 1.6 times than FAST CDC. Our experimental results comparative analysis reveals that TTTD-P using fast BUZ rolling hash function with bucket indexing on Hadoop Distributed File System (HDFS) provide a comparatively maximum redundancy detection with higher throughput, higher deduplication ratio, lesser computation time and very low hash values comparison time as being best data deduplication for distributed big data storage systems.","Big Data;cryptography;data compression;database indexing;distributed databases;file organisation;genetic algorithms;redundancy;storage management;unimodal baseline Rabin CDC;AE CDC;FAST CDC;Hadoop Distributed File System;content defined chunking;chunk cut-points;bucket based index partitioning;computing operations;optimal threshold value;chunk-size variance;TTTD algorithm;unimodal Rabin CDC;GE based TTTD-P;redundant chunk;computation time;genetic optimized data deduplication;distributed Big Data storage systems;maximum redundancy detection;fingerprint lookup;genetic evolution algorithm based approach;optimized two thresholds-two divisors;single dynamic optimal parameter divisor;additional backup divisor;CDC performance bottleneck;chunking throughput;deduplication efficiency;asymmetric extremum CDC;deduplication ratio;Genetics;Fingerprint recognition;Signal processing algorithms;Algorithm design and analysis;Throughput;Redundancy;Gears;Big Data;Content Defined Chunking;Data Deduplication;TTTD;Genetic Algorithm;HDFS","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kumar et al. 2017 - Genetic optimized data deduplication for distributed big data storage systems.pdf","","","",""
"Journal article","Zhang Y,Feng D,Jiang H,Xia W,Fu M,Huang F,Zhou Y","","A Fast Asymmetric Extremum Content Defined Chunking Algorithm for Data Deduplication in Backup Storage Systems","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2017","66","2","199-211","All;Grouped by Publication/IEEE Transactions on Computers","","","","","","","","","2017-02","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2016.2595565;https://ieeexplore.ieee.org/abstract/document/7524782/;http://ranger.uta.edu/~jiang/publication/Journals/2017/2017-TC-An%20Fast%20Asymmetric%20Extremum%20Content%20Defined%20Chunking%20Algorithm%20for%20Data%20Deduplication%20in%20Backup%20Storage%20Systems.pdf","10.1109/TC.2016.2595565","","","","Chunk-level deduplication plays an important role in backup storage systems. Existing Content-Defined Chunking (CDC) algorithms, while robust in finding suitable chunk boundaries, face the key challenges of (1) low chunking throughput that renders the chunking stage a serious deduplication performance bottleneck, (2) large chunk size variance that decreases deduplication efficiency, and (3) being unable to find proper chunk boundaries in low-entropy strings and thus failing to deduplicate these strings. To address these challenges, this paper proposes a new CDC algorithm called the Asymmetric Extremum (AE) algorithm. The main idea behind AE is based on the observation that the extreme value in an asymmetric local range is not likely to be replaced by a new extreme value in dealing with the boundaries-shifting problem. As a result, AE has higher chunking throughput, smaller chunk size variance than the existing CDC algorithms, and is able to find proper chunk boundaries in low-entropy strings. The experimental results based on realworld datasets show that AE improves the throughput performance of the state-of-the-art CDC algorithms by more than 2.3×, which is fast enough to remove the chunking-throughput performance bottleneck of deduplication, and accelerates the system throughput by more than 50 percent, while achieving comparable deduplication efficiency.","storage management;fast asymmetric extremum content defined chunking algorithm;data deduplication;backup storage systems;chunk-level deduplication;CDC algorithms;asymmetric extremum algorithm;AE algorithm;boundaries-shifting problem;chunking-throughput performance;Throughput;Optimization;Power capacitors;Approximation algorithms;Redundancy;Robustness;Acceleration;Storage systems;data deduplication;content-defined chunking algorithm;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2017 - A Fast Asymmetric Extremum Content Defined Chunking Algorithm for Data Deduplication in Backup Storage Systems.pdf","","","",""
"Conference paper","Zhang Y,Xia W,Feng D,Jiang H,Hua Y,Wang Q","","Finesse: fine-grained feature locality based fast resemblance detection for post-deduplication delta compression","","","","","17th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 19)","","","2019","","","121-128","All;Grouped by Publication/FAST Papers;Thesis","Similarity/Resemblance;Chunking;Datasets","","","","","","","","2019","","","","","","","https://www.usenix.org/conference/fast19/presentation/zhang;https://www.usenix.org/system/files/fast19-zhang.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2019 - Finesse - fine-grained feature locality based fast resemblance detection for post-deduplication delta compression.pdf","","","",""
"Journal article","Xia W,Jiang H,Feng D,Tian L","","DARE: A Deduplication-Aware Resemblance Detection and Elimination Scheme for Data Reduction with Low Overheads","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2016","65","6","1692-1705","All;Grouped by Publication/IEEE Transactions on Computers;Thesis","Similarity/Resemblance;Chunking","","","","","","","","2016-06","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2015.2456015;https://ieeexplore.ieee.org/abstract/document/7155488/;https://pdfs.semanticscholar.org/d0d5/2f7d5afdaea531250d7408ab5934349d636a.pdf","10.1109/TC.2015.2456015","","","","Data reduction has become increasingly important in storage systems due to the explosive growth of digital data in the world that has ushered in the big data era. One of the main challenges facing large-scale data reduction is how to maximally detect and eliminate redundancy at very low overheads. In this paper, we present DARE, a low-overhead deduplication-aware resemblance detection and elimination scheme that effectively exploits existing duplicate-adjacency information for highly efficient resemblance detection in data deduplication based backup/archiving storage systems. The main idea behind DARE is to employ a scheme, call Duplicate-Adjacency based Resemblance Detection (DupAdj), by considering any two data chunks to be similar (i.e., candidates for delta compression) if their respective adjacent data chunks are duplicate in a deduplication system, and then further enhance the resemblance detection efficiency by an improved super-feature approach. Our experimental results based on real-world and synthetic backup datasets show that DARE only consumes about 1/4 and 1/2 respectively of the computation and indexing overheads required by the traditional super-feature approaches while detecting 2-10 percent more redundancy and achieving a higher throughput, by exploiting existing duplicate-adjacency information for resemblance detection and finding the “sweet spot” for the super-feature approach.","data handling;storage management;DARE scheme;deduplication-aware resemblance detection and elimination scheme;storage systems;large-scale data reduction;duplicate-adjacency information;data deduplication based backup-archiving storage systems;duplicate-adjacency based resemblance detection;DupAdj;data chunks;super-feature approach;Redundancy;Indexing;Feature extraction;Random access memory;Data structures;Throughput;Data deduplication;delta compression;storage system;index structure;performance evaluation;Data deduplication;delta compression;storage system;index structure;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2016 - DARE - A Deduplication-Aware Resemblance Detection and Elimination Scheme for Data Reduction with Low Overheads.pdf;All Papers/X/Xia et al. 2016 - DARE - A Deduplication-Aware Resemblance Detection and Elimination Scheme for Data Reduction with Low Overheads.pdf","","","",""
"Conference paper","Li J,Lee PP,Ren Y,Zhang X","","Metadedup: Deduplicating Metadata in Encrypted Deduplication via Indirection","","","","","2019 35th Symposium on Mass Storage Systems and Technologies (MSST)","","","2019","","","269-281","All;Superchunking Paper Sources/CDC/Chunking/Metadata;""Super-chunk"" term sources;SCAIL Bibliography;Grouped by Publication/MSST;PR-SCAIL;2023PhDReport;p-scailbib;Thesis;FSL Traces","Metadata;Two-stage","","","","","","","","2019-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2019.00007;https://ieeexplore.ieee.org/abstract/document/8890207/;http://www.cse.cuhk.edu.hk/~pclee/www/pubs/msst19dedup.pdf","10.1109/MSST.2019.00007","","","","Encrypted deduplication combines encryption and deduplication in a seamless way to provide confidentiality guarantees for the physical data in deduplication storage, yet it incurs substantial metadata storage overhead due to the additional storage of keys. We present a new encrypted deduplication storage system called Metadedup, which suppresses metadata storage by also applying deduplication to metadata. Its idea builds on indirection, which adds another level of metadata chunks that record metadata information. We find that metadata chunks are highly redundant in real-world workloads and hence can be effectively deduplicated. In addition, metadata chunks can be protected under the same encrypted deduplication framework, thereby providing confidentiality guarantees for metadata as well. We evaluate Metadedup through microbenchmarks, prototype experiments, and trace-driven simulation. Metadedup has limited computational overhead in metadata processing, and only adds 6.19% of performance overhead on average when storing files in a networked setting. Also, for real-world backup workloads, Metadedup saves the metadata storage by up to 97.46% at the expense of only up to 1.07% of indexing overhead for metadata chunks.","cloud computing;cryptography;file organisation;meta data;confidentiality guarantees;Metadedup;metadata processing;metadata chunks;encrypted deduplication storage system;metadata information;metadata storage overhead;Metadata;Maximum likelihood estimation;Encryption;Indexes;Mathematical model;Prototypes;metadata management;encrypted deduplication;cloud storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2019 - Metadedup - Deduplicating Metadata in Encrypted Deduplication via Indirection.pdf;All Papers/L/Li et al. 2019 - Metadedup - Deduplicating Metadata in Encrypted Deduplication via Indirection.pdf","","","",""
"Journal article","Wang L,Dong X,Zhang X,Guo F,Wang Y,Gong W","","A Logistic Based Mathematical Model to Optimize Duplicate Elimination Ratio in Content Defined Chunking Based Big Data Storage System","Symmetry","Symmetry","","","","","","2016","8","7","69","Superchunking Paper Sources/CDC/Chunking/Metadata;Superchunking Paper Sources/CDC/Chunking;All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","","","","Multidisciplinary Digital Publishing Institute","","","","","2016-07-21","","2020-02-27","","","","","https://www.mdpi.com/2073-8994/8/7/69;http://dx.doi.org/10.3390/sym8070069;https://www.mdpi.com/2073-8994/8/7/69/pdf","10.3390/sym8070069","","","","Deduplication is an efficient data reduction technique, and it is used to mitigate the problem of huge data volume in big data storage systems. Content defined chunking (CDC) is the most widely used algorithm in deduplication systems. The expected chunk size is an important parameter of CDC, and it influences the duplicate elimination ratio (DER) significantly. We collected two realistic datasets to perform an experiment. The experimental results showed that the current approach of setting the expected chunk size to 4 KB or 8 KB empirically cannot optimize DER. Therefore, we present a logistic based mathematical model to reveal the hidden relationship between the expected chunk size and the DER. This model provides a theoretical basis for optimizing DER by setting the expected chunk size reasonably. We used the collected datasets to verify this model. The experimental results showed that the R2 values, which describe the goodness of fit, are above 0.9, validating the correctness of this mathematic model. Based on the DER model, we discussed how to make DER close to the optimum by setting the expected chunk size reasonably.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2016 - A Logistic Based Mathematical Model to Optimize Dupli ... ation Ratio in Content Defined Chunking Based Big Data Storage System.pdf","","","",""
"Patent","","","Method and apparatus to minimize metadata in de-duplication","","","","","","","","2014","","","","Superchunking Paper Sources/CDC/Chunking/Metadata;All;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","","","","","","","","","2014-02-04","","2020-02-27","","","","","https://patentimages.storage.googleapis.com/fc/ce/e3/cf683e0ea96453/US8645333.pdf;https://patents.google.com/patent/US8645333B2/en","","","","","The invention provides a method for reducing identification of chunk portions in data de-duplication. The method includes detecting sequences of stored identification of chunk portions of at least one data object, indexing the detected stored identification of chunk portions based on a sequence type, encoding first repeated sequences of the stored identifications with a first encoding, encoding second repeated sequences of the stored identifications with a second encoding, and avoiding repeated stored identifications of chunk portions.","","&nbsp;In one embodiment de
duplication module 140 reduces an index of identifiers for
chunk portions in de-duplication where the identifiers are
metadata hashes of objects. The chunking module 141 is
configured to create Smaller chunk portions from chunks
received from data chunker module 130. In another embodi
ment, the chunking module 141 performs chunking of an
input stream of larger chunks by one or more of fixed size
chunking, sliding window chunking, variable size chunking
and content dependent chunking, in order to reduce the input
stream of chunk portions to Smaller chunk portions.&nbsp;","","","","","","","","","","","","","","Balachandran S,Constantinescu MC","International Business Machines Corp","USPTO","8645333","2008-05-29","US Patent","United States Patent and Trademark Office","US:12/129432","","All Papers/B/Balachandran and Constantinescu 2014 - Method and apparatus to minimize metadata in de-duplication.pdf","","","",""
"Journal article","Meister D,Brinkmann A,Süß T","","File recipe compression in data deduplication systems","of the 11th {USENIX} Conference on File …","","","","","","","2013","","","","Superchunking Paper Sources/CDC/Chunking/Metadata;All;Grouped by Publication/FAST Papers;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Metadata;Annotated","","","usenix.org","","","","","2013","","","","","","","https://www.usenix.org/conference/fast13/technical-sessions/presentation/meister;https://www.usenix.org/system/files/conference/fast13/fast13-final54.pdf","","","","","Data deduplication systems discover and exploit redundancies between different data blocks. The most common approach divides data into chunks and identiﬁes redundancies via ﬁngerprints. The ﬁle content can be rebuilt by combining the chunk ﬁngerprints which a restored sequentially in a ﬁle recipe. The corresponding ﬁle recipe data can occupy a signiﬁcant fraction of the total disk space, especially if the deduplication ratio is very high. We propose a combination of efﬁcient and scalable compression schemes to shrink the ﬁle …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Meister et al. 2013 - File recipe compression in data deduplication systems.pdf","","","",""
"Journal article","Zhou B,Wen JT","","A Data Deduplication Framework of Disk Images with Adaptive Block Skipping","J. Comput. Sci. Technol.","Journal of computer science and technology","","","","","","2016","31","4","820-835","All;Superchunking Paper Sources/CDC/Chunking/Metadata","Chunking;Metadata","","","","","","","","2016-07-01","","","","","1000-9000","1860-4749","https://doi.org/10.1007/s11390-016-1665-z;http://dx.doi.org/10.1007/s11390-016-1665-z;https://link.springer.com/article/10.1007/s11390-016-1665-z;http://search.proquest.com/openview/ef3affe620c88c6cd36ae6e8e80a43c3/1.pdf?pq-origsite=gscholar&cbl=326258","10.1007/s11390-016-1665-z","","","","We describe an efficient and easily applicable data deduplication framework with heuristic prediction based adaptive block skipping for the real-world dataset such as disk images to save deduplication related overheads and improve deduplication throughput with good deduplication efficiency maintained. Under the framework, deduplication operations are skipped for data chunks determined as likely non-duplicates via heuristic prediction, in conjunction with a hit and matching extension process for duplication identification within skipped blocks and a hysteresis mechanism based hash indexing process to update the hash indices for the re-encountered skipped chunks. For performance evaluation, the proposed framework was integrated and implemented in the existing data domain and sparse indexing deduplication algorithms. The experimental results based on a real-world dataset of 1.0 TB disk images showed that the deduplication related overheads were significantly reduced with adaptive block skipping, leading to a 30%~80% improvement in deduplication throughput when deduplication metadata were stored on the disk for data domain, and 25%~40% RAM space saving with a 15%~20% improvement in deduplication throughput when an in-RAM sparse index was used in sparse indexing. In both cases, the corresponding deduplication ratios reduced were below 5%.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou and Wen 2016 - A Data Deduplication Framework of Disk Images with Adaptive Block Skipping.pdf","","","",""
"Conference paper","Wu H,Wang C,Lu K,Fu Y,Zhu L","","One Size Does Not Fit All: The Case for Chunking Configuration in Backup Deduplication","","","","","2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)","","","2018","","","213-222","All;Superchunking Paper Sources/CDC/Chunking/Metadata","Chunking;Metadata","","","","","","","","2018-05","","","","","","","http://dx.doi.org/10.1109/CCGRID.2018.00036;https://ieeexplore.ieee.org/abstract/document/8411025/;https://www.researchgate.net/profile/Huijun_Wu8/publication/326146596_One_Size_Does_Not_Fit_All_The_Case_for_Chunking_Configuration_in_Backup_Deduplication/links/5b3b2875aca27207850555af/One-Size-Does-Not-Fit-All-The-Case-for-Chunking-Configuration-in-Backup-Deduplication.pdf","10.1109/CCGRID.2018.00036","","","","Data backup is regularly required by both enterprise and individual users to protect their data from unexpected loss. There are also various commercial data deduplication systems or software that help users to eliminate duplicates in their backup data to save storage space. In data deduplication systems, the data chunking process splits data into small chunks. Duplicate data is identified by comparing the fingerprints of the chunks. The chunk size setting has significant impact on deduplication performance. A variety of chunking algorithms have been proposed in recent studies. In practice, existing systems often set the chunking configuration in an empirical manner. A chunk size of 4KB or 8KB is regarded as the sweet spot for good deduplication performance. However, the data storage and access patterns of users vary and change along time, as a result, the empirical chunk size setting may not lead to a good deduplication ratio and sometimes results in difficulties of storage capacity planning. Moreover, it is difficult to make changes to the chunking settings once they are put into use as duplicates in data with different chunk size settings cannot be eliminated directly. In this paper, we propose a sampling-based chunking method and develop a tool named SmartChunker to estimate the optimal chunking configuration for deduplication systems. Our evaluations on real-world datasets demonstrate the efficacy and efficiency of SmartChunker.","storage management;sampling-based chunking method;backup deduplication;data backup;commercial data deduplication systems;storage space;data chunking process;duplicate data;chunking algorithms;data storage;empirical chunk size setting;storage capacity planning;deduplication ratio;chunking configuration;chunk size settings;SmartChunker tool;memory size 4.0 KByte;memory size 8.0 KByte;Metadata;Tools;Linear programming;Software;Large-scale systems;Estimation error;Backup Deduplication;Data Chunking;Deduplication Ratio Estimation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wu et al. 2018 - One Size Does Not Fit All - The Case for Chunking Configuration in Backup Deduplication.pdf","","","",""
"Journal article","Broder A","","Methods in Communications","Security, and Computer Science","","","","","","","1993","143","","152","All","Data Structures","","","","","","","","1993","","","","","","","","","","","","","","Reference in Rabin for calculating parameters.","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Report","Spiridonov A,Thaker S,Patwardhan S","","Sharing and bandwidth consumption in the low bandwidth file system","","","","","","","","2005","","","","All;Thesis","Chunking","Tech. rep., Department of Computer Science, University of Texas at Austin","","","","","","","2005","","","","","","","https://pdfs.semanticscholar.org/4332/74005c747fe2141c1a2f8e819e4f6d4455f5.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Spiridonov et al. 2005 - Sharing and bandwidth consumption in the low bandwidth file system.pdf","","","",""
"Conference paper","Romański B,Heldt Ł,Kilian W,Lichota K,Dubnicki C","","Anchor-driven subchunk deduplication","","","","","Proceedings of the 4th Annual International Conference on Systems and Storage","","","2011","","","1-13","acm_1987816.1987837.bib;All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Thesis","Chunking","","Article 16","Association for Computing Machinery","New York, NY, USA","the 4th Annual International Conference","Haifa, Israel","30/05/2011-01/06/2011","2011-05-30","","","9781450307734","","","","https://doi.org/10.1145/1987816.1987837;http://dx.doi.org/10.1145/1987816.1987837;http://portal.acm.org/citation.cfm?doid=1987816.1987837;http://dl.acm.org/ft_gateway.cfm?id=1987837&amp;ftid=980040&amp;dwn=1","10.1145/1987816.1987837","","","","Data deduplication, implemented usually with content defined chunking (CDC), is today one of key features of advanced storage systems providing space for backup applications. Although simple and effective, CDC generates chunks with sizes clustered around expected chunk size, which is globally fixed for a given storage system and applies to all backups. This creates opportunity for improvement, as the optimal chunk size for deduplication varies not only among backup datasets, but also within one dataset: long stretches of unchanged data favor larger chunks, whereas regions of change prefer smaller ones.In this work, we present a new algorithm which deduplicates with big chunks as well as with their subchunks using a deduplication context containing subchunk-to-container-chunk mappings. When writing data, this context is constructed on-the fly with so-called anchor sequences defined as short sequences of adjacent chunks in a backup stream (a stream of data produced by backup application containing backed up files). For each anchor sequence, we generate an anchor -- a special block with set of mappings covering a contiguous region of the backup stream positioned ahead of this anchor sequence. If anchor sequences have not changed between backups, the mappings created with the previous backup are prefetched and added to the deduplication context. It is of limited size and fits in the main memory unlike solutions which require keeping all subchunk mappings for the entire backup stream. At the same time, the context provides most of mappings needed for subchunk deduplication. Compared to CDC, the new algorithm results in up to 25% dedup ratio improvement achieved with almost 3 times larger average block size, as verified by simulations driven by real backup traces.","deduplication, chunking, CDC, CAS, backup","","","","","","SYSTOR '11","16","","","","","","","","","","","","","","","","","All Papers/R/Romański et al. 2011 - Anchor-driven subchunk deduplication.pdf","","","",""
"Website","","","Mersenne twister","","","","","","","","","","","","All","Datasets;Encryption Techniques","","","","","","","","","","2020-02-07","","","","","https://dl.acm.org/doi/pdf/10.1145/272991.272995?casa_token=rqxb50G0iDkAAAAA:MIGAoNe_iHPk68ZSksrEGyVd0plXP9bVn3DtStJ-noPhw-UsUiiIQND-ALnGX7f-_fSpt3BdbWtsew","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/Mersenne twister - Mersenne twister.pdf","","","",""
"Conference paper","Chapuis B,Garbinato B,Andritsos P","","Throughput: A Key Performance Measure of Content-Defined Chunking Algorithms","","","","","2016 IEEE 36th International Conference on Distributed Computing Systems Workshops (ICDCSW)","","","2016","","","7-12","All","Chunking;Metric/Measure Deduplication","","","","","","","","2016-06","","","","","2332-5666","","http://dx.doi.org/10.1109/ICDCSW.2016.32;https://ieeexplore.ieee.org/abstract/document/7756201/;https://ieeexplore.ieee.org/iel7/7756174/7756178/07756201.pdf","10.1109/ICDCSW.2016.32","","","","Data deduplication techniques are often used by cloud storage systems to reduce network bandwidth and storage requirements. As a consequence, the current research literature tends to focus most of its algorithmic efforts on improving the Duplicate Elimination Ratio (DER), which reflects the compression achieved using a given algorithm. Yet, the importance of this indicator tends to be overestimated, while another key indicator, namely throughput, tends to be underestimated. To substantiate this claim, we reimplement a selection of popular Content-Defined Chunking algorithms (CDC) and perform a detailed performance analysis. On this basis, we show that the gain brought by algorithms that are aggressively focusing on DER often come at a significant cost in terms of throughput. As a consequence, we advocate for future optimizations taking throughput into account and for making balanced tradeoffs between DER and throughput.","cloud computing;file organisation;optimisation;content-defined chunking algorithms;data deduplication techniques;cloud storage systems;network bandwidth;duplicate elimination ratio;DER;CDC algorithms;Throughput;Density estimation robust algorithm;Optimization;Algorithm design and analysis;Redundancy;Cloud computing;Measurement;content-defined chunking;duplicate elimination ratio;rolling hash function;performance;throughput","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chapuis et al. 2016 - Throughput - A Key Performance Measure of Content-Defined Chunking Algorithms.pdf","","","",""
"Journal article","Tan Y,Yan Z","","Multi-Objective Metrics to Evaluate Deduplication Approaches","IEEE Access","","","","","","","2017","5","","5366-5377","All","Metric/Measure Deduplication;Chunking","","","","","","","","2017","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2017.2694014;https://ieeexplore.ieee.org/abstract/document/7907281/;https://ieeexplore.ieee.org/iel7/6287639/6514899/07907281.pdf","10.1109/ACCESS.2017.2694014","","","","Data deduplication is a lossless compression technology that has been widely used in storage systems for space optimization. However, due to the removal of redundant data, the data deduplication has negative influences on data writing, data reading, and data reliability. In this paper, we propose a multi-objective-based performance evaluation framework to analyze the data deduplication performances and evaluate existing well-known deduplication approaches with multiple performance objectives, including compression ratio, data read performance, data write performance, and data reliability. Based on the proposed multi-objective framework and the obtained evaluation results, we further propose a multi-objective-based optimization method. Though our extensive experimental evaluation driven by real-world data sets, it is shown that this method can improve data read/write performance and data reliability while at the cost of little compression ratio.","data compression;optimisation;performance evaluation;reliability;multiobjective metrics;data deduplication approach;lossless compression technology;storage systems;space optimization;data writing;data reading;data reliability;multiobjective-based performance evaluation framework;compression ratio;data read performance;data write performance;multiobjective-based optimization method;Writing;Layout;Measurement;Optimization methods;Redundancy;Throughput;Data deduplication;multi-objective based optimization;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan and Yan 2017 - Multi-Objective Metrics to Evaluate Deduplication Approaches.pdf","","","",""
"Journal article","Akter M,Gani A,Rahman MO,Hassan MM,Almogren A,Ahmad S","","Performance Analysis of Personal Cloud Storage Services for Mobile Multimedia Health Record Management","IEEE Access","","","","","","","2018","6","","52625-52638","All","Chunking","","","","","","","","2018","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2018.2869848;https://ieeexplore.ieee.org/abstract/document/8464048/;https://ieeexplore.ieee.org/iel7/6287639/6514899/08464048.pdf","10.1109/ACCESS.2018.2869848","","","","Recently, the trend of mobile multimedia services and applications being used for e-health is growing in popularity. This is because people can get access to their electronic personal health records (PHRs), such as medical history, lab reports from an X-ray, MRI, clinical audio-visual notes, EEG/ECG data, and insurance policy details from anywhere, at any time, from their mobile or handheld devices. In this scenario, a medical care provider or a patient is responsible for uploading and managing the patient's health information via cloud storage services. There are a number of personal cloud storage services that could be used such as Dropbox, Google Drive, OneDrive, and Box. However, the different designs of these personal cloud storage services mean there are differences in their performance in terms of storing and managing PHRs. In this paper, we present the details of our study on the performance of personal cloud storage services, and we highlight the strengths and weaknesses of such services in terms of PHR management. We investigate the performance of personal cloud storage services by conducting a qualitative and quantitative analysis of them. The qualitative analysis highlights strengths and weaknesses in terms of supported capabilities/features and shortcomings in terms of potential features that have not been implemented. The capabilities we analyze are chunking, bundling, de-duplication, delta-encoding, and data compression. In the quantitative analysis, we investigate performance in terms of control data overhead, impact of data size on number of packets as well as transmission rate, synchronization initialization time, and protocol overhead. During testing with diverse benchmark size on distinct cloud storage services, we attained an average transmission of 93%, 3%, and 4% for application data, control data, and other data, respectively. This research allows us to identify open issues and to determine future directions for developing an efficient personal cloud storage service.","cloud computing;data analysis;electronic health records;health care;mobile computing;multimedia computing;software performance evaluation;storage management;mobile multimedia health record management;mobile multimedia services;electronic personal health records;personal cloud storage services;performance analysis;Cloud computing;Synchronization;Google;Software;Statistical analysis;Protocols;Security;Mobile multimedia health record;personal cloud storage;performance comparison;Dropbox;Google drive","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Akter et al. 2018 - Performance Analysis of Personal Cloud Storage Services for Mobile Multimedia Health Record Management.pdf","","","",""
"Conference paper","Zhang J","","A Data Synchronization Method Oriented to Custom Hierarchical Multi-node System","","","","","2015 IEEE International Conference on Computational Intelligence Communication Technology","","","2015","","","666-669","All","Chunking","","","","","","","","2015-02","","","","","","","http://dx.doi.org/10.1109/CICT.2015.152;https://ieeexplore.ieee.org/abstract/document/7078787/;https://ieeexplore.ieee.org/iel7/7076746/7078645/07078787.pdf","10.1109/CICT.2015.152","","","","In the modern enterprise system, data synchronization programs need to support data cloud storage to ensure data consistency, integrity, and security between two devices. The paper addresses to some observation network system as a hierarchical multi-node system. In its application development process, the synchronization functions in existing database management system cannot meet its system requirements. A kind of data synchronization method oriented to custom hierarchical multi-node system was brought out. The program floe and synchronization task queue structure to implement the method were also given.","business data processing;cloud computing;data integrity;database management systems;storage management;synchronisation;data synchronization method;custom hierarchical multinode system;enterprise system;data cloud storage;data consistency;data integrity;data security;observation network system;application development process;database management system;program floe;synchronization task queue structure;Synchronization;Database systems;Monitoring;Computers;Software;Distributed databases;multi-node system;data synchronization;synchronization task;task queue","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang 2015 - A Data Synchronization Method Oriented to Custom Hierarchical Multi-node System.pdf","","","",""
"Thesis","Constantinou M","","Tuning of rsync algorithm for optimum cloud storage performance","","","","","","","","2013","","","","All","Chunking","","","Department of Computer Science, University of Bath","","","","","2013","","","","","","","https://researchportal.bath.ac.uk/en/publications/tuning-of-rsync-algorithm-for-optimum-cloud-storage-performance;https://purehost.bath.ac.uk/ws/files/23961224/MC380template.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Constantinou 2013 - Tuning of rsync algorithm for optimum cloud storage performance.pdf","","","",""
"Conference paper","Tian W,Li R,Xu Z,Xiao W","","Does the content defined chunking really solve the local boundary shift problem?","","","","","2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)","","","2017","","","1-8","All","Chunking","","","","","","","","2017-12","","","","","2374-9628","","http://dx.doi.org/10.1109/PCCC.2017.8280445;https://ieeexplore.ieee.org/abstract/document/8280445/","10.1109/PCCC.2017.8280445","","","","Data chunking is one of the most important issues in a deduplication system, which not only determines the effectiveness of deduplication such as deduplication ratio, but also impacts the modification overhead. It breaks the file into chunks to find out the redundancy by fingerprint comparisons. The content-defined chunking algorithms such as TTTD, BSW CDC, and RC, can resist the boundary shift problem caused by small modifications. However, we observe that there exist a lot of consecutive maximum chunk sequences in various benchmarks. These consecutive maximum chunk sequences will lead to local boundary shift problem when facing small modifications. Based on this observation, we propose a new chunking algorithm, Elastic Chunking. By leveraging dynamic adjustment policy, elastic chunk can quickly find the boundary to remove the consecutive maximum chunk sequences. To evaluate the performance, we implement a prototype and conduct extensive experiments based on synthetic and realistic datasets. Compared with TTTD, BSW CDC and RC algorithms, proposed chunking algorithm can achieve the higher deduplication ratio and throughput.","computer networks;data handling;storage management;local boundary shift problem;data chunking;deduplication system;BSW CDC;deduplication ratio;maximum chunk sequences;elastic chunking algorithm;Algorithm design and analysis;Heuristic algorithms;Resists;Redundancy;Throughput;Probability distribution;Computer science;Deduplication;Chunking Algorithm;Boundary Shift;Content-Defined","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tian et al. 2017 - Does the content defined chunking really solve the local boundary shift problem.pdf","","","",""
"Conference paper","Graupner H,Torkura KA,Sukmana MI,Meinel C","","Secure Deduplication on Public Cloud Storage","","","","","Proceedings of the 2019 4th International Conference on Big Data and Computing - ICBDC 2019","","","2019","","","34-41","All","Versioning","","","ACM Press","New York, New York, USA","the 2019 4th International Conference","Guangzhou, China","10/05/2019-12/05/2019","2019","","","9781450362788","","","","http://dl.acm.org/citation.cfm?doid=3335484.3335502;http://dl.acm.org/ft_gateway.cfm?id=3335502&ftid=2070056&dwn=1;http://dx.doi.org/10.1145/3335484.3335502;https://dl.acm.org/doi/10.1145/3335484.3335502","10.1145/3335484.3335502","","","","Public cloud storage is an important resource of modern computing. One important aspect is the potential to improve economic efficiency. It can be drastically increased by leveraging deduplication techniques. The authors of this paper aim to minimize the risk of data confidentiality violation while maximizing storage utilization. This secure and efficient deduplication concept combines the benefits of context-sensitive partitioning, convergent encryption and remote update protocols. Those technologies are enhanced by a partition-based sensitivity detection algorithm and the handling of sensitive partitions. Thus, incremental updating of stored files is possible. The scheme features an information concealment algorithm to prevent information retrieval attacks. In addition, our solution prevents gaining of illegitimate access to sensitive data due protection by the proof-of-data-ownership scheme Asymmetric Convergent Encryption (ACE). This holistic concept overcomes the weaknesses of deduplicating storage systems. The results of this paper can be leveraged to enhance not only public cloud storage but also other remote storage systems.","","","","","en","","","","","","","","","","","","","","","","","","","","All Papers/G/Graupner et al. 2019 - Secure Deduplication on Public Cloud Storage.pdf","","","",""
"Conference paper","Almeida JB,Barbosa M,Barthe G,Campagna M,Cohen E,Gregoire B,Pereira V,Portela B,Strub PY,Tasiran S","","A machine-checked proof of security for AWS key management service","","","","","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","","","2019","","","63-78","All","Security Proof","","","","","","","","2019","","","","","","","https://dl.acm.org/doi/abs/10.1145/3319535.3354228?casa_token=kUp66JzIPsoAAAAA:R9przArjhKX_Vg5K-lau6O7Gq14OL0GlE3Drd3HiZvJOuGVYqF_DaKc_ESHPsTA-7JmbP_D4N74NLw;https://dl.acm.org/doi/pdf/10.1145/3319535.3354228?casa_token=uJq6JIJi3qYAAAAA:uBBt0ALpgSJ_LqyVaRn3XiyBEFtVIYIRt9FYWP9SgCd__A2pisMf8-_jBh0HWobURUwhcUB32Gu2Kg","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Hoffmann M,Klooß M,Rupp A","","Efficient zero-knowledge arguments in the discrete log setting, revisited","","","","","Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security","","","2019","","","2093-2110","All","Sec Techniques/Protocols","","","","","","","","2019","","","","","","","https://dl.acm.org/doi/abs/10.1145/3319535.3354251?casa_token=LIpkPAHnrUYAAAAA:1z-eeGA-bSJRFthMvzHbEFYFdlJV0Pumr5IOPcfsxtadr6C4OtV5ifjzg57a5gZmYnTr2b0LwciL5w;https://dl.acm.org/doi/pdf/10.1145/3319535.3354251?casa_token=_j7Fq0EzlxwAAAAA:p20mfosZyxZ3oOIYIxZqVjrhsHNWR3xo1nyhy8oJBnnBkTGe8p1vP_6URq01_qaeN345One9m7IdJQ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Pooranian Z,Chen K,Yu C,Conti M","","RARE: Defeating side channels based on data-deduplication in cloud storage","","","","","IEEE INFOCOM 2018 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)","","","2018","","","444-449","All;Thesis","Side Channels;Maintaining Privacy;CE/MLE","","","","","","","","2018-04","","","","","","","http://dx.doi.org/10.1109/INFCOMW.2018.8406888;https://ieeexplore.ieee.org/abstract/document/8406888/;https://ieeexplore.ieee.org/iel7/8401302/8406774/08406888.pdf;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8406888;https://ieeexplore.ieee.org/ielx7/8401302/8406774/08406888.pdf?tp=&arnumber=8406888&isnumber=8406774&ref=","10.1109/INFCOMW.2018.8406888","","","","Client-side data deduplication enables cloud storage services (e.g., Dropbox) to achieve both storage and bandwidth savings, resulting in reduced operating cost and high level of user satisfaction. However, the deduplication checks (i.e., the corresponding essential message exchange) create a side channel, exposing the privacy of file existence status to the attacker. In particular, the binary response from the deduplication check reveals the information about the existence of a copy of the file in the cloud storage. This behavior can be exploited to launch further attacks such as learning the sensitive file content and establishing a covert channel. While current solutions provide only weaker privacy or rely on unreasonable assumptions, we propose RAndom REsponse (RARE) approach to achieve stronger privacy. The idea behind our proposed RARE solution is that the uploading user sends the deduplication request for two chunks at once. The cloud receiving the deduplication request returns the randomized deduplication response with the careful design so as to preserve the deduplication gain and at the same time minimize the privacy leakage. Our analytical results confirm privacy guarantee and results show that both deduplication benefit and privacy of RARE can be preserved.","cloud computing;data privacy;file organisation;storage management;client-side data deduplication;cloud storage services;bandwidth savings;user satisfaction;deduplication check;file existence status;sensitive file content;covert channel;RARE solution;deduplication request;randomized deduplication response;deduplication gain;privacy leakage;privacy guarantee;deduplication benefit;message exchange;random response approach;Cloud computing;Privacy;Conferences;Servers;Protocols;Bandwidth;Logic gates;Cloud Storage;Data Deduplication;Data Privacy;Side Channel","Client always requests dedup for pairs of fixed length chunks, may need to pad partial chunks to fixed length. If neither chunk on server, server says send both. If either or both on server, server says either send both, or send XOR of chunks, your choice. If it only had one chunk, it is able to derive, and the amount of data set to server is optimal. If both chunks on server, 1/2 of the time it will send up only one. Not optimal, but an improvement.","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Pooranian et al. 2018 - RARE - Defeating side channels based on data-deduplication in cloud storage.pdf","","","",""
"Journal article","Dave J","","Multi- Level Data Security Model for Big Data on Public Cloud: A New Model","Int. J. Advanced Networking and Applications","","","","","","","2018","9","6","3654-3657","All","Steganography","","","","","","","","5/2018","","","","","","","http://oaji.net/articles/2017/2698-1528117989.pdf","","","","","With the advent of cloud computing the big data has emerged as a very crucial technology. The certain type of cloud provides the consumers with the free services like storage, computational power etc. This paper is intended to make use of infrastructure as a service where the storage service from the public cloud providers is going to leveraged by an individual or organization. The paper will emphasize the model which can be used by anyone without any cost. They can store the confidential data without any type of security issue, as the data will be altered in such a way that it cannot be understood by the intruder if any. Not only that but the user can retrieve back the original data within no time. The proposed security model is going to effectively and efficiently provide a robust security while data is on cloud infrastructure as well as when data is getting migrated towards cloud infrastructure or vice versa.","","Use Steganography to store your data encrypted in free photo storage services.","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dave 2018 - Multi- Level Data Security Model for Big Data on Public Cloud - A New Model.pdf","","","",""
"Miscellaneous","Lee G,Ko H,Pack S","","An Efficient Delta Synchronization Algorithm for Mobile Cloud Storage Applications","IEEE Transactions on Services Computing","","","","","","","2017","10","3","341-351","All","Synchronization","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1109/tsc.2015.2498168","10.1109/tsc.2015.2498168","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lee et al. 2017 - An Efficient Delta Synchronization Algorithm for Mobile Cloud Storage Applications.pdf","","","",""
"Miscellaneous","Li Z,Dai Y,Chen G,Liu Y","","Efficient Batched Synchronization for Cloud Storage Services","Content Distribution for Mobile Internet: A Cloud-based Approach","","","","","","","2016","","","197-223","All","Synchronization","","","","","","","","2016","","","","","","","http://dx.doi.org/10.1007/978-981-10-1463-5_9","10.1007/978-981-10-1463-5_9","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2016 - Efficient Batched Synchronization for Cloud Storage Services.pdf","","","",""
"Miscellaneous","Li S,Zhang Q,Yang Z,Dai Y","","Understanding and Surpassing Dropbox: Efficient Incremental Synchronization in Cloud Storage Services","2015 IEEE Global Communications Conference (GLOBECOM)","","","","","","","2014","","","","All","Synchronization","","","","","","","","2014","","","","","","","http://dx.doi.org/10.1109/glocom.2014.7417235","10.1109/glocom.2014.7417235","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Understanding and Surpassing Dropbox - Efficient Incremental Synchronization in Cloud Storage Services.pdf","","","",""
"Conference paper","Li Z,Wilson C,Jiang Z,Liu Y,Zhao BY,Jin C,Zhang ZL,Dai Y","","Efficient Batched Synchronization in Dropbox-Like Cloud Storage Services","","","","","Middleware 2013","","","2013","","","307-327","All","Synchronization","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-45065-5_16;https://link.springer.com/chapter/10.1007/978-3-642-45065-5_16;https://link.springer.com/content/pdf/10.1007/978-3-642-45065-5_16.pdf","10.1007/978-3-642-45065-5_16","","","","As tools for personal storage, file synchronization and data sharing, cloud storage services such as Dropbox have quickly gained popularity. These services provide users with ubiquitous, reliable data storage that can be automatically synced across multiple devices, and also shared among a group of users. To minimize the network overhead, cloud storage services employ binary diff, data compression, and other mechanisms when transferring updates among users. However, despite these optimizations, we observe that in the presence of frequent, short updates to user data, the network traffic generated by cloud storage services often exhibits pathological inefficiencies. Through comprehensive measurements and detailed analysis, we demonstrate that many cloud storage applications generate session maintenance traffic that far exceeds the useful update traffic. We refer to this behavior as the traffic overuse problem. To address this problem, we propose the update-batched delayed synchronization (UDS) mechanism. Acting as a middleware between the user’s file storage system and a cloud storage application, UDS batches updates from clients to significantly reduce the overhead caused by session maintenance traffic, while preserving the rapid file synchronization that users expect from cloud storage services. Furthermore, we extend UDS with a backwards compatible Linux kernel modification that further improves the performance of cloud storage applications by reducing the CPU usage.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2013 - Efficient Batched Synchronization in Dropbox-Like Cloud Storage Services.pdf","","","",""
"Conference paper","Aman MA,Cetinkaya EK","","Towards Cloud Security Improvement with Encryption Intensity Selection","","","","","DRCN 2017-Design of Reliable Communication Networks; 13th International Conference","","","2017","","","1-7","All","NoCE","VDE","","","","","","","2017","","","","","","","https://ieeexplore.ieee.org/abstract/document/7993435/;https://pdfs.semanticscholar.org/9b31/bb5414f864ff05573654879716f9132d3086.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Aman and Cetinkaya 2017 - Towards Cloud Security Improvement with Encryption Intensity Selection.pdf","","","",""
"Conference paper","Sridharan J,Valliyammai C,Karthika RN,Nihil Kulasekaran L","","Data De-duplication Using Cuckoo Hashing in Cloud Storage","","","","","Soft Computing in Data Analytics","","","2019","","","707-715","All","Fingerprint-Indexing","","","Springer Singapore","","","","","2019","","","","","","","http://dx.doi.org/10.1007/978-981-13-0514-6_67;https://link.springer.com/chapter/10.1007/978-981-13-0514-6_67;https://www.researchgate.net/profile/Karthika_Rn/publication/327161977_Data_De-duplication_Using_Cuckoo_Hashing_in_Cloud_Storage/links/5c9c763692851cf0ae9c8744/Data-De-duplication-Using-Cuckoo-Hashing-in-Cloud-Storage.pdf","10.1007/978-981-13-0514-6_67","","","","Cloud computing facilitates on-demand and ubiquitous access to a centralized pool of resources such as applications, networks, and storage services. Redundant copies of the same data are stored in multiple places, thus occupying more space in servers. Recent increase in computing leads to enormous volume of data, which are backed up and stored in cloud and made available to address consent, real-time insights and in regulating the data. In order to address the above problem, an enhanced cuckoo hashing algorithm is proposed for identifying duplicate data. Cuckoo hashing performs insertion, deletion, and retrieval in constant time. The metadata of files such as the basic file attributes, user-defined attributes, and user principal owner attributes is preserved after de-duplication. The experimental results show the efficiency of cuckoo hashing in the de-duplication of data chunks in the cloud environment.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sridharan et al. 2019 - Data De-duplication Using Cuckoo Hashing in Cloud Storage.pdf","","","",""
"Conference paper","Debnath B,Sengupta S,Li J","","SkimpyStash: RAM space skimpy key-value store on flash-based storage","","","","","Proceedings of the 2011 ACM SIGMOD International Conference on Management of data","","","2011","","","25-36","Superchunking Paper Sources/CDC/Fingerprinting/Indexing;All","Fingerprint-Indexing","","","","","","","","2011","","","","","","","https://dl.acm.org/doi/abs/10.1145/1989323.1989327;https://dl.acm.org/doi/pdf/10.1145/1989323.1989327","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Debnath et al. 2011 - SkimpyStash - RAM space skimpy key-value store on flash-based storage.pdf","","","",""
"Conference paper","Pagh R,Rodler FF","","Cuckoo Hashing","","","","","Algorithms — ESA 2001","","","2001","","","121-133","All","Data Structures","","","Springer Berlin Heidelberg","","","","","2001","","","","","","","http://dx.doi.org/10.1007/3-540-44676-1_10;https://link.springer.com/chapter/10.1007/3-540-44676-1_10;http://www.di.ens.fr/~vergnaud/algo0910/Cuckoo.pdf","10.1007/3-540-44676-1_10","","","","We present a simple and efficient dictionary with worst case constant lookup time, equaling the theoretical performance of the classic dynamic perfect hashing scheme of Dietzfelbinger et al. The space usage is similar to that of binary search trees, i.e., three words per key on average. The practicality of the scheme is backed by extensive experiments and comparisons with known methods, showing it to be quite competitive also in the average case.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Pagh and Rodler 2001 - Cuckoo Hashing.pdf","","","",""
"Conference paper","Kim C,Park K,Park K,Park KH","","Rethinking deduplication in cloud: From data profiling to blueprint","","","","","The 7th International Conference on Networked Computing and Advanced Information Management","","","2011","","","101-104","Superchunking Paper Sources/CDC/Fingerprinting/Indexing;All","Fingerprint-Indexing;SSD","","","","","","","","2011-06","","","","","","","https://ieeexplore.ieee.org/abstract/document/5967525/;https://ieeexplore.ieee.org/iel5/5955406/5967504/05967525.pdf","","","","","Cloud storage system is becoming the substantial component of the cloud system due to emerging trend of user data. Different from other computing resources, storage resource is vulnerable to the cost issue since the data should be maintained during the downtime. In this paper, we investigate the benefit and overhead when deduplication techniques are adopted to the cloud storage system. From the result, we discuss several challenges across the cloud storage. Furthermore, we suggest the cloud storage architecture and the deduplication engine to optimize the deduplication feature in the cloud storage system. We expect that our suggestions reduce the cloud storage system cost efficiently without performance degradation of data transfer.","cloud computing;electronic data interchange;performance evaluation;software architecture;storage management;rethinking deduplication;data profiling;blueprint;substantial component;cloud system;user data;computing resources;storage resource;deduplication techniques;cloud storage architecture;deduplication engine;deduplication feature;cloud storage system cost;performance degradation;data transfer;Cloud computing;Indexes;Engines;Computer architecture;USA Councils;Hardware;Graphics processing unit","Use SSD for fingerprints","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2011 - Rethinking deduplication in cloud - From data profiling to blueprint.pdf","","","",""
"Journal article","Min J,Yoon D,Won Y","","Efficient Deduplication Techniques for Modern Backup Operation","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2011","60","6","824-840","Superchunking Paper Sources/CDC/Fingerprinting/Indexing;All;Grouped by Publication/IEEE Transactions on Computers;Thesis;p-scailbib","Fingerprint-Indexing","","","","","","","","2011-06","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2010.263;https://ieeexplore.ieee.org/abstract/document/5669285/;https://ieeexplore.ieee.org/iel5/12/4358213/05669285.pdf","10.1109/TC.2010.263","","","","In this work, we focus on optimizing the deduplication system by adjusting the pertinent factors in fingerprint lookup and chunking, the factors which we identify as the key ingredients of efficient deduplication. For efficient fingerprint lookup, we propose fingerprint management scheme called LRU-based Index Partitioning. For efficient chunking, we propose Incremental Modulo-K(INC-K) algorithm which is optimized Rabin's algorithm where we significantly reduce the number of arithmetic operations exploiting the algebraic nature of modulo arithmetic. LRU-based Index Partitioning uses the notion of tablet and enforces access locality of the fingerprint lookup in storing fingerprints. We maintain tablets with LRU manner to exploit temporal locality of the fingerprint lookup. To preserve access correlation across the tablets, we apply prefetching in maintaining tablet list. We propose Context-aware chunking to maximize chunking speed and deduplication ratio. We develop prototype backup system and performed comprehensive analysis on various factors and their relationship: average chunk size, chunking speed, deduplication ratio, tablet management algorithms, and overall backup speed. By increasing the average chunk size from 4 KB to 10 KB, chunking time increases by 34.3 percent, deduplication ratio decreases by 0.66 percent and the overall backup speed increases by 50 percent (from 51.4 MB/sec to 77.8 MB/sec).","file organisation;deduplication technique;fingerprint lookup;fingerprint chunking;fingerprint management scheme;LRU-based index partitioning;incremental modulo-K algorithm;Rabin algorithm;tablet notion;context-aware chunking;Fingerprint recognition;Servers;Generators;Redundancy;Indexes;Partitioning algorithms;History;Deduplication;chunking;backup;index partitioning;fingerprint lookup.","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Min et al. 2011 - Efficient Deduplication Techniques for Modern Backup Operation.pdf","","","",""
"Journal article","Tan Y,Yan Z,Feng D,He X,Zou Q,Yang L","","De-Frag: an efficient scheme to improve deduplication performance via reducing data placement de-linearization","Cluster Comput.","Cluster computing","","","","","","2015","18","1","79-92","Superchunking Paper Sources/CDC/Fingerprinting/Indexing;All","Fingerprint-Indexing","","","","","","","","2015-03-01","","","","","1386-7857","1573-7543","https://doi.org/10.1007/s10586-014-0397-5;http://dx.doi.org/10.1007/s10586-014-0397-5;https://link.springer.com/article/10.1007/s10586-014-0397-5","10.1007/s10586-014-0397-5","","","","Data deduplication has become a commodity in large-scale storage systems, especially in data backup and archival systems. However, due to the removal of redundant data, data deduplication de-linearizes data placement and forces the data chunks of the same data object to be divided into multiple separate units. In our preliminary study, we found that the de-linearization of data placement compromises the data spatial locality that is used to improve data read performance, deduplication throughput and deduplication efficiency in some deduplication approaches, which significantly affects deduplication performance and makes some deduplication approaches become less effective. In this paper, we first analyze the negative effect of data placement de-linearization to deduplication performance, and then propose an effective approach called De-Frag to reduce the de-linearization of data placement. The key idea of De-Frag is to choose some redundant data to be written to the disks rather than be removed. It quantifies the spatial locality of each chunk group by spatial locality level (SPL for short) and writes the redundant chunks to disks when SPL value is smaller than a preset value, thus to reduce the de-linearization of data placement and enhance the spatial locality. As shown in our experimental results driven by real world datasets, De-Frag effectively enhances data spatial locality and improves deduplication throughput, deduplication efficiency, and data read performance, at the cost of slightly lower compression ratios.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Kaur R,Chana I,Bhattacharya J","","Data deduplication techniques for efficient cloud storage management: a systematic review","J. Supercomput.","The Journal of supercomputing","","","","","","2018","74","5","2035-2085","Superchunking Paper Sources/CDC/Fingerprinting/Indexing;All","Fingerprint-Indexing;Survey","","","","","","","","2018-05-01","","","","","0920-8542","1573-0484","https://doi.org/10.1007/s11227-017-2210-8;http://dx.doi.org/10.1007/s11227-017-2210-8;https://link.springer.com/article/10.1007/s11227-017-2210-8","10.1007/s11227-017-2210-8","","","","The exponential growth of digital data in cloud storage systems is a critical issue presently as a large amount of duplicate data in the storage systems exerts an extra load on it. Deduplication is an efficient technique that has gained attention in large-scale storage systems. Deduplication eliminates redundant data, improves storage utilization and reduces storage cost. This paper presents a broad methodical literature review of existing data deduplication techniques along with various existing taxonomies of deduplication techniques that have been based on cloud data storage. Furthermore, the paper investigates deduplication techniques based on text and multimedia data along with their corresponding taxonomies as these techniques have different challenges for duplicate data detection. This research work is useful to identify deduplication techniques based on text, image and video data. It also discusses existing challenges and significant research directions in deduplication for future researchers, and article concludes with a summary of valuable suggestions for future enhancements in deduplication.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaur et al. 2018 - Data deduplication techniques for efficient cloud storage management - a systematic review.pdf","","","",""
"Journal article","Cao Z,Wen H,Ge X,Ma J,Diehl J,Du DH","","TDDFS: A tier-aware data deduplication-based file system","ACM Transactions on Storage (TOS)","","","","","","","2019","15","1","1-26","Superchunking Paper Sources/CDC/Chunking;All","Chunking","","","ACM New York, NY, USA","","","","","2019","","","","","","","https://dl.acm.org/doi/abs/10.1145/3295461?casa_token=6PpCrZZZh3sAAAAA:FIhUDlLbRZfFPfxsof65yZzuBcdALCbgdhntAkJacccZQXecgc45rVAuUEOUvImRvJHi55s4kWcQnQ;https://dl.acm.org/doi/pdf/10.1145/3295461?casa_token=NltyUQMRVmgAAAAA:PFG4n6Y8k27KQycABQ9lhqbbaNsilZxpY66lPTOgkCR0aHyTiHv5YwvS0C487DqmjjnUgoOAFBfPVw","","","","","With the rapid increase in the amount of data produced and the development of new types of storage devices, storage tiering continues to be a popular way to achieve a good tradeoff between performance and costeffectiveness. In a basic two-tier storage system, a storage tier with higher performance and typically higher cost (the fast tier) is used to store frequently-accessed (active) data while a large amount of less-active data are stored in the lower-performance and low-cost tier (the slow tier). Data are migrated between these two tiers according to their activity. In this article, we propose a Tier-aware Data Deduplication-based File System, called TDDFS, which can operate efficiently on top of a two-tier storage environment. Specifically, to achieve better performance, nearly all file operations are performed in the fast tier. To achieve higher cost-effectiveness, files are migrated from the fast tier to the slow tier if they are no longer active, and this migration is done with data deduplication. The distinctiveness of our design is that it maintains the non-redundant (unique) chunks produced by data deduplication in both tiers if possible. When a file is reloaded (called a reloaded file) from the slow tier to the fast tier, if some data chunks of the file already exist in the fast tier, then the data migration of these chunks from the slow tier can be avoided. Our evaluation shows that TDDFS achieves close to the best overall performance among various file-tiering designs for two-tier storage systems.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cao et al. 2019 - TDDFS - A tier-aware data deduplication-based file system.pdf","","","",""
"Miscellaneous","Hirsch M,Ish-Shalom A,Klein ST","","Optimal partitioning of data chunks in deduplication systems","Discrete Applied Mathematics","","","","","","","2016","212","","104-114","Superchunking Paper Sources/CDC/Chunking;All","Chunking;Fingerprint-Indexing","","","","","","","","2016","","","","","","","http://dx.doi.org/10.1016/j.dam.2015.12.018","10.1016/j.dam.2015.12.018","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hirsch et al. 2016 - Optimal partitioning of data chunks in deduplication systems.pdf","","","",""
"Journal article","Hirsch M,Klein ST,Shapira D,Toaff Y","","Dynamic determination of variable sizes of chunks in a deduplication system","Discrete Appl. Math.","Discrete applied mathematics ","","","","","","2018","","","","Superchunking Paper Sources/CDC/Chunking;All;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking","Chunking","","","","","","","","2018","","","","","0166-218X","","http://dx.doi.org/10.1016/j.dam.2018.07.015","10.1016/j.dam.2018.07.015","","","","Deduplication is a special case of data compression in which repeated chunks of data are stored only once. The input data is cut into chunks and a cryptographically strong hash value of each (different) chunk is stored. To restrict the influence of small inserts and deletes to local perturbations, the chunk boundaries are usually defined in a data dependent way, which implies that the chunks are of variable length. Usually, the chunk sizes may spread over a large range, which might have a negative impact on the storage performance. This can be dealt with by imposing artificial lower and upper bounds. This paper proposes an alternative by which the chunk size distribution is controlled in a natural way. Some analytical and experimental results are given.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hirsch et al. 2018 - Dynamic determination of variable sizes of chunks in a deduplication system.pdf","","","",""
"Journal article","Aronovich L,Asher R,Harnik D,Hirsch M,Klein ST,Toaff Y","","Similarity based deduplication with small data chunks","Discrete Appl. Math.","Discrete applied mathematics ","","","","","","2016","212","","10-22","Superchunking Paper Sources/CDC/Chunking;All;Thesis","Chunking;Similarity/Resemblance","","","","","","","","2016","","","","","0166-218X","","http://dx.doi.org/10.1016/j.dam.2015.09.018","10.1016/j.dam.2015.09.018","","","","Large backup and restore systems may have a petabyte or more data in their repository. Such systems are often compressed by means of deduplication techniques, that partition the input text into chunks and store recurring chunks only once. One of the approaches is to use hashing methods to store fingerprints for each data chunk, detecting identical chunks with very low probability for collisions. As alternative, it has been suggested to use similarity instead of identity based searches, which allows the definition of much larger chunks. This implies that the data structure needed to store the fingerprints is much smaller, so that such a system may be more scalable than systems built on the first approach. This paper deals with an extension of the second approach to systems in which it is still preferred to use small chunks. We describe the design choices made during the development of what we call an approximate hash function, serving as the basic tool of the new suggested deduplication system and report on extensive tests performed on a variety of large input files.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Aronovich et al. 2016 - Similarity based deduplication with small data chunks.pdf","","","",""
"Journal article","Murugan M,Kant K,Raghavan A,Du DH","","Software defined deduplicated replica management in scale-out storage systems","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2019","97","","340-354","Superchunking Paper Sources/CDC/Chunking;All","Chunking","","","","","","","","2019-08-01","","","","","0167-739X","","http://www.sciencedirect.com/science/article/pii/S0167739X18323884;http://dx.doi.org/10.1016/j.future.2019.02.018;https://www.sciencedirect.com/science/article/pii/S0167739X18323884","10.1016/j.future.2019.02.018","","","","Given the unabated growth of storage in data centers, its energy footprint continues to grow, which makes storage energy management a crucial issue. Furthermore, with the emerging trend of underprovisioning of power and cooling infrastructures in large facilities, it is important to flexibly adapt the entire infrastructure of a client, including its storage system, to changing energy limitations. In this paper, we presentan energy-adaptive framework called flexStore that provides a flexible mechanism to specify and control the energy vs. performance tradeoffs. These mechanisms are defined and enforced by a software layer called “Policy Engine” that controls the number of active copies of deduplicated data chunks in storage containers based on energy availability. The mechanism synchronizes data chunks in inactive storage containers with those in the active containers, in the background to enable them to be put into service quickly when needed. We evaluate flexStore with different workloads on a sample data center environment and demonstrate the effectiveness of its control mechanisms in adapting to the performance and energy constraints.","Software defined energy management; Adaptive deduplication; Energy adaptive storage; Flexible distributed storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Murugan et al. 2019 - Software defined deduplicated replica management in scale-out storage systems.pdf","","","",""
"Journal article","Wang G,Chen S,Lin M,Liu X","","SBBS: A sliding blocking algorithm with backtracking sub-blocks for duplicate data detection","Expert Syst. Appl.","Expert systems with applications","","","","","","2014","41","5","2415-2423","Superchunking Paper Sources/CDC/Chunking;All","Chunking","","","","","","","","2014-04-01","","","","","0957-4174","","http://www.sciencedirect.com/science/article/pii/S095741741300794X;http://dx.doi.org/10.1016/j.eswa.2013.09.040;https://www.sciencedirect.com/science/article/pii/S095741741300794X","10.1016/j.eswa.2013.09.040","","","","With the explosive growth of data, storage systems are facing huge storage pressure due to a mass of redundant data caused by the duplicate copies or regions of files. Data deduplication is a storage-optimization technique that reduces the data footprint by eliminating multiple copies of redundant data and storing only unique data. The basis of data deduplication is duplicate data detection techniques, which divide files into a number of parts, compare corresponding parts between files via hash techniques and find out redundant data. This paper proposes an efficient sliding blocking algorithm with backtracking sub-blocks called SBBS for duplicate data detection. SBBS improves the duplicate data detection precision of the traditional sliding blocking (SB) algorithm via backtracking the left/right 1/4 and 1/2 sub-blocks in matching-failed segments. Experimental results show that SBBS averagely improves the duplicate detection precision by 6.5% compared with the traditional SB algorithm and by 16.5% compared with content-defined chunking (CDC) algorithm, and it does not increase much extra storage overhead when SBBS divides the files into equal chunks of size 8kB.","Data deduplication; Duplicate data detection; Sliding blocking algorithm; Backtracking; SBBS; Content-defined chunking algorithm","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2014 - SBBS - A sliding blocking algorithm with backtracking sub-blocks for duplicate data detection.pdf","","","",""
"Journal article","Widodo RN,Lim H,Atiquzzaman M","","A new content-defined chunking algorithm for data deduplication in cloud storage","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2017","71","","145-156","Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;All","Chunking","","","","","","","","2017-06-01","","","","","0167-739X","","http://www.sciencedirect.com/science/article/pii/S0167739X16305829;http://dx.doi.org/10.1016/j.future.2017.02.013;https://www.sciencedirect.com/science/article/pii/S0167739X16305829","10.1016/j.future.2017.02.013","","","","Chunking is a process to split a file into smaller files called chunks. In some applications, such as remote data compression, data synchronization, and data deduplication, chunking is important because it determines the duplicate detection performance of the system. Content-defined chunking (CDC) is a method to split files into variable length chunks, where the cut points are defined by some internal features of the files. Unlike fixed-length chunks, variable-length chunks are more resistant to byte shifting. Thus, it increases the probability of finding duplicate chunks within a file and between files. However, CDC algorithms require additional computation to find the cut points which might be computationally expensive for some applications. In our previous work (Widodo et al., 2016), the hash-based CDC algorithm used in the system took more process time than other processes in the deduplication system. This paper proposes a high throughput hash-less chunking method called Rapid Asymmetric Maximum (RAM). Instead of using hashes, RAM uses bytes value to declare the cut points. The algorithm utilizes a fix-sized window and a variable-sized window to find a maximum-valued byte which is the cut point. The maximum-valued byte is included in the chunk and located at the boundary of the chunk. This configuration allows RAM to do fewer comparisons while retaining the CDC property. We compared RAM with existing hash-based and hash-less deduplication systems. The experimental results show that our proposed algorithm has higher throughput and bytes saved per second compared to other chunking algorithms.","Data deduplication; Cloud storage; Content-defined chunking; Hash-less chunking; Asymmetric window","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Widodo et al. 2017 - A new content-defined chunking algorithm for data deduplication in cloud storage.pdf","","","",""
"Conference paper","Ni F,Lin X,Jiang S","","SS-CDC: a two-stage parallel content-defined chunking for deduplicating backup storage","","","","","Proceedings of the 12th ACM International Conference on Systems and Storage","","","2019","","","86-96","Superchunking Paper Sources/CDC/Chunking;All;Parallel Schemes;Thesis","Chunking;Accelerate","","","","","","","","2019","","","","","","","https://dl.acm.org/doi/abs/10.1145/3319647.3325834;http://ranger.uta.edu/~sjiang/pubs/papers/ni19-ss-cdc.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Ni et al. 2019 - SS-CDC - a two-stage parallel content-defined chunking for deduplicating backup storage.pdf","","","",""
"Conference paper","Tang Z,Won Y","","Multithread Content Based File Chunking System in CPU-GPGPU Heterogeneous Architecture","","","","","2011 First International Conference on Data Compression, Communications and Processing","","","2011","","","58-64","Superchunking Paper Sources/CDC/Chunking;All","Chunking;GPU","","","","","","","","2011-06","","","","","","","http://dx.doi.org/10.1109/CCP.2011.20;https://ieeexplore.ieee.org/abstract/document/6061028/;http://esos.hanyang.ac.kr/files/publication/conferences/international/Multithread_Content_Based_File_Chunking_System_in_CPU-GPGPU_Heterogeneous_Architecture.pdf","10.1109/CCP.2011.20","","","","The fast development of Graphics Processing Unit (GPU) leads to the popularity of General-purpose usage of GPU (GPGPU). So far, most modern computers are CPU-GPGPU heterogeneous architecture and CPU is used as host processor. In this work, we promote a multithread file chunking prototype system, which is able to exploit the hardware organization of the CPU-GPGPU heterogeneous computer and determine which device should be used to chunk the file to accelerate the content based file chunking operation of deduplication. We built rules for the system to choose which device should be used to chunk file and also found the optimal choice of other related parameters of both CPU and GPGPU subsystem like segment size and block dimension. This prototype was implemented and tested. The result of using GTX460(336 cores) and Intel i5 (four cores) shows that this system can increase the chunking speed 63% compared to using GPGPU alone and 80% compared to using CPU alone.","computer graphic equipment;coprocessors;file organisation;multiprocessing systems;multi-threading;multithread content based file chunking prototype system;CPU-GPGPU heterogeneous architecture;graphics processing unit;general-purpose GPU;file deduplication;Intel i5 core;GTX460 core;Computers;Instruction sets;Graphics processing unit;Switches;Multicore processing;Hardware;CPU-GPGPU;Content Based File Chunking;Deduplication;Incremental Modulo-K","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tang and Won 2011 - Multithread Content Based File Chunking System in CPU-GPGPU Heterogeneous Architecture.pdf","","","",""
"Journal article","Won Y,Lim K,Min J","","MUCH: Multithreaded Content-Based File Chunking","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2015","64","5","1375-1388","Superchunking Paper Sources/CDC/Chunking;All;Grouped by Publication/IEEE Transactions on Computers","Chunking;Accelerate","","","","","","","","2015-05","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2014.2322600;https://ieeexplore.ieee.org/abstract/document/6815680/;http://www.esos.hanyang.ac.kr/files/publication/conferences/international/MUCH_Multithreaded_Content_Based_File_Chunking.pdf","10.1109/TC.2014.2322600","","","","In this work, we developed a novel multithreaded variable size chunking method, MUCH, which exploits the multicore architecture of the modern microprocessors. The legacy single threaded variable size chunking method leaves much to be desired in terms of effectively exploiting the bandwidth of the state of the art storage devices. MUCH guarantees chunking invariability: The result of chunking does not change regardless of the degree of multithreading or the segment size. This is achieved by inter and intra-segment coalescing at the master thread and Dual Mode Chunking at the client thread. We developed an elaborate performance model to determine the optimal multithreading degree and the segment size. MUCH is implemented in the prototype deduplication system. By fully exploiting the available CPU cores (quad-core), we achieved up to ×4 increase in the chunking performance (MByte/sec). MUCH successfully addresses the performance issues of file chunking which is one of the performance bottlenecks in modern deduplication systems by parallelizing the file chunking operation while guaranteeing Chunking Invariability.","multiprocessing systems;multi-threading;parallel architectures;storage management;MUCH;multithreaded content-based file chunking;multithreaded variable size chunking method;multicore architecture;modern microprocessors;legacy single threaded variable size chunking method;art storage devices;chunking invariability;segment size multithreading;inter-segment coalescing;intra-segment coalescing;master thread;dual mode chunking;client thread;optimal multithreading degree;prototype deduplication system;CPU cores;quad-core;chunking performance;Instruction sets;Bandwidth;Multithreading;Hardware;Upper bound;Central Processing Unit;Redundancy;Content-based chunking;deduplication;multithread","""Segment"" here is a coarse paritioning of a file to spread amoung processors for chunking.","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Won et al. 2015 - MUCH - Multithreaded Content-Based File Chunking.pdf","","","",""
"Conference paper","Raju R,Moh M,Moh T","","Compression of Wearable Body Sensor Network Data Using Improved Two-Threshold-Two-Divisor Data Chunking Algorithms","","","","","2018 International Conference on High Performance Computing Simulation (HPCS)","","","2018","","","949-956","All","","","","","","","","","2018-07","","","","","","","http://dx.doi.org/10.1109/HPCS.2018.00150;https://ieeexplore.ieee.org/abstract/document/8514455/;https://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1609&context=etd_projects","10.1109/HPCS.2018.00150","","","","Data compression plays a significant role in Body Sensor Networks (BSN). This is true since the sensors in BSNs have limited battery power and memory; sensor data needs to be transmitted regularly, and in lossless manner to provide prompt, accurate feedback. The paper evaluates lossless data compression algorithms including Run Length Encoding (RLE), Lempel Zev Welch (LZW), and Huffman on data from wearable devices and compares them in terms of Compression Ratio, Compression Factor, Savings Percentage and Compression Time. It also evaluates a data deduplication technique used for Low Bandwidth File Systems (LBFS), Two Thresholds Two Divisors (TTTD) algorithm, to determine if it is suitable for BSN data. First, through experiments s we arrive at a set of parameter values that give compression ratio above 50 on BSN data. Next, based on performance evaluation results of TTTD and classical compression algorithms including RLW, LAW, and Huffman, it proposes a technique to combine multiple algorithms in sequence. Upon comparison of the performance, it is found that the new algorithm, TTTD-H, which executes TTTD and Huffman in sequence, significantly improves the compression factor against both TTTD and Huffman. Performance evaluation has been carried out in two sets of BSN data.","body sensor networks;data compression;compression factor;data deduplication technique;BSN data;compression ratio;classical compression algorithms;lossless data compression algorithms;compression time;Huffman algorithm;wearable body sensor network data compression;two-threshold-two-divisor data chunking algorithms;run length encoding;RLE;lempel zev welch;LZW;wearable devices;savings percentage;low bandwidth file systems;LBFS;RLW algorithm;LAW algorithm;TTTD-H algorithm;Feature extraction;Data compression;Encoding;Wireless communication;Wireless sensor networks;Compression algorithms;Data processing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Raju et al. 2018 - Compression of Wearable Body Sensor Network Data Using Improved Two-Threshold-Two-Divisor Data Chunking Algorithms.pdf","","","",""
"Journal article","Zhang C,Qi D,Li W,Guo J","","Function of Content Defined Chunking Algorithms in Incremental Synchronization","IEEE Access","","","","","","","2020","8","","5316-5330","Superchunking Paper Sources/CDC/Chunking;All;FSL Traces;Thesis","Chunking","","","","","","","","2020","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2019.2963625;https://ieeexplore.ieee.org/abstract/document/8949536/;https://ieeexplore.ieee.org/iel7/6287639/8948470/08949536.pdf","10.1109/ACCESS.2019.2963625","","","","Data chunking algorithms divide data into several small data chunks in a certain way, thus transforming the operation of data into the one of multiple small data chunks. Data chunking algorithms have been widely used in duplicate data detection, parallel computing and other fields, but it is seldom used in data incremental synchronization. Aiming at the characteristics of incremental data synchronization, this paper proposes a novel data chunking algorithm. By dividing two data that need synchronization into small data chunks, comparing the contents of these small data chunks, different ones are the incremental data that need to be found. The new algorithm determines to set a cut-point based on the number of 1 contained in the binary format of all bytes in an interval. Thus it improves the resistance against the byte shifting problem at the expense of the chunk size stability, which makes it more suitable for the incremental data synchronization. Comparing this algorithm with several known classical or state of art algorithms, experiments show that the incremental data found by this algorithm can be reduced by 32% 57% compared to the others with same changes between two data. The experimental results based on real-world datasets show that PCI improves the calculation speed of classic Rsync algorithm up to 70%, however, with a drawback of increasing the Transmission compression rate up to 11.8%.","Data synchronization;chunking algorithm;data backup;increment","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2020 - Function of Content Defined Chunking Algorithms in Incremental Synchronization.pdf","","","",""
"Journal article","Zhang C,Qi D,Cai Z,Huang W,Wang X,Li W,Guo J","","MII: A Novel Content Defined Chunking Algorithm for Finding Incremental Data in Data Synchronization","IEEE Access","","","","","","","2019","7","","86932-86945","Superchunking Paper Sources/CDC/Chunking;All;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Superchunking Paper Sources/CDC/Chunking/Metadata","Chunking;Datasets;Metadata","","","","","","","","2019","","","","","2169-3536","","http://dx.doi.org/10.1109/ACCESS.2019.2926195","10.1109/ACCESS.2019.2926195","","","","In the data backup system, to reduce the bandwidth and processing time overhead caused by full backup technology during data synchronization between backups and source data, incremental backup technology is emerging as the focus of academic and industrial research. It is key but poorly-solved to find the incremental data between backups and source data for incremental backup technology. To find out the incremental data during the backup process, here, in this paper, we propose a novel content-defined chunking algorithm. The source data and backup data are chunked into some small chunks in the same way with the variable length. Then, by comparing whether a chunk of source data is different from any of the chunks in backup data, we can evaluate whether the chunk of source data is incremental data. By experiments, the chunking algorithm in this paper is compared to other ones which are the classical or state-of-the-art algorithms. The experimental results show that the incremental data found by this algorithm can be reduced by 13%-34% compared to the others with the same chunk throughput.","back-up procedures;disc storage;security of data;source data;content defined chunking algorithm;incremental backup technology;data backup system;data synchronization;incremental data;backup data;Synchronization;Databases;Resistance;Throughput;Classification algorithms;Servers;Companies;Data synchronization;chunking algorithm;data backup;increment","Authors use PRF to generate experimental datasets","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2019 - MII - A Novel Content Defined Chunking Algorithm for Finding Incremental Data in Data Synchronization.pdf","","","",""
"Conference paper","Koo D,Hur J,Yoon H","","Secure and Efficient Deduplication over Encrypted Data with Dynamic Updates in Cloud Storage","","","","","Frontier and Innovation in Future Computing and Communications","","","2014","","","229-235","ILL Request;All;Superchunking Paper Sources/CDC/Chunking/Metadata","Secure Dedup;Metadata","","","Springer Netherlands","","","","","2014","","","","","","","http://dx.doi.org/10.1007/978-94-017-8798-7_28;https://link.springer.com/chapter/10.1007/978-94-017-8798-7_28;https://link.springer.com/content/pdf/10.1007%2F978-94-017-8798-7_28.pdf","10.1007/978-94-017-8798-7_28","","","","Cloud service providers adopt a deduplication technique to minimize resource utility costs. However, it is one of the most challenging issues to manage the outsourced data in a storage-efficient way when users encrypt data for preserving privacy and frequently update it. When the data is updated, file-level deduplication makes entire copy of updated file although there are small modifications. Block-level deduplication solves this problem, but it requires metadata larger than the outsourced blocks. To address this problem, we propose a hybrid deduplication scheme that minimizes storage overhead. Our scheme performs file-level deduplication along with isolation of only updated blocks with augmented metadata. The analysis results show that our scheme minimizes storage costs while guaranteeing secure update with efficient verification.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Tezuka S,Uda R,Okada K","","ADEC: Assured Deletion and Verifiable Version Control for Cloud Storage","2012 IEEE 26th International Conference on Advanced Information Networking and Applications","","","","","","","2012","","","","All","","","","","","","","","2012","","","","","","","http://dx.doi.org/10.1109/aina.2012.116","10.1109/aina.2012.116","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Lai J,Xiong J,Wang C,Wu G,Li Y","","A Secure Cloud Backup System with Deduplication and Assured Deletion","Provable Security","","","","","","","2017","","","74-83","All","Versioning","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1007/978-3-319-68637-0_5","10.1007/978-3-319-68637-0_5","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lai et al. 2017 - A Secure Cloud Backup System with Deduplication and Assured Deletion.pdf","","","",""
"Conference paper","Rahumed A,Chen HC,Tang Y,Lee PP,Lui JC","","A Secure Cloud Backup System with Assured Deletion and Version Control","","","","","2011 40th International Conference on Parallel Processing Workshops","","","2011","","","160-167","All","Versioning","","","","","","","","2011-09","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPPW.2011.17;https://ieeexplore.ieee.org/abstract/document/6047288/;http://www.cs.cuhk.hk/~cslui/PUBLICATION/cloudsec11.pdf","10.1109/ICPPW.2011.17","","","","Cloud storage is an emerging service model that enables individuals and enterprises to outsource the storage of data backups to remote cloud providers at a low cost. However, cloud clients must enforce security guarantees of their outsourced data backups. We present Fade Version, a secure cloud backup system that serves as a security layer on top of today's cloud storage services. Fade Version follows the standard version-controlled backup design, which eliminates the storage of redundant data across different versions of backups. On top of this, Fade Version applies cryptographic protection to data backups. Specifically, it enables fine-grained assured deletion, that is, cloud clients can assuredly delete particular backup versions or files on the cloud and make them permanently inaccessible to anyone, while other versions that share the common data of the deleted versions or files will remain unaffected. We implement a proof-of-concept prototype of Fade Version and conduct empirical evaluation atop Amazon S3. We show that Fade Version only adds minimal performance overhead over a traditional cloud backup service that does not support assured deletion.","back-up procedures;cloud computing;configuration management;cryptography;outsourcing;secure cloud backup system;cloud storage;remote cloud provider;outsourced data backup;FadeVersion;version-controlled backup design;cryptographic protection;fine-grained assured deletion;proof-of-concept prototype;Clouds;Encryption;Cloud computing;Control systems;Databases","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rahumed et al. 2011 - A Secure Cloud Backup System with Assured Deletion and Version Control.pdf","","","",""
"Conference paper","Blasco J,Di Pietro R,Orfila A,Sorniotti A","","A tunable proof of ownership scheme for deduplication using Bloom filters","","","","","2014 IEEE Conference on Communications and Network Security","","","2014","","","481-489","All;Thesis","PoW","","","","","","","","2014-10","","","","","","","http://dx.doi.org/10.1109/CNS.2014.6997518;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6997518","10.1109/CNS.2014.6997518","","","","Deduplication is a widely used technique in storage services, since it affords a very efficient usage of resources-being especially effective for consumer-grade storage services (e.g. Dropbox). Deduplication has been shown to suffer from several security weaknesses, the most severe ones enabling a malicious user to obtain possession of a file it is not entitled to. Standard solutions to this problem require users to prove possession of data prior to its upload. Unfortunately, the schemes proposed in the literature are very taxing on either the server or the client side. In this paper, we introduce a novel solution based on Bloom filters that provides a flexible, scalable, and provably secure solution to the weaknesses of deduplication, and that overcomes the deficiencies of existing approaches. We provide a formal description of the scheme, a thorough security analysis, and compare our solution against multiple existing ones, both analytically and by means of extensive benchmarking. Our results confirm the quality and viability of our approach.","data structures;security of data;theorem proving;deduplication;consumer-grade storage services;security weaknesses;malicious user;bloom filters;thorough security analysis;tunable proof of ownership scheme;Servers;Security;Cloud computing;Bandwidth;Indexes;Proposals;Memory management","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Blasco et al. 2014 - A tunable proof of ownership scheme for deduplication using Bloom filters.pdf","","","",""
"Conference paper","Boneh D,Boyen X","","Short Signatures Without Random Oracles","","","","","Advances in Cryptology - EUROCRYPT 2004","","","2004","","","56-73","All","Encryption Techniques","","","Springer Berlin Heidelberg","","","","","2004","","","","","","","http://dx.doi.org/10.1007/978-3-540-24676-3_4;https://link.springer.com/chapter/10.1007/978-3-540-24676-3_4;https://link.springer.com/content/pdf/10.1007/978-3-540-24676-3_4.pdf","10.1007/978-3-540-24676-3_4","","","","We describe a short signature scheme which is existentially unforgeable under a chosen message attack without using random oracles. The security of our scheme depends on a new complexity assumption we call the Strong Diffie-Hellman assumption. This assumption has similar properties to the Strong RSA assumption, hence the name. Strong RSA was previously used to construct signature schemes without random oracles. However, signatures generated by our scheme are much shorter and simpler than signatures from schemes based on Strong RSA. Furthermore, our scheme provides a limited form of message recovery.","","See more recent (2008) paper on this subject by these authors.","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boneh and Boyen 2004 - Short Signatures Without Random Oracles.pdf","","","",""
"Miscellaneous","Boneh D,Boyen X","","Short Signatures Without Random Oracles and the SDH Assumption in Bilinear Groups","Journal of Cryptology","","","","","","","2008","21","2","149-177","All","Encryption Techniques","","","","","","","","2008","","","","","","","http://dx.doi.org/10.1007/s00145-007-9005-7","10.1007/s00145-007-9005-7","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boneh and Boyen 2008 - Short Signatures Without Random Oracles and the SDH Assumption in Bilinear Groups.pdf","","","",""
"Conference paper","Ng WK,Wen Y,Zhu H","","Private data deduplication protocols in cloud storage","","","","","Proceedings of the 27th Annual ACM Symposium on Applied Computing","","","2012","","","441-446","All;Thesis","PoW","","","Association for Computing Machinery","New York, NY, USA","","Trento, Italy","","2012-03-26","","","9781450308571","","","","https://doi.org/10.1145/2245276.2245361;http://dx.doi.org/10.1145/2245276.2245361;https://dl.acm.org/doi/abs/10.1145/2245276.2245361?casa_token=FZkeLv4fs88AAAAA:Zj9DSpXIYvC--mixHp2KmMTAVD4r36-O-HXICAjGYgqvA9d78zuGg4ldUko71Rtc8fxrLsbaFOVv3g;https://dl.acm.org/doi/pdf/10.1145/2245276.2245361?casa_token=2K1ZCRlb1UAAAAAA:486qljBcHIkAN8h2Ri_lIt3RmuySUI0E0rRdASPc4vBBA8gzdFDvxHm8xEFyb87vpTvALfCN-7G6Aw","10.1145/2245276.2245361","","","","In this paper, a new notion which we call private data deduplication protocol, a deduplication technique for private data storage is introduced and formalized. Intuitively, a private data deduplication protocol allows a client who holds a private data proves to a server who holds a summary string of the data that he/she is the owner of that data without revealing further information to the server. Our notion can be viewed as a complement of the state-of-the-art public data deduplication protocols of Halevi et al [7]. The security of private data deduplication protocols is formalized in the simulation-based framework in the context of two-party computations. A construction of private deduplication protocols based on the standard cryptographic assumptions is then presented and analyzed. We show that the proposed private data deduplication protocol is provably secure assuming that the underlying hash function is collision-resilient, the discrete logarithm is hard and the erasure coding algorithm can erasure up to α-fraction of the bits in the presence of malicious adversaries in the presence of malicious adversaries. To the best our knowledge this is the first deduplication protocol for private data storage.","private data deduplication, data storage, cloud computing","","","","","","SAC '12","","","","","","","","","","","","","","","","","","All Papers/N/Ng et al. 2012 - Private data deduplication protocols in cloud storage.pdf","","","",""
"Conference paper","Puzio P,Molva R,Önen M,Loureiro S","","PerfectDedup: Secure Data Deduplication","","","","","Data Privacy Management, and Security Assurance","","","2016","","","150-166","All","Secure Dedup","","","Springer International Publishing","","","","","2016","","","","","","","http://dx.doi.org/10.1007/978-3-319-29883-2_10;https://link.springer.com/chapter/10.1007/978-3-319-29883-2_10;http://www.eurecom.fr/fr/publication/4683/download/rs-publi-4683.pdf;http://www.eurecom.fr/en/publication/4683/download/rs-publi-4683.pdf","10.1007/978-3-319-29883-2_10","","","","With the continuous increase of cloud storage adopters, data deduplication has become a necessity for cloud providers. By storing a unique copy of duplicate data, cloud providers greatly reduce their storage and data transfer costs. Unfortunately, deduplication introduces a number of new security challenges. We propose PerfectDedup, a novel scheme for secure data deduplication, which takes into account the popularity of the data segments and leverages the properties of Perfect Hashing in order to assure block-level deduplication and data confidentiality at the same time. We show that the client-side overhead is minimal and the main computational load is outsourced to the cloud storage provider.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Puzio et al. 2016 - PerfectDedup - Secure Data Deduplication.pdf","","","",""
"Conference paper","Liu J,Duan L,Li Y,Asokan N","","Secure Deduplication of Encrypted Data: Refined Model and New Constructions","","","","","Topics in Cryptology – CT-RSA 2018","","","2018","","","374-393","All","Secure Dedup","","","Springer International Publishing","","","","","2018","","","","","","","http://dx.doi.org/10.1007/978-3-319-76953-0_20;https://link.springer.com/chapter/10.1007/978-3-319-76953-0_20;https://eprint.iacr.org/2017/1089.pdf","10.1007/978-3-319-76953-0_20","","","","Cloud providers tend to save storage via cross-user deduplication, while users who care about privacy tend to encrypt their files on client-side. Secure deduplication of encrypted data (SDoE) which aims to reconcile this apparent contradiction is an active research topic. In this paper, we propose a formal security model for SDoE. We also propose two single-server SDoE protocols and prove their security in our model. We evaluate their deduplication effectiveness via simulations with realistic datasets.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Liu et al. 2018 - Secure Deduplication of Encrypted Data - Refined Model and New Constructions.pdf","","","",""
"Journal article","He Y,Xian H,Wang L,Zhang S","","Secure Encrypted Data Deduplication Based on Data Popularity","Mobile Networks and Applications","","","","","","","2020","","","","All","Secure Dedup","","","Springer","","","","","2020-01-07","","","","","1572-8153","","https://doi.org/10.1007/s11036-019-01504-3;http://dx.doi.org/10.1007/s11036-019-01504-3;https://link.springer.com/article/10.1007/s11036-019-01504-3;https://link.springer.com/content/pdf/10.1007%2Fs11036-019-01504-3.pdf","10.1007/s11036-019-01504-3","","","","Deduplication eliminates duplicated data copies and reduces storage costs of cloud service providers. However, deduplication of encrypted data is difficult. Current solutions rely heavily on trusted third parties, and does not recognize the popularity of data, resulting in unsatisfying security and efficiency. A secure encrypted data deduplication scheme based on data popularity is proposed. Tags are calculated via bilinear mapping to determine whether different encrypted data originate from the same plaintext. Ciphertext policy attribute-based encryption is used to protect the tags. A secure key delivery scheme is designed to pass the data encryption key from an initial data uploader to subsequent uploaders via the cloud server in an offline manner. The cloud server can perform deduplication without the assistance of any online third party. Security analysis and simulation experiments are provided, proving the practicability and efficiency of the proposed scheme.","","Requested from Birkbeck ILL 27 Jan 2020","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/He et al. 2020 - Secure Encrypted Data Deduplication Based on Data Popularity.pdf","","","",""
"Journal article","Yuan H,Chen X,Jiang T,Zhang X,Yan Z,Xiang Y","","DedupDUM: Secure and scalable data deduplication with dynamic user management","Inf. Sci. ","Information sciences","","","","","","2018","456","","159-173","All","Ownership Management;Secure Dedup","","","","","","","","2018-08-01","","","","","0020-0255","","http://www.sciencedirect.com/science/article/pii/S0020025518303803;http://dx.doi.org/10.1016/j.ins.2018.05.024;https://www.sciencedirect.com/science/article/pii/S0020025518303803","10.1016/j.ins.2018.05.024","","","","Data deduplication on cloud enables the cloud servers to store a cope of data and eliminate redundant one so that a goal to save storage space and network bandwidth is realized. Recently, many research works which are concerning to the privacy-preserving problem of dynamic ownership management in the secure data deduplication setting are published. However, to our knowledge, the existing schemes are not efficient when the cloud user joining and revocation frequently go on, especially in the absence of a trusted third party in practical cloud storage systems. In this paper, we propose a secure and scalable data deduplication scheme with dynamic user management, which updates dynamic group users in a secure way and restricts the unauthorized cloud users from the sensitive data owned by valid users. To further mitigate the communication overhead, the pre-verified accessing control technology is adopted, which prevents the unauthorized cloud users from downloading data. In other words, our present scheme also ensures that only the valid cloud users are able to download and decrypt the ciphertext from the cloud server. All this reduces the communication overhead in our scheme implementation.","Data deduplication; Random convergent encryption; Dynamic user management; Access control; User joining","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Su H,Zheng D,Zhang Y","","An Efficient and Secure Deduplication Scheme Based on Rabin Fingerprinting in Cloud Storage","","","","","2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)","","","2017","1","","833-836","All","Secure Dedup","","","","","","","","2017-07","","","","","","","http://dx.doi.org/10.1109/CSE-EUC.2017.166;https://ieeexplore.ieee.org/abstract/document/8005917/","10.1109/CSE-EUC.2017.166","","","","Data deduplication has been widely used in backups to save storage space and network bandwidth. In order to improve efficiency and security of the current deduplication schemes, this paper proposes a secure deduplication scheme. The proposed scheme supports cloud storage servers to eliminate deduplicate data before users' encryption operations, which can reduce computation overheads. Our scheme realizes variable-size block-level deduplication based on the technique of Rabin fingerprinting. Rabin fingerprinting selects blocks based on property of the block contents and hence supports data update and variable files. In the proposed scheme, a trusted third-party server is introduced to randomize the convergent keys and manage them. Security analysis indicates the proposed scheme is secure against offline brute-force dictionary attacks.","cloud computing;cryptography;fingerprint identification;storage management;trusted computing;Rabin fingerprinting;cloud storage;data deduplication;storage space;network bandwidth;users encryption operations;block-level deduplication;trusted third-party server;security analysis;Servers;Cloud computing;Encryption;Dictionaries;Fingerprint recognition;cloud computing;deduplication;Rabin fingerprinting;convergent encryption","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Su et al. 2017 - An Efficient and Secure Deduplication Scheme Based on Rabin Fingerprinting in Cloud Storage.pdf","","","",""
"Conference paper","Duan Y,Canny J","","Protecting User Data in Ubiquitous Computing: Towards Trustworthy Environments","","","","","Privacy Enhancing Technologies","","","2005","","","167-185","All","Encryption Techniques;Secure Dedup","","","Springer Berlin Heidelberg","","","","","2005","","","","","","","http://dx.doi.org/10.1007/11423409_11;https://link.springer.com/chapter/10.1007/11423409_11;https://cloudfront.escholarship.org/dist/prd/content/qt70m4z882/qt70m4z882.pdf;http://www.cs.berkeley.edu/~jfc/papers/04/PET/pet04.pdf","10.1007/11423409_11","","","","In a Ubiquitous Computing environment, sensors are actively collecting data, much of which can be very sensitive. Data will often be streaming at high rates (video and audio) and it must be dealt with in real-time. Protecting the privacy of users is of central importance. Dealing with these issues will be a central challenge for ubicomp for some time to come. Here we propose some simple design principles which address several of these issues. We illustrate them through the design of a smart room capture system we are building. The main design principle is “data discretion:” users should have access and control of data about them, and should be able to determine how it is used. We show how data discretion supports both personal and collaborative uses. In our implementation, the data discretion principle is enforced with cryptographic techniques. Unlike ACL based access control systems, our scheme embeds access rights of legitimate users within the data. An important property of the method is that it hides meta-information about data access: no user can determine who (else) has access to any given datum. Access information is sensitive because it discloses information about which and when users were in the room. We have implemented a prototype system in the smart room equipped with several cameras, and we give data throughput rates under various degrees of protection. Finally we describe ongoing work towards a trustworthy ubicomp environment whose discretion is realistically checkable.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Duan and Canny 2005 - Protecting User Data in Ubiquitous Computing - Towards Trustworthy Environments.pdf","","","",""
"Conference paper","Shoup V","","Practical Threshold Signatures","","","","","Advances in Cryptology — EUROCRYPT 2000","","","2000","","","207-220","All","Encryption Techniques;Secure Dedup","","","Springer Berlin Heidelberg","","","","","2000","","","","","","","http://dx.doi.org/10.1007/3-540-45539-6_15;https://link.springer.com/content/pdf/10.1007/3-540-45539-6_15.pdf","10.1007/3-540-45539-6_15","","","","We present an RSA threshold signature scheme. The scheme enjoys the following properties:1.it is unforgeable and robust in the random oracle model, assuming the RSA problem is hard;2.signature share generation and verification is completely non-interactive;3.the size of an individual signature share is bounded by a constant times the size of the RSA modulus.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shoup 2000 - Practical Threshold Signatures.pdf","","","",""
"Conference paper","Rogaway P,Shrimpton T","","A Provable-Security Treatment of the Key-Wrap Problem","","","","","Advances in Cryptology - EUROCRYPT 2006","","","2006","","","373-390","All","Encryption Techniques;Secure Dedup","","","Springer Berlin Heidelberg","","","","","2006","","","","","","","http://dx.doi.org/10.1007/11761679_23;https://link.springer.com/chapter/10.1007/11761679_23;https://link.springer.com/content/pdf/10.1007/11761679_23.pdf","10.1007/11761679_23","","","","We give a provable-security treatment for the key-wrap problem, providing definitions, constructions, and proofs. We suggest that key-wrap’s goal is security in the sense of deterministic authenticated-encryption (DAE), a notion that we put forward. We also provide an alternative notion, a pseudorandom injection (PRI), which we prove to be equivalent. We provide a DAE construction, SIV, analyze its concrete security, develop a blockcipher-based instantiation of it, and suggest that the method makes a desirable alternative to the schemes of the X9.102 draft standard. The construction incorporates a method to turn a PRF that operates on a string into an equally efficient PRF that operates on a vector of strings, a problem of independent interest. Finally, we consider IV-based authenticated-encryption (AE) schemes that are maximally forgiving of repeated IVs, a goal we formalize as misuse-resistant AE. We show that a DAE scheme with a vector-valued header, such as SIV, directly realizes this goal.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rogaway and Shrimpton 2006 - A Provable-Security Treatment of the Key-Wrap Problem.pdf","","","",""
"Conference paper","Bellare M,Rogaway P","","The Exact Security of Digital Signatures-How to Sign with RSA and Rabin","","","","","Advances in Cryptology — EUROCRYPT ’96","","","1996","","","399-416","All","Encryption Techniques;Secure Dedup","","","Springer Berlin Heidelberg","","","","","1996","","","","","","","http://dx.doi.org/10.1007/3-540-68339-9_34","10.1007/3-540-68339-9_34","","","","We describe an RSA-based signing scheme which combines essentially optimal efficiency with attractive security properties. Signing takes one RSA decryption plus some hashing, verification takes one RSA encryption plus some hashing, and the size of the signature is the size of the modulus. Assuming the underlying hash functions are ideal, our schemes are not only provably secure, but are so in a tight way—an ability to forge signatures with a certain amount of computational resources implies the ability to invert RSA (on the same size modulus) with about the same computational effort. Furthermore, we provide a second scheme which maintains all of the above features and in addition provides message recovery. These ideas extend to provide schemes for Rabin signatures with analogous properties; in particular their security can be tightly related to the hardness of factoring.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare and Rogaway 1996 - The Exact Security of Digital Signatures-How to Sign with RSA and Rabin.pdf","","","",""
"Conference paper","Goyal V,O’Neill A,Rao V","","Correlated-Input Secure Hash Functions","","","","","Theory of Cryptography","","","2011","","","182-200","All","Encryption Techniques;Secure Dedup","","","Springer Berlin Heidelberg","","","","","2011","","","","","","","http://dx.doi.org/10.1007/978-3-642-19571-6_12;https://link.springer.com/chapter/10.1007/978-3-642-19571-6_12;https://link.springer.com/content/pdf/10.1007/978-3-642-19571-6_12.pdf","10.1007/978-3-642-19571-6_12","","","","We undertake a general study of hash functions secure under correlated inputs, meaning that security should be maintained when the adversary sees hash values of many related high-entropy inputs. Such a property is satisfied by a random oracle, and its importance is illustrated by study of the “avalanche effect,” a well-known heuristic in cryptographic hash function design. One can interpret “security” in different ways: e.g., asking for one-wayness or that the hash values look uniformly and independently random; the latter case can be seen as a generalization of correlation-robustness introduced by Ishai et al. (CRYPTO 2003). We give specific applications of these notions to password-based login and efficient search on encrypted data. Our main construction achieves them (without random oracles) for inputs related by polynomials over the input space (namely ℤp), based on corresponding variants of the q-Diffie Hellman Inversion assumption. Additionally, we show relations between correlated-input secure hash functions and cryptographic primitives secure under related-key attacks. Using our techniques, we are also able to obtain a host of new results for such related-key attack secure cryptographic primitives.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Goyal et al. 2011 - Correlated-Input Secure Hash Functions.pdf","","","",""
"Journal article","Anil Kumar G,Shantala CP","","An extensive research survey on data integrity and deduplication towards privacy in cloud storage","International Journal of Electrical and Computer Engineering","","","","","","","2020","10","2","2011-2011","All","Survey;Secure Dedup","","","","","","","","2020-04","","","","","","","http://dx.doi.org/10.11591/ijece.v10i.pp2011-2022","10.11591/ijece.v10i.pp2011-2022","","","","Owing to the highly distributed nature of the cloud storage system, it is one of the challenging tasks to incorporate a higher degree of security towards the vulnerable data. Apart from various security concerns, data privacy is still one of the unsolved problems in this regards. The prime reason is that existing approaches of data privacy doesn't offer data integrity and secure data deduplication process at the same time, which is highly essential to ensure a higher degree of resistance against all form of dynamic threats over cloud and internet systems. Therefore, data integrity, as well as data deduplication is such associated phenomena which influence data privacy. Therefore, this manuscript discusses the explicit research contribution toward data integrity, data privacy, and data deduplication. The manuscript also contributes towards highlighting the potential open research issues followed by a discussion of the possible future direction of work towards addressing the existing problems.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Anil Kumar and Shantala 2020 - An extensive research survey on data integrity and deduplication towards privacy in cloud storage.pdf","","","",""
"Journal article","Yan XA,Shi, WQ,Tian H","","Cloud Storage Security Deduplication Scheme Based on Dynamic Bloom Filter","Journal of Information Processing Systems","","","","","","","2019","15","6","1265-1276","All","Secure Dedup","","","","","","","","2019-12","","","","","","","http://xml.jips-k.org/full-text/view?doi=10.3745/JIPS.04.0143","10.3745/JIPS.04.0143","","","","Data deduplication is a common method to improve cloud storage efficiency and save network communication bandwidth, but it also brings a series of problems such as privacy disclosure and dictionary attacks. This paper proposes a secure deduplication scheme for cloud storage based on Bloom filter, and dynamically extends the standard Bloom filter. A public dynamic Bloom filter array (PDBFA) is constructed, which improves the efficiency of ownership proof, realizes the fast detection of duplicate data blocks and reduces the false positive rate of the system. In addition, in the process of file encryption and upload, the convergent key is encrypted twice, which can effectively prevent violent dictionary attacks. The experimental results show that the PDBFA scheme has the characteristics of low computational overhead and low false positive rate.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yan et al. 2019 - Cloud Storage Security Deduplication Scheme Based on Dynamic Bloom Filter.pdf","","","",""
"Presentation","","","Lecture 25: Pairing-Based Cryptography","","","","","","","","2004","","","","All","Encryption Techniques;Secure Dedup","","","","","","","","","2004","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Canetti R RR","All Papers/C/Canetti R 2004 - Lecture 25 - Pairing-Based Cryptography.pdf","","","",""
"Conference paper","Raghunathan A,Segev G,Vadhan S","","Deterministic Public-Key Encryption for Adaptively Chosen Plaintext Distributions","","","","","Advances in Cryptology – EUROCRYPT 2013","","","2013","","","93-110","All","Security Proof;Secure Dedup","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-38348-9_6;https://link.springer.com/chapter/10.1007/978-3-642-38348-9_6;https://link.springer.com/content/pdf/10.1007/978-3-642-38348-9_6.pdf","10.1007/978-3-642-38348-9_6","","","","Bellare, Boldyreva, and O’Neill (CRYPTO ’07) initiated the study of deterministic public-key encryption as an alternative in scenarios where randomized encryption has inherent drawbacks. The resulting line of research has so far guaranteed security only for adversarially-chosen plaintext distributions that are independent of the public key used by the scheme. In most scenarios, however, it is typically not realistic to assume that adversaries do not take the public key into account when attacking a scheme.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Raghunathan et al. 2013 - Deterministic Public-Key Encryption for Adaptively Chosen Plaintext Distributions.pdf","","","",""
"Conference paper","Zheng Q,Xu S","","Secure and efficient proof of storage with deduplication","","","","","Proceedings of the second ACM conference on Data and Application Security and Privacy","","","2012","","","1-12","All;Thesis","PoW;Secure Dedup","ACM","","Association for Computing Machinery","New York, NY, USA","","San Antonio, Texas, USA","","2012-02-07","","","9781450310918","","","","https://doi.org/10.1145/2133601.2133603;http://dx.doi.org/10.1145/2133601.2133603;https://dl.acm.org/citation.cfm?id=2133603;http://www.cs.utsa.edu/~shxu/codaspy-2012-p1-zheng.pdf","10.1145/2133601.2133603","","","","Both security and efficiency are crucial to the success of cloud storage. So far, security and efficiency of cloud storage have been separately investigated as follows: On one hand, security notions such as Proof of Data Possession (PDP) and Proof of Retrievability (POR) have been introduced for detecting that the data stored in the cloud has been tampered with. On the other hand, the notion of Proof of Ownership (POW) has also been proposed to alleviate the cloud server from storing multiple copies of the same data, which could substantially reduce the consumption of both network bandwidth and server storage space. These two aspects are seemingly quite to the opposite of each other. In this paper, we show, somewhat surprisingly, that the two aspects can actually co-exist within the same framework. This is possible fundamentally because of the following insight: The public verifiability offered by PDP/POR schemes can be naturally exploited to achieve POW. This ""one stone, two birds"" phenomenon not only inspired us to propose the novel notion of Proof of Storage with Deduplication (POSD), but also guided us to design a concrete scheme that is provably secure in the Random Oracle model based on the Computational Diffie-Hellman (CDH) assumption.","proof of storage, proof of retrievability, proof of ownership, proof of data possession, outsourced storage, integrity checking, deduplication, cloud storage","","","","","","CODASPY '12","","","","","","","","","","","","","","","","","","All Papers/Z/Zheng and Xu 2012 - Secure and efficient proof of storage with deduplication.pdf","","","",""
"Journal article","Yu GH","","Research on mobile internet big data detecting method for the redundant data","Int. J. Internet Protoc. Technol.","International Journal of Internet Protocol Technology","","","","","","2018","11","1","29-37","All","Redundant Data;Secure Dedup","","","Inderscience Publishers (IEL)","","","","","2018","","","","","","","https://pdfs.semanticscholar.org/4810/64c9f75cf01d138c3843c97ba22314a45aca.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu 2018 - Research on mobile internet big data detecting method for the redundant data.pdf","","","",""
"Journal article","Pietro RD,Di Pietro R,Sorniotti A","","Boosting efficiency and security in proof of ownership for deduplication","Proceedings of the 7th ACM Symposium on Information, Computer and Communications Security - ASIACCS '12","","","","","","","2012","","","","All;Thesis","PoW;Secure Dedup","","","","","","","","2012","","","","","","","http://dx.doi.org/10.1145/2414456.2414504","10.1145/2414456.2414504","","","","Deduplication is a technique used to reduce the amount of storage needed by service providers. It is based on the intuition that several users may want (for different reasons) to store the same content. Hence, storing a single copy of these files is sufficient. Albeit simple in theory, the imple-mentation of this concept introduces many security risks. In this paper we address the most severe one: an adversary (who possesses only a fraction of the original file, or even just partially colluding with a rightful owner) claiming to possess such a file. The paper’s contributions are manifold: first, we introduce a novel Proof of Ownership (POW) scheme that has all features of the state-of-the-art solution while incurring only a fraction of the overhead experienced by the competitor; second, the security of the proposed mechanisms relies on information theoretical (combinatoric) rather than computational assumptions; we also propose viable optimization techniques that further improve the scheme’s performance. Finally, the quality of our proposal is supported by extensive benchmarking.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Pietro et al. 2012 - Boosting efficiency and security in proof of ownership for deduplication.pdf","","","",""
"Conference paper","Yuan J,Yu S","","Secure and constant cost public cloud storage auditing with deduplication","","","","","2013 IEEE Conference on Communications and Network Security (CNS)","","","2013","","","145-153","All;Thesis","PoW;Secure Dedup","","","","","","","","2013-10","","","","","","","http://dx.doi.org/10.1109/CNS.2013.6682702;https://ieeexplore.ieee.org/abstract/document/6682702/;https://eprint.iacr.org/2013/149.pdf","10.1109/CNS.2013.6682702","","","","Data integrity and storage efficiency are two important requirements for cloud storage. Proof of Retrievability (POR) and Proof of Data Possession (PDP) techniques assure data integrity for cloud storage. Proof of Ownership (POW) improves storage efficiency by securely removing unnecessarily duplicated data on the storage server. However, trivial combination of the two techniques, in order to achieve both data integrity and storage efficiency, results in non-trivial duplication of metadata (i.e., authentication tags), which contradicts the objectives of POW. Recent attempts to this problem introduce tremendous computational and communication costs and have also been proven not secure. It calls for a new solution to support efficient and secure data integrity auditing with storage deduplication for cloud storage. In this paper we solve this open problem with a novel scheme based on techniques including polynomial-based authentication tags and homomorphic linear authenticators. Our design allows deduplication of both files and their corresponding authentication tags. Data integrity auditing and storage deduplication are achieved simultaneously. Our proposed scheme is also characterized by constant realtime communication and computational cost on the user side. Public auditing and batch auditing are both supported. Hence, our proposed scheme outperforms existing POR and PDP schemes while providing the additional functionality of deduplication. We prove the security of our proposed scheme based on the Computational Diffie-Hellman problem, the Static Diffie-Hellman problem and the t-Strong Diffie-Hellman problem. Numerical analysis and experimental results on Amazon AWS show that our scheme is efficient and scalable.","cloud computing;data integrity;file servers;meta data;security of data;storage management;secure cost public cloud storage auditing;constant cost public cloud storage auditing;storage deduplication;data integrity auditing;storage efficiency;proof of retrievability;POR;proof of data possession;PDP technique;proof of ownership;POW;storage server;metadata nontrivial duplication;polynomial-based authentication tags;homomorphic linear authenticator;constant realtime communication;computational cost;public auditing;batch auditing;computational Diffie-Hellman problem;static Diffie-Hellman problem;t-strong Diffie-Hellman problem;Amazon AWS;Servers;Cloud computing;Authentication;Public key;Polynomials;Computational efficiency","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yuan and Yu 2013 - Secure and constant cost public cloud storage auditing with deduplication.pdf","","","",""
"Miscellaneous","Bolosky WJ,Douceur JR,Ely D,Theimer M","","Feasibility of a serverless distributed file system deployed on an existing set of desktop PCs","ACM SIGMETRICS Performance Evaluation Review","","","","","","","2000","28","1","34-43","All;Thesis","CE/MLE;Secure Dedup","","","","","","","","2000","","","","","","","http://dx.doi.org/10.1145/345063.339345","10.1145/345063.339345","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bolosky et al. 2000 - Feasibility of a serverless distributed file system deployed on an existing set of desktop PCs.pdf","","","",""
"Conference paper","Douceur JR,Adya A,Bolosky WJ,Simon P,Theimer M","","Reclaiming space from duplicate files in a serverless distributed file system","","","","","Proceedings 22nd International Conference on Distributed Computing Systems","","","2002","","","617-624","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","CE/MLE;Secure Dedup;Distributed File Systems","","","","","","","","2002-07","","","","","1063-6927","","http://dx.doi.org/10.1109/ICDCS.2002.1022312;https://ieeexplore.ieee.org/abstract/document/1022312/;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2002-30.pdf","10.1109/ICDCS.2002.1022312","","","","The Farsite distributed file system provides availability by replicating each file onto multiple desktop computers. Since this replication consumes significant storage space, it is important to reclaim used space where possible. Measurement of over 500 desktop file systems shows that nearly half of all consumed space is occupied by duplicate files. We present a mechanism to reclaim space from this incidental duplication to make it available for controlled file replication. Our mechanism includes: (1) convergent encryption, which enables duplicate files to be coalesced into the space of a single file, even if the files are encrypted with different users' keys; and (2) SALAD, a Self-Arranging Lossy Associative Database for aggregating file content and location information in a decentralized, scalable, fault-tolerant manner. Large-scale simulation experiments show that the duplicate-file coalescing system is scalable, highly effective, and fault-tolerant.","network operating systems;software fault tolerance;storage management;replicated databases;cryptography;content-addressable storage;self-organising storage;Farsite;availability;storage space reclamation;desktop file systems;duplicate files;controlled file replication;convergent encryption;SALAD;file content aggregation;Self-Arranging Lossy Associative Database;location information;decentralized scalable system;fault-tolerant system;large-scale simulation;duplicate-file coalescing system;serverless distributed file system;File systems;Cryptography;Extraterrestrial measurements;Databases;Large-scale systems;File servers;Availability;Fault diagnosis;Secure storage;Distributed computing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Douceur et al. 2002 - Reclaiming space from duplicate files in a serverless distributed file system.pdf","","","",""
"Conference paper","Xia W,Zhou Y,Jiang H,Feng D,Hua Y,Hu Y,Liu Q,Zhang Y","","Fastcdc: a fast and efficient content-defined chunking approach for data deduplication","","","","","2016 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 16)","","","2016","","","101-114","All;Superchunking Paper Sources/CDC/Chunking","Chunking","","","","","","","","2016","","2020-01-15","9781931971300","","","","https://www.usenix.org/node/196197;https://www.usenix.org/system/files/conference/atc16/atc16-paper-xia.pdf","","","","","Content-Defined Chunking (CDC) has been playing a key role in data deduplication systems in the past 15 years or so due to its high redundancy detection ability. However, existing CDC-based approaches introduce heavy CPU overhead because they declare the chunk cutpoints by computing and judging the rolling hashes of the data stream byte by byte. In this paper, we propose FastCDC, a Fast and efficient CDC approach, that builds and improves on the latest Gear-based CDC approach, one of the fastest CDC methods to our knowledge. The key idea behind FastCDC is the combined use of three key techniques, namely, simplifying and enhancing the hash judgment to address our observed challenges facing Gear-based CDC, skipping sub-minimum chunk cut-point to further speed up CDC, and normalizing the chunk-size distribution in a small specified region to address the problem of the decreased deduplication ratio stemming from the cut-point skipping. Our evaluation results show that, by using a combination of the three techniques, FastCDC is about 10× faster than the best of open-source Rabin-based CDC, and about 3× faster than the state-of-the-art Gear- and AE-based CDC, while achieving nearly the same dedup","","25 min Video Presentation at:&nbsp;<a href=""https://www.usenix.org/node/196197"">https://www.usenix.org/node/196197</a>","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2016 - Fastcdc - a fast and efficient content-defined chunking approach for data deduplication.pdf","","","",""
"Conference paper","Halevi S,Harnik D,Pinkas B,Shulman-Peleg A","","Proofs of ownership in remote storage systems","","","","","Proceedings of the 18th ACM conference on Computer and communications security","","","2011","","","491-500","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","PoW","ACM","","Association for Computing Machinery","New York, NY, USA","","Chicago, Illinois, USA","","2011-10-17","","","9781450309486","","","","https://doi.org/10.1145/2046707.2046765;http://dx.doi.org/10.1145/2046707.2046765;https://dl.acm.org/citation.cfm?id=2046765;http://www.eng.tau.ac.il/semcom/2013/Students%20Presentations/Eli%20Haim%20-%20PoW.pdf","10.1145/2046707.2046765","","","","Cloud storage systems are becoming increasingly popular. A promising technology that keeps their cost down is deduplication, which stores only a single copy of repeating data. Client-side deduplication attempts to identify deduplication opportunities already at the client and save the bandwidth of uploading copies of existing files to the server. In this work we identify attacks that exploit client-side deduplication, allowing an attacker to gain access to arbitrary-size files of other users based on a very small hash signatures of these files. More specifically, an attacker who knows the hash signature of a file can convince the storage service that it owns that file, hence the server lets the attacker download the entire file. (In parallel to our work, a subset of these attacks were recently introduced in the wild with respect to the Dropbox file synchronization service.) To overcome such attacks, we introduce the notion of proofs-of-ownership (PoWs), which lets a client efficiently prove to a server that that the client holds a file, rather than just some short information about it. We formalize the concept of proof-of-ownership, under rigorous security definitions, and rigorous efficiency requirements of Petabyte scale storage systems. We then present solutions based on Merkle trees and specific encodings, and analyze their security. We implemented one variant of the scheme. Our performance measurements indicate that the scheme incurs only a small overhead compared to naive client-side deduplication.","proofs of ownership, merkle trees, deduplication, cloud storage","","","","","","CCS '11","","","","","","","","","","","","","","","","","","All Papers/H/Halevi et al. 2011 - Proofs of ownership in remote storage systems.pdf","","","",""
"Conference paper","Keelveedhi S,Bellare M,Ristenpart T","","DupLESS: server-aided encryption for deduplicated storage","","","","","Presented as part of the 22nd Usenix Security Symposium (Usenix) Security 13)","","","2013","","","179-194","All;Grouped by Publication/Usenix Security;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Secure Dedup","","","","","","","","2013","","2020-01-15","9781931971034","","","","https://www.usenix.org/conference/usenixsecurity13/technical-sessions/presentation/bellare;https://www.usenix.org/system/files/conference/usenixsecurity13/sec13-paper_bellare.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Keelveedhi et al. 2013 - DupLESS - server-aided encryption for deduplicated storage.pdf","","","",""
"Journal article","Bjørner N,Blass A,Gurevich Y","","Content-dependent chunking for differential compression, the local maximum approach","J. Comput. System Sci.","Journal of Computer and System Sciences","","","","","","2010","76","3","154-203","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Chunking","","","","","","","","2010-05-01","","","","","0022-0000","","https://www.sciencedirect.com/science/article/pii/S0022000009000580;http://dx.doi.org/10.1016/j.jcss.2009.06.004","10.1016/j.jcss.2009.06.004","","","","When a file is to be transmitted from a sender to a recipient and when the latter already has a file somewhat similar to it, remote differential compression seeks to determine the similarities interactively so as to transmit only the part of the new file not already in the recipient's old file. Content-dependent chunking means that the sender and recipient chop their files into chunks, with the cutpoints determined by some internal features of the files, so that when segments of the two files agree (possibly in different locations within the files) the cutpoints in such segments tend to be in corresponding locations, and so the chunks agree. By exchanging hash values of the chunks, the sender and recipient can determine which chunks of the new file are absent from the old one and thus need to be transmitted. We propose two new algorithms for content-dependent chunking, and we compare their behavior, on random files, with each other and with previously used algorithms. One of our algorithms, the local maximum chunking method, has been implemented and found to work better in practice than previously used algorithms. Theoretical comparisons between the various algorithms can be based on several criteria, most of which seek to formalize the idea that chunks should be neither too small (so that hashing and sending hash values become inefficient) nor too large (so that agreements of entire chunks become unlikely). We propose a new criterion, called the slack of a chunking method, which seeks to measure how much of an interval of agreement between two files is wasted because it lies in chunks that don't agree. Finally, we show how to efficiently find the cutpoints for local maximum chunking.","Networking; Distributed file systems; Compression; Ergodic theory","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bjørner et al. 2010 - Content-dependent chunking for differential compression, the local maximum approach.pdf","","","",""
"Website","NA,GH,Shigeo Mitsunari Cybozu Labs, Inc. , Tokyo, Japan,YS,Kana Shimizu Waseda University, Tokyo, Japan,TT","","Efficient Two-level Homomorphic Encryption in Prime-order Bilinear Groups and A Fast Implementation in WebAssembly | Proceedings of the 2018 on Asia Conference on Computer and Communications Security","","","","","","","","","","","","All","Sec Techniques/Protocols","","","","","","","","","","","","","","","https://dl.acm.org/doi/10.1145/3196494.3196552;http://dx.doi.org/10.1145/3196494.3196552","10.1145/3196494.3196552","","","","","","Paper for the github implementation at <a href=""https://github.com/herumi/mcl"">https://github.com/herumi/mcl</a>","","","en","","","","","","","","","","","","","","","","","","","","All Papers/N/NA et al. - Efficient Two-level Homomorphic Encryption in Prime-order Bi ... f the 2018 on Asia Conference on Computer and Communications Security.pdf","","","",""
"Miscellaneous","Cramer R,Gennaro R,Schoenmakers B","","A Secure and Optimally Efficient Multi-Authority Election Scheme","Advances in Cryptology — EUROCRYPT ’97","","","","","","","1997","","","103-118","All","Sec Techniques/Protocols","","","","","","","","1997","","","","","","","http://dx.doi.org/10.1007/3-540-69053-0_9","10.1007/3-540-69053-0_9","","","","","","First literature on Lifted ElGamal? Also uses shared key for private key of Lifted ElGamal. Also&nbsp;Proofs of knowledge for equality of discrete logs","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cramer et al. 1997 - A Secure and Optimally Efficient Multi-Authority Election Scheme.pdf","","","",""
"Conference paper","Xu J,Chang EC,Zhou J","","Weak leakage-resilient client-side deduplication of encrypted data in cloud storage","","","","","Proceedings of the 8th ACM SIGSAC symposium on Information, computer and communications security","","","2013","","","195-206","All;Thesis","PoW","ACM","","Association for Computing Machinery","New York, NY, USA","","Hangzhou, China","","2013-05-08","","","9781450317672","","","","https://doi.org/10.1145/2484313.2484340;http://dx.doi.org/10.1145/2484313.2484340;https://dl.acm.org/citation.cfm?id=2484340;https://www.comp.nus.edu.sg/~changec/publications/2013_asiaccs.pdf","10.1145/2484313.2484340","","","","Recently, Halevi et al. (CCS '11) proposed a cryptographic primitive called proofs of ownership (PoW) to enhance security of client-side deduplication in cloud storage. In a proof of ownership scheme, any owner of the same file F can prove to the cloud storage that he/she owns file F in a robust and efficient way, in the bounded leakage setting where a certain amount of efficiently-extractable information about file F is leaked. Following this work, we propose a secure client-side deduplication scheme, with the following advantages: our scheme protects data confidentiality (and some partial information) against both outside adversaries and honest-but-curious cloud storage server, while Halevi et al. trusts cloud storage server in data confidentiality; our scheme is proved secure w.r.t. any distribution with sufficient min-entropy, while Halevi et al. (the last and the most practical construction) is particular to a specific type of distribution (a generalization of ""block-fixing"" distribution) of input files.The cost of our improvements is that we adopt a weaker leakage setting: We allow a bounded amount one-time leakage of a target file before our scheme starts to execute, while Halevi et al. allows a bounded amount multi-time leakage of the target file before and after their scheme starts to execute. To the best of our knowledge, previous works on client-side deduplication prior Halevi et al. do not consider any leakage setting.","client-side deduplication, cloud storage, leakage-resilient, privacy, proofs of ownership, universal hash","Proof in Random Oracle but does not address low min-entropy files.","","","","","ASIA CCS '13","","","","","","","","","","","","","","","","","","All Papers/X/Xu et al. 2013 - Weak leakage-resilient client-side deduplication of encrypted data in cloud storage.pdf","","","",""
"Journal article","Wen M,Ota K,Li H,Lei J,Gu C,Su Z","","Secure Data Deduplication With Reliable Key Management for Dynamic Updates in CPSS","IEEE Transactions on Computational Social Systems","","","","","","","2015","2","4","137-147","All","Key Management","","","","","","","","2015-12","","","","","2373-7476","","http://dx.doi.org/10.1109/TCSS.2015.2514088;https://ieeexplore.ieee.org/abstract/document/7398002/;https://ieeexplore.ieee.org/iel7/6570650/6780646/07398002.pdf","10.1109/TCSS.2015.2514088","","","","With the increasing sensing and communication in cyber physical social system (CPSS), the data volume is growing much rapidly in recent years. Secure deduplication has attracted considerable interests of storage provider for data management efficiency and data privacy preserving. One of the most challenging issues in secure deduplication is how to manage data and the convergent key when users frequently update it. To solve this problem, D. Koo et al. use bilinear paring as the key method. However, bilinear paring requires high computation cost for implementations. In this paper, we propose a session-key-based convergent key management scheme, named SKC, to secure the dynamic update in the data deduplication. Specifically, each data owner in SKC can verify the correctness of the session key and dynamically change it with the data update. Furthermore, to enable group combination and remove the aid of gateway (GW), a convergent key sharing scheme, named CKS, is presented. Security analysis demonstrates that both SKC and CKS can protect the confidentiality of the data and the convergent key in the case of dynamic updates. The simulation results show that our SKC and CKS can significantly reduce computation complexity and communication during the data uploading phase.","cloud computing;computational complexity;cyber-physical systems;data protection;internetworking;private key cryptography;secure data deduplication;reliable key management;CPSS;cyber physical social system;data volume;data management efficiency;data privacy preservation;data management;bilinear paring;computation cost;session-key-based convergent key management scheme;SKC;dynamic update security;session key correctness;data update;group combination;gateway;GW;convergent key sharing scheme;CKS;data confidentiality protection;computation complexity reduction;communication reduction;data uploading phase;Encryption;Cyber-physical systems;Cloud computing;Data privacy;Computational complexity;Convergent encryption;CPSS;data deduplication;dynamic update;key management;Convergent encryption;CPSS;data deduplication;dynamic update;key management","What's CPSS (cyber physical social system) mean?<div>Also - Dynamic key mgmt for cross-user data deduplication using SKC Session Key-Based Convergent Key and CKS Convergent Key Sharing scheme.</div>","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wen et al. 2015 - Secure Data Deduplication With Reliable Key Management for Dynamic Updates in CPSS.pdf","","","",""
"Journal article","Li J,Chen X,Huang X,Tang S,Xiang Y,Hassan MM,Alelaiwi A","","Secure Distributed Deduplication Systems with Improved Reliability","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2015","64","12","3569-3579","All;Grouped by Publication/IEEE Transactions on Computers","Fog/Multicloud","","","","","","","","2015-12","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2015.2401017;https://ieeexplore.ieee.org/abstract/document/7035033/;https://ieeexplore.ieee.org/iel7/12/4358213/07035033.pdf","10.1109/TC.2015.2401017","","","","Data deduplication is a technique for eliminating duplicate copies of data, and has been widely used in cloud storage to reduce storage space and upload bandwidth. However, there is only one copy for each file stored in cloud even if such a file is owned by a huge number of users. As a result, deduplication system improves storage utilization while reducing reliability. Furthermore, the challenge of privacy for sensitive data also arises when they are outsourced by users to cloud. Aiming to address the above security challenges, this paper makes the first attempt to formalize the notion of distributed reliable deduplication system. We propose new distributed deduplication systems with higher reliability in which the data chunks are distributed across multiple cloud servers. The security requirements of data confidentiality and tag consistency are also achieved by introducing a deterministic secret sharing scheme in distributed storage systems, instead of using convergent encryption as in previous deduplication systems. Security analysis demonstrates that our deduplication systems are secure in terms of the definitions specified in the proposed security model. As a proof of concept, we implement the proposed systems and demonstrate that the incurred overhead is very limited in realistic environments.","cloud computing;data integrity;security of data;secure distributed deduplication systems;data deduplication;data chunks;multiple cloud servers;security requirements;data confidentiality;tag consistency;deterministic secret sharing scheme;distributed storage systems;security analysis;Servers;Cryptography;Storage automation;Cloud computing;Distributed databases;Deduplication;distributed storage system;reliability;secret sharing;Deduplication;distributed storage system;reliability;secret sharing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2015 - Secure Distributed Deduplication Systems with Improved Reliability.pdf","","","",""
"Journal article","Chen R,Mu Y,Yang G,Guo F","","BL-MLE: Block-Level Message-Locked Encryption for Secure Large File Deduplication","IEEE Trans. Inf. Forensics Secur.","IEEE Transactions on Information Forensics and Security","","","","","","2015","10","12","2643-2652","All","CE/MLE;Sec Techniques/Protocols","","","","","","","","2015-12","","","","","1556-6021","","http://dx.doi.org/10.1109/TIFS.2015.2470221;https://ieeexplore.ieee.org/abstract/document/7210226/;https://ieeexplore.ieee.org/iel7/10206/4358835/07210226.pdf","10.1109/TIFS.2015.2470221","","","","Deduplication is a popular technique widely used to save storage spaces in the cloud. To achieve secure deduplication of encrypted files, Bellare et al. formalized a new cryptographic primitive named message-locked encryption (MLE) in Eurocrypt 2013. Although an MLE scheme can be extended to obtain secure deduplication for large files, it requires a lot of metadata maintained by the end user and the cloud server. In this paper, we propose a new approach to achieve more efficient deduplication for (encrypted) large files. Our approach, named block-level message-locked encryption (BL-MLE), can achieve file-level and block-level deduplication, block key management, and proof of ownership simultaneously using a small set of metadata. We also show that our BL-MLE scheme can be easily extended to support proof of storage, which makes it multi-purpose for secure cloud storage.","cryptography;meta data;BL-MLE scheme;block-level message-locked encryption;secure large file deduplication;Eurocrypt 2013;metadata;file-level;block-level deduplication;block key management;secure cloud storage;Servers;Encryption;Maximum likelihood estimation;Cloud computing;Message-locked encryption;deduplication;proof of ownership;proof of storage;Message-locked encryption;deduplication;proof of ownership;proof of storage","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chen et al. 2015 - BL-MLE - Block-Level Message-Locked Encryption for Secure Large File Deduplication.pdf","","","",""
"Journal article","Huang Q,Yang Y,Yue W,He Y","","Secure Data Group Sharing and Conditional Dissemination with Multi-Owner in Cloud Computing","IEEE Transactions on Cloud Computing","","","","","","","2019","","","1-1","All;Grouped by Publication/IEEE Trans Cloud Computing","Ownership Management","","","ieeexplore.ieee.org","","","","","2019","","","","","2372-0018","","http://dx.doi.org/10.1109/TCC.2019.2908163;https://ieeexplore.ieee.org/abstract/document/8676353/;https://ieeexplore.ieee.org/iel7/6245519/6562694/08676353.pdf","10.1109/TCC.2019.2908163","","","","With the rapid development of cloud services, huge volume of data is shared via cloud computing. Although cryptographic techniques have been utilized to provide data confidentiality in cloud computing, current mechanisms cannot enforce privacy concerns over ciphertext associated with multiple owners, which makes co-owners unable to appropriately control whether data disseminators can actually disseminate their data. In this paper, we propose a secure data group sharing and conditional dissemination scheme with multi-owner in cloud computing, in which data owner can share private data with a group of users via the cloud in a secure way, and data disseminator can disseminate the data to a new group of users if the attributes satisfy the access policies in the ciphertext. We further present a multiparty access control mechanism over the disseminated ciphertext, in which the data co-owners can append new access policies to the ciphertext due to their privacy preferences. Moreover, three policy aggregation strategies, including full permit, owner priority and majority permit, are provided to solve the privacy conflicts problem caused by different access policies. The security analysis and experimental results show our scheme is practical and efficient for secure data sharing with multi-owner in cloud computing.","Cloud computing;Privacy;Data privacy;Access control;Encryption;Data sharing;cloud computing;conditional proxy re-encryption;attribute-based encryption;privacy conflict","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Huang et al. 2019 - Secure Data Group Sharing and Conditional Dissemination with Multi-Owner in Cloud Computing.pdf","","","",""
"Journal article","Verma N,Singh D","","Deduplication in Encrypted Data: A Comprehensive Review","ijcem.org","","","","","","","","","","","All","Survey","","","","","","","","","","","","","","","http://www.ijcem.org/papers052018/ijcem_052018_02.pdf","","","","","Cloud computing provides several facilities to the users amongst which the most prevalent is data storage. The users are permitted to download or upload their data anywhere and anytime to the server. But retrieving data leads to several issues related to the confidentiality …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Verma and Singh - Deduplication in Encrypted Data - A Comprehensive Review.pdf","","","",""
"Journal article","Zhao Y,Chow SS","","Updatable Block-Level Message-Locked Encryption","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","2019","","","1-1","All","CE/MLE;Sec Techniques/Protocols","","","ieeexplore.ieee.org","","","","","2019","","","","","2160-9209","","http://dx.doi.org/10.1109/TDSC.2019.2922403;https://ieeexplore.ieee.org/abstract/document/8735832/;https://ieeexplore.ieee.org/iel7/8858/4358699/08735832.pdf","10.1109/TDSC.2019.2922403","","","","Deduplication is widely used for reducing the storage requirement for storage service providers. Nevertheless, it is unclear how to support deduplication of encrypted data securely until the study of Bellare et. al. on message-locked encryption (MLE, Eurocrypt 2013). While updating (shared) files is natural, existing MLE solutions do not allow efficient update of encrypted files stored remotely. Even modifying a single bit requires the expensive way of downloading and decrypting a large ciphertext (then re-uploading). This paper initiates the study of updatable block-level MLE, a new primitive in incremental cryptography and cloud cryptography. Our proposed provably-secure construction is updatable with computation cost logarithmic in the file size. It naturally supports block-level deduplication. It also supports proof-of-ownership which protects storage providers from being abused as a free content distribution network. Our experiments show its practical performance relative to the original MLE and existing non-updatable block-level MLE.","Encryption;Maximum likelihood estimation;Servers;Cloud computing;Protocols;Message-Locked Encryption;Proofs-of-Ownership;Deduplication;Hash Function;Incremental Cloud Cryptography","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhao and Chow 2019 - Updatable Block-Level Message-Locked Encryption.pdf","","","",""
"Journal article","Zhou Y,Feng D,Hua Y,Xia W,Fu M,Huang F,et al.","","A similarity-aware encrypted deduplication scheme with flexible access control in the cloud","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2018","","","","All;Superchunking Paper Sources/CDC/Chunking","Chunking;Similarity/Resemblance","","","Elsevier","","","","","2018","","","","","0167-739X","","https://www.sciencedirect.com/science/article/pii/S0167739X17309238;https://scholar.google.ca/scholar?cluster=11757571199296928319&hl=en&as_sdt=0,5&sciodt=0,5","","","","","Data deduplication has been widely used in the cloud to reduce storage space. To protect data security, users encrypt data with message-locked encryption (MLE) to enable deduplication over ciphertexts. However, existing secure deduplication schemes suffer from security weakness (i.e., brute-force attacks) and fail to support flexible access control. The process of chunk-level MLE key generation and sharing exists potential privacy issues and heavy computation consumption. We propose EDedup, a similarity-aware encrypted deduplication scheme that supports flexible access control with revocation. Specifically, EDedup groups files into segments and performs server-aided MLE at segment-level, which exploits similarity via a representative hash (e.g., the min-hash) to reduce computation consumption. This nevertheless faces a new attack that an attacker gets keys by guessing the representative hash. And hence EDedup combines source-based similar-segment detection and targetbased duplicate-chunk checking to resist attacks and guarantee deduplication efficiency. Furthermore, EDedup generates message-derived file keys for duplicate files to manage metadata. EDedup encrypts file keys via proxy-based attribute-based encryption, which reduces metadata storage overheads and implements flexible access control with revocation. Evaluation results demonstrate that EDedup improves the speed of MLE up to 10.9X and 0.36X compared with DupLESS-chunk and SecDep respectively. EDedup reduces metadata storage overheads by 39.9%–65.7% relative to REED.","","""Segments"" here are groups of files.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2018 - A similarity-aware encrypted deduplication scheme with flexible access control in the cloud.pdf","","","",""
"Conference paper","Thakur MA,Bari S,Deshmukh R,Auty S","","Secure Key Agreement Model for Group Data Sharing and Achieving Data Deduplication in Cloud Computing","","","","","Information and Communication Technology for Sustainable Development","","","2020","","","121-127","All","Secure Dedup","","","Springer Singapore","","","","","2020","","","","","","","http://dx.doi.org/10.1007/978-981-13-7166-0_12;https://link.springer.com/chapter/10.1007/978-981-13-7166-0_12","10.1007/978-981-13-7166-0_12","","","","Safe along with sound knowledge deduplication will noticeably cut back the communication and storage outlay in server storage space services, and has potential application in our huge data-driven society. Existing knowledge deduplication scheme area unit naturally intends to also resist brute-force attacks or confirm the strength and knowledge convenience. In the existing system, key written agreement is a drawback. In the proposed system, to get rid of key written agreement drawback, we tend to implement block style based mostly on key agreement protocol to share knowledge in CSP. It allows multiple partners to freely distribute information in cluster. The proposed system is a chunk base-type contract procedure to wire numerous partners, which might supplely expand to amount of partners inside very CSP setting as per the construction of the chunk style, and to remove knowledge redundancy drawback we tend to use knowledge deduplication system within which knowledge owner can transfer file and send to cluster manager and cluster manager checks knowledge deduplication over native domain. In this, the approved person transfers knowledge over cloud environment to transfer file, and knowledge owner can send key request to key authority for secret key. Once the key is received from key authority owner can transfer file and send to cluster manager and check file deduplication on native domain and if file isn’t offered on native domain then send file to cloud. During access, the user can send key request to any or all cluster members and after receiving the key from cluster member, file can be transferred.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Thakur et al. 2020 - Secure Key Agreement Model for Group Data Sharing and Achieving Data Deduplication in Cloud Computing.pdf","","","",""
"Journal article","Hur J,Koo D,Shin Y,Kang K","","Secure Data Deduplication with Dynamic Ownership Management in Cloud Storage","IEEE Trans. Knowl. Data Eng.","IEEE transactions on knowledge and data engineering","","","","","","2016","28","11","3113-3125","All","Ownership Management","","","","","","","","2016-11","","","","","1041-4347","2326-3865","http://dx.doi.org/10.1109/TKDE.2016.2580139;https://ieeexplore.ieee.org/abstract/document/7490366/;http://www.projectsgoal.com/download_projects/cloud-computing/cloud-computing-projects-GCC00070.pdf","10.1109/TKDE.2016.2580139","","","","In cloud storage services, deduplication technology is commonly used to reduce the space and bandwidth requirements of services by eliminating redundant data and storing only a single copy of them. Deduplication is most effective when multiple users outsource the same data to the cloud storage, but it raises issues relating to security and ownership. Proof-of-ownership schemes allow any owner of the same data to prove to the cloud storage server that he owns the data in a robust way. However, many users are likely to encrypt their data before outsourcing them to the cloud storage to preserve privacy, but this hampers deduplication because of the randomization property of encryption. Recently, several deduplication schemes have been proposed to solve this problem by allowing each owner to share the same encryption key for the same data. However, most of the schemes suffer from security flaws, since they do not consider the dynamic changes in the ownership of outsourced data that occur frequently in a practical cloud storage service. In this paper, we propose a novel server-side deduplication scheme for encrypted data. It allows the cloud server to control access to outsourced data even when the ownership changes dynamically by exploiting randomized convergent encryption and secure ownership group key distribution. This prevents data leakage not only to revoked users even though they previously owned that data, but also to an honest-but-curious cloud storage server. In addition, the proposed scheme guarantees data integrity against any tag inconsistency attack. Thus, security is enhanced in the proposed scheme. The efficiency analysis results demonstrate that the proposed scheme is almost as efficient as the previous schemes, while the additional computational overhead is negligible.","authorisation;cloud computing;cryptography;storage management;data deduplication security;dynamic ownership management;cloud storage services;space reduction;bandwidth reduction;proof-of-ownership schemes;data encryption;encryption randomization property;access control;data leakage;Cloud computing;Encryption;Servers;Data privacy;Electronic mail;Deduplication;cloud storage;encryption;proof-of-ownership;revocation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Hur et al. 2016 - Secure Data Deduplication with Dynamic Ownership Management in Cloud Storage.pdf","","","",""
"Journal article","Tian G,Ma H,Xie Y,Liu Z","","Randomized deduplication with ownership management and data sharing in cloud storage","Journal of Information Security and Applications","","","","","","","2020","51","","102432","All","Secure Dedup;Ownership Management","","","","","","","","2020-04-01","","","","","2214-2126","","http://www.sciencedirect.com/science/article/pii/S2214212618307737;http://dx.doi.org/10.1016/j.jisa.2019.102432","10.1016/j.jisa.2019.102432","","","","In commercial cloud services, the client-side deduplication is widely used to save the system resource of servers. However, this kind of deduplication technique is vulnerable to the collusive authentication attack, brute-force attack and duplicate-faking attack. Most existing schemes cannot resolve those problems efficiently. Besides, how to realize the ownership management in client-side deduplication to ensure the forward and backward secrecy of outsourced data is also a hot issue. In this paper, we propose a randomized client-side deduplication scheme, which uses a randomized deduplication protocol to prevent the collusive authentication attack and offline brute-force attack launched by the outside adversaries, and stores each data according to two file tags to resist duplicate-faking attack. In addition, we realize a more available ownership management and data sharing with the aid of dynamic Key-Encrypting Key tree. Security and performance analysis show that the proposed scheme can achieve the desired security requirements while saving the system resource efficiently.","Data deduplication; Ownership management; Dynamic key-encrypting key tree; Data sharing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tian et al. 2020 - Randomized deduplication with ownership management and data sharing in cloud storage.pdf","","","",""
"Journal article","Anderson P,Zhang L","","Fast and secure laptop backups with encrypted DE-duplication","USENIX Large Install Syst Adm Conf","LiSA","","","","","","2010","","","1-8","All;Thesis;p-scailbib","Secure Dedup;CE/MLE","","","","","","","","2010-11-07","","","","","1340-8836","","","","","","","Many people now store large quantities of personal and corporate data on laptops or home computers. These often have poor or intermittent connectivity, and are vulnerable to theft or hardware failure. Conventional backup solutions are not well suited to this environment, and backup regimes are frequently inadequate. This paper describes an algorithm which takes advantage of the data which is common between users to increase the speed of backups, and reduce the storage requirements. This algorithm supports client-end per-user encryption which is necessary for confidential personal data. It also supports a unique feature which allows immediate detection of common subtrees, avoiding the need to query the backup system for every file. We describe a prototype implementation of this algorithm for Apple OS X, and present an analysis of the potential effectiveness, using real data obtained from a set of typical users. Finally, we discuss the use of this prototype in conjunction with remote cloud storage, and present an analysis of the typical cost savings.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Anderson and Zhang 2010 - Fast and secure laptop backups with encrypted DE-duplication.pdf","","","",""
"Conference paper","Bing Wang,Wenjing Lou,Hou YT","","Modeling the side-channel attacks in data deduplication with game theory","","","","","2015 IEEE Conference on Communications and Network Security (CNS)","","","2015","","","200-208","All","Side Channels;Security Proof","","","","","","","","2015-09","","","","","","","http://dx.doi.org/10.1109/CNS.2015.7346829;https://ieeexplore.ieee.org/abstract/document/7346829/;https://ieeexplore.ieee.org/iel7/7336674/7346791/07346829.pdf","10.1109/CNS.2015.7346829","","","","The cross-user data deduplication improves disk space efficiency of cloud storage by keeping only one copy of same files among all service users. As a result, the cloud storage service is able to offer a considerable amount of storage at an attractive price. Therefore, people begin to use cloud storage such as Dropbox and Google Drive not only as data backup but also as their primary storage for everyday usage. However, the cross-user data deduplication also rises data privacy concerns. A side-channel attack called “confirmation-of-a-file” and its variant “learn-the-remaining-information” breach the user data privacy through observing the deduplication operation of the cloud storage server. These attacks allow malicious users to pinpoint specific files if they exist in the cloud. The existing solutions sacrifice either the network bandwidth efficiency or the storage efficiency to defend the side-channel attacks without analyzing the defensive cost from the standpoint of cloud storage providers. Because profit is the key factor that motivates cloud service providers, the question that how to defend the side-channel attacks efficiently in terms of cost is not only important but also practical. However, this question remains unanswered. In this paper, we try to address this problem using game theory. We model the interaction between the attacker and the defender, i.e., the cloud storage server, using a game-theoretic framework. Our framework captures underlying complexity of the side-channel attack problem under several practical assumptions. We prove there exists a unique solution of the problem, i.e., a mixed-strategy Nash equilibrium. Our simulation results show the efficiency of our scheme.","cloud computing;cryptography;data privacy;game theory;storage management;side-channel attacks;data deduplication;game theory;cloud storage;data privacy;cloud service providers;Cloud computing;Servers;Encryption;Game theory;Data privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bing Wang et al. 2015 - Modeling the side-channel attacks in data deduplication with game theory.pdf","","","",""
"Journal article","Yu C,Gochhayat SP,Conti M,Lu C","","Privacy Aware Data Deduplication for Side Channel in Cloud Storage","IEEE Transactions on Cloud Computing","","","","","","","2018","","","1-1","All;Grouped by Publication/IEEE Trans Cloud Computing;Thesis","Side Channels","","","","","","","","2018","","","","","2372-0018","","http://dx.doi.org/10.1109/TCC.2018.2794542;https://ieeexplore.ieee.org/document/8260900/computer.org/computer.org/tcc","10.1109/TCC.2018.2794542","","","","Cloud storage services enable individuals and organizations to outsource data storage to remote servers. Cloud storage providers generally adopt data deduplication, a technique for eliminating redundant data by keeping only a single copy of a file, thus saving a considerable amount of storage and bandwidth. However, an attacker can abuse deduplication protocols to steal information. For example, an attacker can perform the duplicate check to verify whether a file (e.g., a pay slip, with a specific name and salary amount) is already stored (by someone else), hence breaching the user privacy. In this paper, we propose ZEUS (zero-knowledge deduplication response) framework. We develop ZEUS and ZEUS+, two privacy-aware deduplication protocols: ZEUS provides weaker privacy guarantees while being more efficient in the communication cost, while ZEUS+ guarantees stronger privacy properties, at an increased communication cost. To our knowledge, ZEUS is the first solution which addresses two-side privacy by neither using any extra hardware nor depending on heuristically chosen parameters used by the existing solutions, thus reducing both cost and complexity of the cloud storage. We show the efficiency of the proposed framework by evaluating on real dataset and comparing the communication cost of the proposed solutions, and prove the privacy.","Cloud computing;Privacy;Data privacy;Bandwidth;Organizations;Servers;Protocols;Cloud Storage;Side Channel;Data Deduplication;Privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu et al. 2018 - Privacy Aware Data Deduplication for Side Channel in Cloud Storage.pdf","","","",""
"Journal article","Shin Y,Kim K","","Differentially private client-side data deduplication protocol for cloud storage services","Security and Communication Networks","","","","","","","2015","8","12","2114-2123","All;Thesis","Side Channels;Gateway","","","Wiley Online Library","","","","","2015","","","","","","","https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.1159;https://onlinelibrary.wiley.com/doi/pdf/10.1002/sec.1159","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shin and Kim 2015 - Differentially private client-side data deduplication protocol for cloud storage services.pdf","","","",""
"Conference paper","Heen O,Neumann C,Montalvo L,Defrance S","","Improving the Resistance to Side-Channel Attacks on Cloud Storage Services","","","","","2012 5th International Conference on New Technologies, Mobility and Security (NTMS)","","","2012","","","1-5","All;Thesis;p-scailbib","Side Channels;Gateway","","","","","","","","2012-05","","","","","2157-4952","","http://dx.doi.org/10.1109/NTMS.2012.6208705;https://ieeexplore.ieee.org/abstract/document/6208705/;https://ieeexplore.ieee.org/iel5/6203453/6203739/06208705.pdf","10.1109/NTMS.2012.6208705","","","","Providers of cloud storage services usually apply deduplication across multiple user accounts in order to optimize savings of both upload bandwidth and storage space. However, deduplication can be used as a side channel by an adversary for obtaining sensitive information about other user's data. We propose a new gateway-based deduplication model that lets the storage service provider apply efficient deduplication while substantially reducing the risk of information leakage. We suppose that the cloud storage service is provided by a Network Service Provider that also ships advanced gateways to its customers. We discuss why it is much harder for an adversary to infer deduplication from the gateway than from a fully controlled host.","cloud computing;computer crime;computer network security;data privacy;information storage;internetworking;network servers;side-channel attacks;cloud storage services;multiple user;storage space;gateway-based deduplication model;storage service provider;information leakage;network service provider;fully controlled host;Logic gates;Servers;Bandwidth;Cloud computing;Security;Privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Heen et al. 2012 - Improving the Resistance to Side-Channel Attacks on Cloud Storage Services.pdf","","","",""
"Miscellaneous","Armknecht F,Boyd C,Davies GT,Gjøsteen K,Toorani M","","Side Channels in Deduplication","Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security - ASIA CCS '17","","","","","","","2017","","","","All","Side Channels","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1145/3052973.3053019","10.1145/3052973.3053019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Armknecht et al. 2017 - Side Channels in Deduplication.pdf","","","",""
"Journal article","Yu C,Chen C,Chao H","","Proof of ownership in deduplicated cloud storage with mobile device efficiency","IEEE Netw.","IEEE network","","","","","","2015","29","2","51-55","All;Thesis","PoW","","","","","","","","2015-03","","","","","0890-8044","1558-156X","http://dx.doi.org/10.1109/MNET.2015.7064903;https://ieeexplore.ieee.org/abstract/document/7064903/","10.1109/MNET.2015.7064903","","","","Cloud storage such as Dropbox and Bitcasa is one of the most popular cloud services. Currently, with the prevalence of mobile cloud computing, users can even collaboratively edit the newest version of documents and synchronize the newest files on their smart mobile devices. A remarkable feature of current cloud storage is its virtually infinite storage. To support unlimited storage, the cloud storage provider uses data deduplication techniques to reduce the data to be stored and therefore reduce the storage expense. Moreover, the use of data deduplication also helps significantly reduce the need for bandwidth and therefore improve the user experience. Nevertheless, in spite of the above benefits, data deduplication has its inherent security weaknesses. Among them, the most severe is that the adversary may have an unauthorized file downloading via the file hash only. In this article we first review the previous solutions and identify their performance weaknesses. Then we propose an alternative design that achieves cloud server efficiency and especially mobile device efficiency.","authorisation;cloud computing;mobile computing;virtual storage;poof of ownership;deduplicated cloud storage;Dropbox;Bitcasa;cloud services;mobile cloud computing;smart mobile devices;virtually infinite storage;unlimited storage;data deduplication techniques;storage expense reduction;user experience;security weaknesses;unauthorized file downloading;file hash;cloud server efficiency;mobile device efficiency;Cloud computing;5G mobile communication;Computer security;Mobile communication;Bandwidth allocation;Mobile handsets;Chaotic communication","Summarized 28/1/2020","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu et al. 2015 - Proof of ownership in deduplicated cloud storage with mobile device efficiency.pdf","","","",""
"Journal article","Yan Z,Ding W,Yu X,Zhu H,Deng RH","","Deduplication on Encrypted Big Data in Cloud","IEEE Transactions on Big Data","","","","","","","2016","2","2","138-150","All","Secure Dedup","","","","","","","","2016-06","","","","","2372-2096","","http://dx.doi.org/10.1109/TBDATA.2016.2587659;https://ieeexplore.ieee.org/abstract/document/7511769/;https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=4347&context=sis_research","10.1109/TBDATA.2016.2587659","","","","Cloud computing offers a new way of service provision by re-arranging various resources over the Internet. The most important and popular cloud service is data storage. In order to preserve the privacy of data holders, data are often stored in cloud in an encrypted form. However, encrypted data introduce new challenges for cloud data deduplication, which becomes crucial for big data storage and processing in cloud. Traditional deduplication schemes cannot work on encrypted data. Existing solutions of encrypted data deduplication suffer from security weakness. They cannot flexibly support data access control and revocation. Therefore, few of them can be readily deployed in practice. In this paper, we propose a scheme to deduplicate encrypted data stored in cloud based on ownership challenge and proxy re-encryption. It integrates cloud data deduplication with access control. We evaluate its performance based on extensive analysis and computer simulations. The results show the superior efficiency and effectiveness of the scheme for potential practical deployment, especially for big data deduplication in cloud storage.","authorisation;Big Data;cloud computing;cryptography;data privacy;storage management;encrypted Big Data;cloud computing;service provision;Internet;cloud service;data storage;privacy preservatrion;data holders;cloud data deduplication;Big Data storage;Big Data processing;ownership challenge;proxy reencryption;access control;cloud storage;Cloud computing;Big data;Data privacy;Encryption;Servers;Access control;big data;cloud computing;data deduplication;proxy re-encryption","Data ownership challenge and Proxy Re-Encryption PRE gives flexible data access control and revocation.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yan et al. 2016 - Deduplication on Encrypted Big Data in Cloud.pdf","","","",""
"Miscellaneous","Frey D,Kermarrec AM,Kloudas K","","Probabilistic deduplication for cluster-based storage systems","Proceedings of the Third ACM Symposium on Cloud Computing - SoCC '12","","","","","","","2012","","","","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing","Fingerprint-Indexing;Chunking","","","","","","","","2012","","","","","","","http://dx.doi.org/10.1145/2391229.2391246","10.1145/2391229.2391246","","","","","","Produck","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Frey et al. 2012 - Probabilistic deduplication for cluster-based storage systems.pdf","","","",""
"Miscellaneous","Kaiser J,Meister D,Brinkmann A,Effert S","","Design of an exact data deduplication cluster","012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST)","","","","","","","2012","","","","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/MSST","Fingerprint-Indexing","","","","","","","","2012","","","","","","","http://dx.doi.org/10.1109/msst.2012.6232380","10.1109/msst.2012.6232380","","","","","","dedupv1, flash based chunk index","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaiser et al. 2012 - Design of an exact data deduplication cluster.pdf","","","",""
"Book chapter","Fu Y,Jiang H,Xiao N","","A scalable inline cluster deduplication framework for big data protection","","","Lecture Notes in Computer Science","","","","","2012","","","354-373","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Super-chunking;Routing with Superchunks","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","2012","","","9783642351693","9783642351709","0302-9743","1611-3349","http://dx.doi.org/10.1007/978-3-642-35170-9_18;http://link.springer.com/10.1007/978-3-642-35170-9_18","10.1007/978-3-642-35170-9_18","","","","","","Sigma-Dedupe, handprint similarity, super-chunks","","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2012 - A scalable inline cluster deduplication framework for big data protection.pdf","","","",""
"Conference paper","Dong W,Douglis F,Li K,Patterson RH,Reddy S,Shilane P","","Tradeoffs in Scalable Data Routing for Deduplication Clusters","","","","","FAST","","","2011","11","","15-29","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/FAST Papers;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Super-chunking;Routing with Superchunks","","","","","","","","2011","","","","","","","https://www.usenix.org/legacy/events/fast11/tech/full_papers/Dong.pdf","","","","","","","Introduces superchunk term? They switch from previous paper's segment term to chunk term.","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dong et al. 2011 - Tradeoffs in Scalable Data Routing for Deduplication Clusters.pdf","","","",""
"Conference paper","Yang T,Jiang H,Feng D,Niu Z,Zhou K,Wan Y","","DEBAR: A scalable high-performance de-duplication storage system for backup and archiving","","","","","2010 IEEE International Symposium on Parallel Distributed Processing (IPDPS)","","","2010","","","1-12","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Parallel Schemes","Fingerprint-Indexing","","","","","","","","2010-04","","","","","1530-2075","","http://dx.doi.org/10.1109/IPDPS.2010.5470468;https://ieeexplore.ieee.org/abstract/document/5470468/;https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1059&context=csetechreports","10.1109/IPDPS.2010.5470468","","","","Driven by the increasing demand for large-scale and high-performance data protection, disk-based de-duplication storage has become a new research focus of the storage industry and research community where several new schemes have emerged recently. So far these systems are mainly inline de-duplication approaches, which are centralized and do not lend themselves easily to be extended to handle global de-duplication in a distributed environment. We present DEBAR, a de-duplication storage system designed to improve capacity, performance and scalability for de-duplication backup/archiving. DEBAR performs post-processing de-duplication, where backup streams are de-duplicated and cached on server-disks through an in-memory preliminary filter in phase I, and then completely de-duplicated in-batch in phase II. By decentralizing fingerprint lookup and update, DEBAR supports a cluster of servers to perform de-duplication backup in parallel, and is shown to scale linearly in both write throughput and physical capacity, achieving an aggregate throughput of 1.7GB/s and supporting a physical capacity of 2PB with 16 backup servers.","information retrieval systems;records management;storage allocation;DEBAR system;scalable high-performance de-duplication storage system;backup;archiving;high-performance data protection;storage industry;distributed environment;postprocessing de-duplication;Fingerprint recognition;Throughput;Protection;Bandwidth;Scalability;Aggregates;Large-scale systems;Computer science;Data engineering;Computer industry;backup and archive;data de-duplication;post-processing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2010 - DEBAR - A scalable high-performance de-duplication storage system for backup and archiving.pdf","","","",""
"Conference paper","Dubnicki C,Gryz L,Heldt L,Kaczmarczyk M,Kilian W,Strzelczak P,Szczepkowski J,Ungureanu C,Welnicki M","","HYDRAstor: A scalable secondary storage","","","","","FAST","","","2009","9","","197-210","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/FAST Papers;Thesis","Fingerprint-Indexing","","","","","","","","2009","","","","","","","https://www.usenix.org/event/fast09/tech/slides/dubnicki.pdf","","","","","","","Note slides are attached as well.","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dubnicki et al. 2009 - HYDRAstor - A scalable secondary storage.pdf;All Papers/D/Dubnicki et al. 2009 - HYDRAstor - A scalable secondary storage.pdf","","","",""
"Conference paper","Lu G,Nam YJ,Du DH","","BloomStore: Bloom-Filter based memory-efficient key-value store for indexing of data deduplication on flash","","","","","2012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST)","","","2012","","","1-11","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/MSST","Fingerprint-Indexing","","","","","","","","2012-04","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2012.6232390;https://ieeexplore.ieee.org/abstract/document/6232390/;http://storageconference.us/2012/Papers/26.Flash.2.BloomStore.pdf","10.1109/MSST.2012.6232390","","","","Due to its better scalability, Key-Value (KV) store has superseded traditional relational databases for many applications, such as data deduplication, on-line multi-player gaming, and Internet services like Amazon and Facebook. The KV store efficiently supports two operations (key lookup and KV pair insertion) through an index structure that maps keys to their associated values. The KV store is also commonly used to implement the chunk index in data deduplication, where a chunk ID (SHA1 value computed based on the chunk's content) is a key and its associative chunk metadata (e.g., physical storage location, stream ID) is the value. For a deduplication system, typically the number of chunks is too large to store the KV store solely in RAM. Thus, the KV store maintains a large (hash-table based) index structure in RAM to index all KV pairs stored on secondary storage. Hence, its available RAM space limits the maximum number of KV pairs that can be stored. Moving the index data structure from RAM to flash can possibly overcome the space limitation. In this paper, we propose efficient KV store on flash with a Bloom Filter based index structure called BloomStore. The unique features of the BloomStore include (1) no index structure is required to be stored in RAM so that a small RAM space can support a large number of KV pairs and (2) both index structure and KV pairs are stored compactly on flash memory to improve its performance. Compared with the state-of-the-art KV store designs, the BloomStore achieves a significantly better key lookup performance and roughly the same insertion performance with multiple times less RAM usage based on our experiments with deduplication workloads.","flash memories;indexing;meta data;storage management;table lookup;BloomStore;bloom-filter based memory-efficient key-value store;indexing;data deduplication;flash memory;key lookup;KV pair insertion;associative chunk metadata;hash-table based index structure;RAM;Ash;Random access memory;Buffer storage;Throughput;Indexing;Scalability","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lu et al. 2012 - BloomStore - Bloom-Filter based memory-efficient key-value store for indexing of data deduplication on flash.pdf","","","",""
"Conference paper","Debnath BK,Sengupta S,Li J","","ChunkStash: Speeding Up Inline Storage Deduplication Using Flash Memory","","","","","USENIX annual technical conference","","","2010","","","1-16","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/Usenix ATC;Thesis","Fingerprint-Indexing","","","","","","","","2010","","","","","","","https://www.usenix.org/legacy/event/usenix10/tech/full_papers/Debnath.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Debnath et al. 2010 - ChunkStash - Speeding Up Inline Storage Deduplication Using Flash Memory.pdf","","","",""
"Conference paper","Meister D,Brinkmann A","","dedupv1: Improving deduplication throughput using solid state drives (SSD)","","","","","2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)","","","2010","","","1-6","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/MSST","Fingerprint-Indexing;SSD","","","","","","","","2010-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2010.5496992;https://ieeexplore.ieee.org/abstract/document/5496992/;https://d-nb.info/1036142256/34","10.1109/MSST.2010.5496992","","","","Data deduplication systems discover and remove redundancies between data blocks. The search for redundant data blocks is often based on hashing the content of a block and comparing the resulting hash value with already stored entries inside an index. The limited random IO performance of hard disks limits the overall throughput of such systems, if the index does not fit into main memory. This paper presents the architecture of the dedupv1 dedupli-cation system that uses solid-state drives (SSDs) to improve its throughput compared to disk-based systems. dedupv1 is designed to use the sweet spots of SSD technology (random reads and sequential operations), while avoiding random writes inside the data path. This is achieved by using a hybrid deduplication design. It is an inline deduplication system as it performs chunking and fingerprinting online and only stores new data, but it is able to delay much of the processing as well as IO operations.","data reduction;disc drives;data deduplication systems;solid state drives;dedupv1 system;disk-based systems;hybrid deduplication design;Throughput;Solid state circuits;Fingerprint recognition;Hard disks;Drives;Parallel processing;Delay;Cryptography;Data structures;Large-scale systems","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Meister and Brinkmann 2010 - dedupv1 - Improving deduplication throughput using solid state drives (SSD).pdf","","","",""
"Conference paper","Xia W,Jiang H,Feng D,Hua Y","","SiLo: A Similarity-Locality based Near-Exact Deduplication Scheme with Low RAM Overhead and High Throughput","","","","","USENIX Annual Technical Conference","","","2011","","","26-30","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Locality Based (per Ma 2017);Grouped by Publication/Usenix ATC;SCAIL Bibliography;""Segment"" term sources;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Similarity/Resemblance","","","","","","","","2011","","","","","","","http://static.usenix.org/events/atc11/tech/final_files/Xia.pdf","","","","","","","Their 2015 paper probably supersedes this paper. Keep for background.","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2011 - SiLo - A Similarity-Locality based Near-Exact Deduplication Scheme with Low RAM Overhead and High Throughput.pdf","","","",""
"Conference paper","Aronovich L,Asher R,Bachmat E,Bitner H,Hirsch M,Klein ST","","The Design of a Similarity Based Deduplication System","","","","","Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference","","","2009","","","6:1-6:14","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Fingerdiff Refs;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Similarity/Resemblance","","","ACM","New York, NY, USA","","Haifa, Israel","","2009","","","9781605586236","","","","http://doi.acm.org/10.1145/1534530.1534539;http://dx.doi.org/10.1145/1534530.1534539;https://dl.acm.org/citation.cfm?id=1534539;https://www.cs.bgu.ac.il/~ebachmat/systor.pdf","10.1145/1534530.1534539","","","","We describe some of the design choices that were made during the development of a fast, scalable, inline, deduplication device. The system’s design goals and how they were achieved are presented. This is the first deduplication device that uses similarity matching. The paper provides the following original research contributions: we show how similarity signatures can serve in a deduplication scheme; a novel type of similarity signatures is presented and its advantages in the context of deduplication requirements are explained. It is also shown how to combine similarity matching schemes with byte by byte comparison or hash based identity schemes.","","","","","","","SYSTOR '09","","","","","","","","","","","","","","","","","","All Papers/A/Aronovich et al. 2009 - The Design of a Similarity Based Deduplication System.pdf","","","",""
"Conference paper","Bhagwat D,Eshghi K,Long DD,Lillibridge M","","Extreme Binning: Scalable, parallel deduplication for chunk-based file backup","","","","","2009 IEEE International Symposium on Modeling, Analysis Simulation of Computer and Telecommunication Systems","","","2009","","","1-9","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing/Seminal-Fingerprint/Indexing;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Locality Based (per Ma 2017);SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Similarity/Resemblance","","","","","","","","2009-09","","","","","2375-0227","","http://dx.doi.org/10.1109/MASCOT.2009.5366623;https://ieeexplore.ieee.org/abstract/document/5366623/;http://shiftleft.com/mirrors/www.hpl.hp.com/techreports/2009/HPL-2009-10R2.pdf","10.1109/MASCOT.2009.5366623","","","","Data deduplication is an essential and critical component of backup systems. Essential, because it reduces storage space requirements, and critical, because the performance of the entire backup operation depends on its throughput. Traditional backup workloads consist of large data streams with high locality, which existing deduplication techniques require to provide reasonable throughput. We present Extreme Binning, a scalable deduplication technique for non-traditional backup workloads that are made up of individual files with no locality among consecutive files in a given window of time. Due to lack of locality, existing techniques perform poorly on these workloads. Extreme Binning exploits file similarity instead of locality, and makes only one disk access for chunk lookup per file, which gives reasonable throughput. Multi-node backup systems built with Extreme Binning scale gracefully with the amount of input data; more backup nodes can be added to boost throughput. Each file is allocated using a stateless routing algorithm to only one node, allowing for maximum parallelization, and each backup node is autonomous with no dependency across nodes, making data management tasks robust with low overhead.","data handling;file organisation;parallel processing;replicated databases;security of data;extreme binning;scalable deduplication;parallel deduplication;chunk-based file backup;data deduplication;backup operation;data streams;backup workloads;chunk lookup;multinode backup systems;backup nodes;file allocation;stateless routing algorithm;data management task;Throughput;Intrusion detection;Laboratories;Milling machines;Space technology;Routing;Robustness;Web pages;Digital images;Electronic mail","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bhagwat et al. 2009 - Extreme Binning - Scalable, parallel deduplication for chunk-based file backup.pdf","","","",""
"Conference paper","Meister D,Kaiser J,Brinkmann A","","Block Locality Caching for Data Deduplication","","","","","Proceedings of the 6th International Systems and Storage Conference","","","2013","","","15:1-15:12","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Thesis;p-scailbib","Fingerprint-Indexing","","","ACM","New York, NY, USA","","Haifa, Israel","","2013","","","9781450321167","","","","http://doi.acm.org/10.1145/2485732.2485748;http://dx.doi.org/10.1145/2485732.2485748;https://dl.acm.org/citation.cfm?id=2485748","10.1145/2485732.2485748","","","","","backup, data deduplication, disk bottleneck","","","","","","SYSTOR '13","","","","","","","","","","","","","","","","","","All Papers/M/Meister et al. 2013 - Block Locality Caching for Data Deduplication.pdf","","","",""
"Conference paper","Tan Y,Jiang H,Feng D,Tian L,Yan Z,Zhou G","","SAM: A Semantic-Aware Multi-tiered Source De-duplication Framework for Cloud Backup","","","","","2010 39th International Conference on Parallel Processing","","","2010","","","614-623","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing","Fingerprint-Indexing","","","","","","","","2010-09","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPP.2010.69;https://ieeexplore.ieee.org/abstract/document/5599246/;https://www.researchgate.net/profile/Hong_Jiang9/publication/221084718_SAM_A_Semantic-Aware_Multi-tiered_Source_De-duplication_Framework_for_Cloud_Backup/links/00b7d5143096de05dd000000/SAM-A-Semantic-Aware-Multi-tiered-Source-De-duplication-Framework-for-Cloud-Backup.pdf","10.1109/ICPP.2010.69","","","","Existing de-duplication solutions in cloud backup environment either obtain high compression ratios at the cost of heavy de-duplication overheads in terms of increased latency and reduced throughput, or maintain small de-duplication overheads at the cost of low compression ratios causing high data transmission costs, which results in a large backup window. In this paper, we present SAM, a Semantic-Aware Multitiered source de-duplication framework that first combines the global file-level de-duplication and local chunk-level deduplication, and further exploits file semantics in each stage in the framework, to obtain an optimal tradeoff between the deduplication efficiency and de-duplication overhead and finally achieve a shorter backup window than existing approaches. Our experimental results with real world datasets show that SAM not only has a higher de-duplication efficiency/overhead ratio than existing solutions, but also shortens the backup window by an average of 38.7%.","client-server systems;data compression;Internet;semantic aware multitiered source deduplication framework;cloud backup environment;compression ratio;high data transmission cost;global file level deduplication;local chunk level deduplication;Servers;Semantics;Redundancy;Clouds;Data communication;Image coding;Indexes;Cloud Backup;Backup Window;Data Deduplication;File Semantics","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tan et al. 2010 - SAM - A Semantic-Aware Multi-tiered Source De-duplication Framework for Cloud Backup.pdf","","","",""
"Conference paper","Wei J,Jiang H,Zhou K,Feng D","","MAD2: A scalable high-throughput exact deduplication approach for network backup services","","","","","2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)","","","2010","","","1-14","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/MSST","Fingerprint-Indexing","","","","","","","","2010-05","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2010.5496987;https://ieeexplore.ieee.org/abstract/document/5496987/;https://www.researchgate.net/profile/Ke_Zhou4/publication/228359189_MAD2_A_scalable_high-throughput_exact_deduplication_approach_for_network_backup_services/links/00b49538c8bd94e705000000.pdf","10.1109/MSST.2010.5496987","","","","Deduplication has been widely used in disk-based secondary storage systems to improve space efficiency. However, there are two challenges facing scalable high-throughput deduplication storage. The first is the duplicate-lookup disk bottleneck due to the large size of data index that usually exceeds the available RAM space, which limits the deduplication throughput. The second is the storage node island effect resulting from duplicate data among multiple storage nodes that are difficult to eliminate. Existing approaches fail to completely eliminate the duplicates while simultaneously addressing the challenges. This paper proposes MAD2, a scalable high-throughput exact deduplication approach for network backup services. MAD2 eliminates duplicate data both at the file level and at the chunk level by employing four techniques to accelerate the deduplication process and evenly distribute data. First, MAD2 organizes fingerprints into a Hash Bucket Matrix (HBM), whose rows can be used to preserve the data locality in backups. Second, MAD2 uses Bloom Filter Array (BFA) as a quick index to quickly identify non-duplicate incoming data objects or indicate where to find a possible duplicate. Third, Dual Cache is integrated in MAD2 to effectively capture and exploit data locality. Finally, MAD2 employs a DHT-based Load-Balance technique to evenly distribute data objects among multiple storage nodes in their backup sequences to further enhance performance with a well-balanced load. We evaluate our MAD2 approach on the backend storage of B-Cloud, a research-oriented distributed system that provides network backup services. Experimental results show that MAD2 significantly outperforms the state-of-the-art approximate deduplication approaches in terms of deduplication efficiency, supporting a deduplication throughput of at least 100MB/s for each storage component.","disc storage;distributed databases;random-access storage;resource allocation;MAD2;scalable high throughput exact deduplication;network backup service;disk based secondary storage system;duplicate lookup disk bottleneck;RAM;storage node island effect;hash bucket matrix;bloom filter array;dual cache;HT based load balance technique;research oriented distributed system;Space technology;Peer to peer computing;Throughput;Acceleration;Fingerprint recognition;Costs;Scalability;Network servers;Computer networks;Laboratories","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wei et al. 2010 - MAD2 - A scalable high-throughput exact deduplication approach for network backup services.pdf","","","",""
"Conference paper","Lillibridge M,Eshghi K,Bhagwat D,Deolalikar V,Trezis G,Camble P","","Sparse Indexing: Large Scale, Inline Deduplication Using Sampling and Locality","","","","","7th USENIX Conference on File and Storage Technologies (FAST 09)","","","2009","9","","111-123","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/FAST Papers;Locality Based (per Ma 2017);SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Accelerate;Fingerprint-Indexing","","","","","","","","2009","","","","","","","https://www.usenix.org/event/fast09/tech/full_papers/lillibridge/lillibridge_html/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lillibridge et al. 2009 - Sparse Indexing - Large Scale, Inline Deduplication Using Sampling and Locality.pdf","","","",""
"Conference paper","Zhu B,Li K,Patterson RH","","Avoiding the Disk Bottleneck in the Data Domain Deduplication File System","","","","","6th USENIX Conference on File and Storage Technologies (FAST 08)","","","2008","8","","1-14","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing/Seminal-Fingerprint/Indexing;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/FAST Papers;Locality Based (per Ma 2017);SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Code/OpenSource;Chunking","","","","","","","","2008","","","","","","","https://www.usenix.org/event/fast08/tech/full_papers/zhu/zhu_html/","","","","","Disk-based deduplication storage has emerged as the new-generation storage system for enterprise data protection to replace tape libraries. Deduplication removes redundant data segments to compress data into a highly compact form and makes it economical to store backups on disk instead of tape. A crucial requirement for enterprise data protection is high throughput, typically over 100 MB/sec, which enables backups to complete quickly. A significant challenge is to identify and eliminate duplicate data segments at this rate on a low-cost system that cannot afford enough RAM to store an index of the stored segments and may be forced to access an on-disk index for every input segment. This paper describes three techniques employed in the production Data Domain deduplication file system to relieve the disk bottleneck. These techniques include: (1) the Summary Vector, a compact in-memory data structure for identifying new segments; (2) Stream-Informed Segment Layout, a data layout method to improve on-disk locality for sequentially accessed segments; and (3) Locality Preserved Caching, which maintains the locality of the fingerprints of duplicate segments to achieve high cache hit ratios. Together, they can remove 99% of the disk accesses for deduplication of real world workloads. These techniques enable a modern two-socket dual-core system to run at 90% CPU utilization with only one shelf of 15 disks and achieve 100 MB/sec for single-stream throughput and 210 MB/sec for multi-stream throughput.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhu et al. 2008 - Avoiding the Disk Bottleneck in the Data Domain Deduplication File System.pdf","","","",""
"Conference paper","Kim C,Park KW,Park KH","","GHOST: GPGPU-offloaded high performance storage I/O deduplication for primary storage system","","","","","Proceedings of the 2012 International Workshop on Programming Models and Applications for Multicores and Manycores","","","2012","","","17-26","All;Thesis","Accelerate","","","Association for Computing Machinery","New York, NY, USA","","New Orleans, Louisiana","","2012-02-26","","","9781450312110","","","","https://doi.org/10.1145/2141702.2141705;http://dx.doi.org/10.1145/2141702.2141705;http://doi.acm.org/10.1145/2141702.2141705;https://dl.acm.org/citation.cfm?id=2141705;http://core.kaist.ac.kr/publication/paper_list/2012_pmam_cmkim.pdf;https://dl.acm.org/ft_gateway.cfm?id=2141705&ftid=1152236&dwn=1&CFID=178773324&CFTOKEN=4d6ee8c96b967f32-FAF6A5FF-0EBB-EF67-F02BBB1649890781","10.1145/2141702.2141705","","","","Data deduplication has been an effective way to eliminate redundant data mainly for backup storage systems. Since the recent primary storage systems in cloud services are expected to have the redundancy, the deduplication technique can also bring significant cost saving for the primary storage. However, the primary storage system requires high performance requirement about several GBs per second. Most conventional deduplication techniques targeted the performance requirement of 200-300MB/s.In an attempt to achieve a high performance storage deduplication system at the primary storage, we thoroughly analyze the performance bottleneck of previous deduplication systems to enhance the system to meet the requirement of the primary storage. The new performance bottleneck of deduplication in the primary storage lies on not only key-value store lookup, also computation for data segmentation and fingerprinting due to recent technology improvement of flash devices such as SSD. To overcome the bottlenecks, we propose a new deduplication system utilizing GPGPU. Our proposed system, termed GHOST, includes the followings to offload and optimize the deduplication processing in GPGPU: (1) In-Host Data Cache, (2) Destage-aware Data offloading to GPGPU and (3) In-GPGPU Table Cache of key-value store. These techniques improve the offloaded deduplication processing about 10-20% on the reasonable workload of the primary storage compared to the naive approach. Our proposed deduplication system can achieve 1.5GB/s in maximum which is about 5 times of the deduplication systems used CPU only.","GPGPU, deduplication, primary storage","","","","","","PMAM '12","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2012 - GHOST - GPGPU-offloaded high performance storage I - O deduplication for primary storage system.pdf","","","",""
"Conference paper","Bhatotia P,Rodrigues R,Verma A","","Shredder: GPU-accelerated incremental storage and computation","","","","","FAST","","","2012","14","","14","All;Grouped by Publication/FAST Papers;Thesis","Accelerate;GPU","","","","","","","","2012","","","","","","","https://www.usenix.org/legacy/event/fast12/tech/full_papers/Bhatotia2-5-12.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bhatotia et al. 2012 - Shredder - GPU-accelerated incremental storage and computation.pdf","","","",""
"Conference paper","Al-Kiswany S,Gharaibeh A,Santos-Neto E,Yuan G,Ripeanu M","","StoreGPU: Exploiting Graphics Processing Units to Accelerate Distributed Storage Systems","","","","","Proceedings of the 17th International Symposium on High Performance Distributed Computing","","","2008","","","165-174","All;Thesis","Accelerate;GPU","","","ACM","New York, NY, USA","","Boston, MA, USA","","2008","","","9781595939975","","","","http://doi.acm.org/10.1145/1383422.1383443;http://dx.doi.org/10.1145/1383422.1383443;https://dl.acm.org/citation.cfm?id=1383443;https://cs.uwaterloo.ca/~alkiswan/papers/StoreGPU-TR-2008-01.pdf","10.1145/1383422.1383443","","","","","gpu hashing, graphics processing unit, middleware, storage system, storegpu","","","","","","HPDC '08","","","","","","","","","","","","","","","","","","All Papers/A/Al-Kiswany et al. 2008 - StoreGPU - Exploiting Graphics Processing Units to Accelerate Distributed Storage Systems.pdf","","","",""
"Conference paper","Xia W,Jiang H,Feng D,Tian L,Fu M,Wang Z","","P-dedupe: Exploiting parallelism in data deduplication system","","","","","2012 IEEE Seventh International Conference on Networking, Architecture, and Storage","","","2012","","","","All;Superchunking Paper Sources/CDC/Chunking;Parallel Schemes;Thesis","Accelerate;Chunking;Datasets","","","IEEE","","2012 IEEE 7th International Conference on Networking, Architecture, and Storage (NAS)","Xiamen, China","2012/6/28-2012/6/30","2012-06","","","9780769547220","9781467318891","","","http://dx.doi.org/10.1109/nas.2012.46;http://ieeexplore.ieee.org/document/6310788/","10.1109/nas.2012.46","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2012 - P-dedupe - Exploiting parallelism in data deduplication system.pdf","","","",""
"Conference paper","Ma J,Zhao B,Wang G,Liu X","","Adaptive pipeline for deduplication","","","","","2012 IEEE 28th Symposium on Mass Storage Systems and Technologies (MSST)","","","2012","","","1-6","All;Superchunking Paper Sources/CDC/Chunking;Grouped by Publication/MSST","Chunking;Accelerate","","","","","","","","2012-04","","","","","2160-1968","","http://dx.doi.org/10.1109/MSST.2012.6232377;https://ieeexplore.ieee.org/abstract/document/6232377/;http://storageconference.us/2012/Papers/13.Short.1.AdaptivePipeline.pdf","10.1109/MSST.2012.6232377","","","","Deduplication has become one of the hottest topics in the field of data storage. Quite a few methods towards reducing disk I/O caused by deduplication have been proposed. Some methods also have been studied to accelerate computational sub-tasks in deduplication. However, the order of computational sub-tasks can affect overall deduplication throughput significantly, because computational sub-tasks exhibit quite different workload and concurrency in different orders and with different data sets. This paper proposes an adaptive pipelining model for the computational sub-tasks in deduplication. It takes both data type and hardware platform into account. Taking the compression ratio and the duplicate ratio of the data stream, and the compression speed and the fingerprinting speed on different processing units as parameters, it determines the optimal order of the pipeline stages (computational sub-tasks) and assigns each stage to the processing unit which processes it fastest. That is, “adaptive” refers to both data adaptive and hardware adaptive. Experimental results show that the adaptive pipeline improves the deduplication throughput up to 50% compared with the plain fixed pipeline, which implies that it is suitable for simultaneous deduplication of various data types on modern heterogeneous multi-core systems.","data compression;data reduction;input-output programs;multiprocessing systems;pipeline processing;storage management;adaptive pipelining model;overall deduplication throughput;data storage;disk I/O reduction;computational subtasks;data sets;compression ratio;duplicate ratio;data stream;compression speed;fingerprinting speed;data adaptive;hardware adaptive;heterogeneous multicore systems;Pipeline processing;Throughput;Pipelines;Hardware;Adaptation models;Graphics processing unit;Computational modeling","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Ma et al. 2012 - Adaptive pipeline for deduplication.pdf","","","",""
"Conference paper","Guo F,Efstathopoulos P","","Building a High-performance Deduplication System","","","","","USENIX annual technical conference","","","2011","","","","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Locality Based (per Ma 2017);Grouped by Publication/Usenix ATC;Thesis","Chunking;Accelerate;Fingerprint-Indexing","","","","","","","","2011","","","","","","","https://www.usenix.org/legacy/events/atc11/tech/final_files/GuoEfstathopoulos.pdf","","","","","","","Progressively Sampled. Less memory than Sampled Indexing. 4 steps to dedupe.","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Guo and Efstathopoulos 2011 - Building a High-performance Deduplication System.pdf","","","",""
"Conference paper","Liu C,Xue Y,Ju D,Wang D","","A Novel Optimization Method to Improve De-duplication Storage System Performance","","","","","2009 15th International Conference on Parallel and Distributed Systems","","","2009","","","228-235","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Chunking;Accelerate","","","IEEE","","","","","2009-12","","","9781424457892","9781424457885","1521-9097","1521-9097","http://dx.doi.org/10.1109/ICPADS.2009.103;https://ieeexplore.ieee.org/abstract/document/5395260/;https://www.researchgate.net/profile/Chuanyi_Liu/publication/221041112_A_Novel_Optimization_Method_to_Improve_De-duplication_Storage_System_Performance/links/00b4953b4af8158b40000000.pdf;https://ieeexplore.ieee.org/ielx5/5394475/5394991/05395260.pdf?tp=&arnumber=5395260&isnumber=5394991&ref=","10.1109/ICPADS.2009.103","","","","Data De-duplication has become a commodity component in data-intensive storage systems. But compared with other traditional storage paradigms, de-duplication system achieves elimination of data duplications or redundancies at the cost of bringing several additional layers or function components into the I/O path, and these additional components are either CPU-intensive or I/O intensive, largely hindering the overall system performance. Direct against the above potential system bottlenecks, this paper quantitatively analyzes the overhead of each main component introduced by de-duplication, and then proposes two performance optimization methods. The one is parallel calculation of content aware chunk identifiers, which fully utilizes the parallelism both inter and intra chunks by using a certain task partition and chunk content distribution algorithm. Experiments demonstrate that it can improve up to 150% of the system throughput, and at the same time much better utilize the multiprocessor resources. The other one is storage pipelining, which overlaps the CPU-bound, I/O-bound and network communication tasks. Through a dedicated five-stage storage pipeline design for file archival operations, experimental results show that the system throughput can increase up to 25% according to our workloads.","data compression;parallel programming;storage allocation;data de-duplication;data-intensive storage systems;performance optimization methods;task partition algorithm;chunk content distribution algorithm;parallel calculation;Optimization methods;System performance;Throughput;Cost function;Pipeline processing;Cryptography;Computer science;Laboratories;Information science;Performance analysis;Data De-duplication;Performance Optimization;Storage Pipeline;Parallel Hash","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Liu et al. 2009 - A Novel Optimization Method to Improve De-duplication Storage System Performance.pdf","","","",""
"Conference paper","Zhou B,Wen J","","Hysteresis Re-chunking Based Metadata Harnessing Deduplication of Disk Images","","","","","2013 42nd International Conference on Parallel Processing","","","2013","","","389-398","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Superchunking Paper Sources/CDC/Chunking/Metadata;Thesis","Chunking;Metadata;Container Fragmentation","","","","","","","","2013-10","","","","","2332-5690","","http://dx.doi.org/10.1109/ICPP.2013.48;https://ieeexplore.ieee.org/abstract/document/6687372/","10.1109/ICPP.2013.48","","","","Metadata-related overhead can significantly impact the performance of data deduplication systems, including the real duplication elimination ratio and the deduplication throughput. The amount of metadata produced is mainly determined by the chunking mechanism for the input data stream. In this paper, we propose a metadata harnessing deduplication (MHD) algorithm utilizing a duplication-distribution-based hysteresis re-chunking strategy. MHD harnesses the metadata by dynamically merging multiple non-duplicate chunks into one big chunk represented by one hash value while dividing big chunks straddling duplicate and non-duplicate data regions into small chunks represented with multiple hashes. Experimental results show that the proposed algorithm achieves a lower metadata overhead and a higher deduplication throughput for a given duplication elimination ratio, as compared with other state-of-the-art algorithms such as the Bimodal, Sub Chunk and Sparse Indexing algorithms.","meta data;storage management;hysteresis re-chunking based metadata harnessing deduplication system;disk images;real duplication elimination ratio;deduplication throughput;input data stream;MHD algorithm;duplication-distribution-based hysteresis re-chunking strategy;bimodal algorithm;subchunk algorithm;sparse indexing algorithms;data storage system;metadata-related overhead;Magnetohydrodynamics;Random access memory;Algorithm design and analysis;Hysteresis;Throughput;Merging;Indexes;Data Deduplication;Metadata Harnessing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou and Wen 2013 - Hysteresis Re-chunking Based Metadata Harnessing Deduplication of Disk Images.pdf","","","",""
"Conference paper","Lu G,Jin Y,Du DH","","Frequency Based Chunking for Data De-Duplication","","","","","2010 IEEE International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems","","","2010","","","","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Thesis","Chunking","","","IEEE","","Simulation of Computer and Telecommunication Systems (MASCOTS)","Miami Beach, FL, USA","2010/8/17-2010/8/19","2010-08","","","9781424481811","","","","http://dx.doi.org/10.1109/mascots.2010.37;http://ieeexplore.ieee.org/document/5581583/","10.1109/mascots.2010.37","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lu et al. 2010 - Frequency Based Chunking for Data De-Duplication.pdf","","","",""
"Conference paper","Kruus E,Ungureanu C,Dubnicki C","","Bimodal content defined chunking for backup streams","","","","","Fast","","","2010","","","239-252","All;Superchunking Paper Sources/CDC/Chunking;Grouped by Publication/FAST Papers;""Super-chunk"" term sources;Fingerdiff Refs;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Chunking","","","","","","","","2010","","","","","","","https://www.usenix.org/legacy/event/fast10/tech/full_papers/kruus.pdf","","","","","Data deduplication has become a popular technology for reducing the amount of storage space necessary for backup and archival data. Content defined chunking (CDC) techniques are well established methods of sep- arating a data stream into variable-size chunks such that duplicate content has a good chance of being discov- ered irrespective of its position in the data stream. Re- quirements for CDC include fast and scalable operation, as well as achieving good duplicate elimination. While the latter can be achieved by using chunks of small av- erage size, this also increases the amount of metadata necessary to store the relatively more numerous chunks, and impacts negatively the system’s performance. We propose a new approach that achieves comparable du- plicate elimination while using chunks of larger average size. It involves using two chunk size targets, and mech- anisms that dynamically switch between the two based on querying data already stored; we use small chunks in limited regions of transition from duplicate to non- duplicate data, and elsewhere we use large chunks. The algorithms rely on the block store’s ability to quickly de- liver a high-quality reply to existence queriesfor already- stored blocks. A chunking decision is made with limited lookahead and number of queries. We present results of running these algorithms on actual backup data, as well as four sets of source code archives. Our algorithms typ- ically achieve similar duplicate elimination to standard algorithms while using chunks 2–4 times as large. Such approaches may be particularly interesting to distributed storage systems that use redundancy techniques (such as error-correcting codes) requiring multiple chunk frag- ments, for which metadata overheads per stored chunk are high. We find that algorithm variants with more flex- ibility in location and size of chunks yield better dupli- cate elimination, at a cost of a higher number of existence queries.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kruus et al. 2010 - Bimodal content defined chunking for backup streams.pdf","","","",""
"Conference paper","Yu C,Zhang C,Mao Y,Li F","","Leap-based Content Defined Chunking — Theory and Implementation","","","","","2015 31st Symposium on Mass Storage Systems and Technologies (MSST)","","","2015","","","1-12","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Grouped by Publication/MSST;Thesis","Chunking","IEEE","","IEEE","","","","","2015-05","","","9781467376198","","2160-195X","2160-1968","http://dx.doi.org/10.1109/MSST.2015.7208290;https://ieeexplore.ieee.org/abstract/document/7208290/?casa_token=XHjhKM332x0AAAAA:E_elhlWBcpI4dEbcxVnmQoayasqadiai2-vgFymmpioIRFFuRBn7eyUIrKcOVXqdcsDXsCL5cYQjEg;https://ieeexplore.ieee.org/iel7/7167175/7208272/07208290.pdf?casa_token=22f8CRKd1LgAAAAA:rVbZ94kRsVsdijBzdHZ37eg4Ng3UIVL8BVlERaWwKNb3c1AMXNn9OzNvDweHbHv8KS4EVv0MFFv_Og;https://ieeexplore.ieee.org/abstract/document/7208290/;http://www.storageconference.us/2015/Papers/17.Yu.pdf;https://ieeexplore.ieee.org/ielx7/7167175/7208272/07208290.pdf?tp=&arnumber=7208290&isnumber=7208272&ref=","10.1109/MSST.2015.7208290","","","","Content Defined Chunking (CDC) is an important component in data deduplication, which affects both the deduplication ratio as well as deduplication performance. The sliding-window-based CDC algorithm and its variants have been the most popular CDC algorithms for the last 15 years. However, their performance is limited in certain application scenarios since they have to slide byte by byte. The authors present a leap-based CDC algorithm which provides significant improvement in deduplication performance without compromising the deduplication ratio. Compared to the sliding-window-based CDC algorithm, the new algorithm enables up to two-fold improvement in performance.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yu et al. 2015 - Leap-based Content Defined Chunking — Theory and Implementation.pdf","","","",""
"Journal article","Teodosiu D,Bjørner NS,Gurevich Y,Manasse M,Porkka J","","Optimizing file replication over limited-bandwidth networks using remote differential compression","Microsoft Corp","","","","","","","2006","","","","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Chunking","","","","","","","","2006-11-01","","","","","","","https://www.microsoft.com/en-us/research/publication/optimizing-file-replication-over-limited-bandwidth-networks-using-remote-differential-compression/;https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2006-157.pdf","","","","","A horizontal circulating carbonizer comprising an annular horizontally rotatable circulating hearth disposed rotatably, a carbonizer body covering said hearth, a coal feeder equipped on the carbonizer body, a preheating zone disposed in the vicinity of said feed coal supply opening, a carbonizing zone connected to said preheating zone, a cooling zone connected to the carbonizing zone, and a discharger of cooled coke, and a process for the preparation of coke using this horizontal circulating carbonizer.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Teodosiu et al. 2006 - Optimizing file replication over limited-bandwidth networks using remote differential compression.pdf","","","",""
"Conference paper","El-Shimi A,Kalach R,Kumar A,Ottean A,Li J,Sengupta S","","Primary data deduplication—large scale study and system design","","","","","Presented as part of the 2012 USENIX Annual Technical Conference (USENIX ATC 12)","","","2012","","","285-296","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Thesis","Chunking","","","","","","","","2012","","","","","","","https://www.usenix.org/conference/atc12/technical-sessions/presentation/el-shimi;https://www.usenix.org/system/files/conference/atc12/atc12-final293.pdf","","","","","","","<div><br></div><div></div>Implements gradient boundaries, or partial powers.&nbsp;<div>""<span style=""word-spacing: normal;"">We design
a regression chunking algorithm that aims to reduce the
peaks in the chunk size distribution around Smax, while
preserving or even improving the dedup space savings.
Reducing forced boundary declaration at maximum
chunk size. The basic idea involves relaxing the strict
pattern matching rule – we allow matches on suffixes of
the bit pattern P so as to avoid forced boundary declaration at maximum chunk size when possible. Let k denote the maximum number of prefix bits in the pattern
P whose matching can be relaxed. Then, we attempt to
match the last L − i bits of the pattern P with the lower
order bits of the Rabin hash, with decreasing priority
for i = 0,1,..., k. A boundary is declared at maximum
chunk size only when no relaxed suffixes match after
Smax bytes are scanned. For ties within the same suffix
match, the later match position in the sequence is used.
In summary, this technique enables the maximum
chunk size bound to be satisfied in a content dependent
manner more often through gradual regression to larger
match sets with smaller number of matching bits. With
k = 1 level of regression, the probability of forcing a
chunk boundary at Smax for random data reduces to 1.8%.
At k = 4 levels of regression, the probability is extremely
low at 10−14. We use this value of k in our system.""</span></div>","","","","","","","","","","","","","","","","","","","","","","","All Papers/E/El-Shimi et al. 2012 - Primary data deduplication—large scale study and system design.pdf","","","",""
"Journal article","Muthitacharoen A,Chen B,Mazières D","","A low-bandwidth network file system","Oper. Syst. Rev.","ACM SIGOPS Operating Systems Review","","","","","","2001","35","5","174-187","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Chunking;Code/OpenSource;Fixed Size Blocks","","","ACM Press","New York, New York, USA","","","","2001","","","","","0163-5980","","http://dx.doi.org/10.1145/502051.502052;https://doi.org/10.1145/502034.502052?locatt=mode:legacy","10.1145/502051.502052","","","","Users rarely consider running network file systems over slow or wide-area networks, as the performance would be unacceptable and the bandwidth consumption too high. Nonetheless, efficient remote file access would often be desirable over such networks---particularly when high latency makes remote login sessions unresponsive. Rather than run interactive programs such as editors remotely, users could run the programs locally and manipulate remote files through the file system. To do so, however, would require a network file system that consumes less bandwidth than most current file systems.This paper presents LBFS, a network file system designed for low-bandwidth networks. LBFS exploits similarities between files or versions of the same file to save bandwidth. It avoids sending data over the network when the same data can already be found in the server's file system or the client's cache. Using this technique in conjunction with conventional compression and caching, LBFS consumes over an order of magnitude less bandwidth than traditional network file systems on common workloads.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Muthitacharoen et al. 2001 - A low-bandwidth network file system.pdf;All Papers/M/Muthitacharoen et al. 2001 - lbfs-muthitacharoen.pdf","","","",""
"Computer program","Andres Erbsen, Jade Philipoom, Jason Gross, Robert Sloan, Adam Chlipala","","Fiat-Crypto: Synthesizing Correct-by-Construction Code for Cryptographic Primitives","","","","","","","","","","","","All","Implementations","Github","","","","","","","","","2019-12-12","","","","","https://github.com/mit-plv/fiat-crypto","","","","","Cryptographic Primitive Code Generation by Fiat. Contribute to mit-plv/fiat-crypto development by creating an account on GitHub.","","","","","","Scientific software","","","","","","","","","","","","","","","","","","","All Papers/A/Andres Erbsen, Jade Philipoom, Jason ... - Fiat-Crypto - Synthesizing Correct-by-Construction Code for Cryptographic Primitives.pdf","","","",""
"Journal article","Liu J,Wang J,Tao X,Shen J","","Secure similarity-based cloud data deduplication in Ubiquitous city","Pervasive Mob. Comput.","Pervasive and mobile computing","","","","","","2017","41","","231-242","All","Similarity/Resemblance","","","","","","","","2017-10-01","","","","","1574-1192","","http://www.sciencedirect.com/science/article/pii/S1574119217301657;http://dx.doi.org/10.1016/j.pmcj.2017.03.010","10.1016/j.pmcj.2017.03.010","","","","Ubiquitous city, a wonderful vision of future urban, enables citizens to access to any infrastructure and enjoy high quality urban services by integrating information and communication technologies into urban management. However, it inevitably brings a huge amount of data in the Ubiquitous city scenario. It makes how to efficiently manage the ever-increasing datum while preserving data privacy a challenge task. To cope with the above issue, secure data deduplication has attracted considerable interests both academic and industrial community. It can reduce the amount of storage cost by eliminating duplicate data copies, while providing data privacy. Although message-locked encryption has been widely adopted to perform secure cross-client deduplication, it will bring many additional metadata located both client and cloud sides. Recently, some researchers proposed a novel extension of message-locked encryption, named block-level message-locked encryption (BL-MLE), in which block keys are encapsulated into block tags to save metadata storage space. We argue that BL-MLE suffers from high computation overhead for block tag comparison, especially in dissimilar files setting. In this paper, we propose a novel secure similarity-based data deduplication scheme by integrating the technologies of bloom filter and content-defined chunking, which can significantly reduce the computation overhead by only performing deduplication operations for similar files. Security and efficiency evaluations show that the proposed scheme can achieve the desired security goals, while providing a comparable computation overhead.","Secure deduplication; Ubiquitous city; Content-defined chunking; Proofs of ownership","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Liu et al. 2017 - Secure similarity-based cloud data deduplication in Ubiquitous city.pdf","","","",""
"Computer program","Vasily Tarasov, Geoff Kuenning, Sonam Mandal, Karthikeyani Palanisami, Philip Shilane, Sagar Trehan, and Erez Zadok","","dmdedup3.19","","","","","","","","","","","","All","Datasets;Implementations","Github","","","","","","","","","2019-12-12","","","","","https://github.com/dmdedup/dmdedup3.19","","","","","Device-mapper's dedup target provides transparent data deduplication of block devices. Every write coming to a dm-dedup instance is deduplicated against previously written data. For datasets that contain many duplicates scattered across the disk (e.g., virtual machine disk image collections, backups, home directory servers) deduplication provides a significant amount of space savings.","","","","","","Scientific software","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Tarasov V,Zadok E,Shepler S","","Filebench: A flexible framework for file system benchmarking","login: The USENIX Magazine","","","","","","","2016","41","1","6-12","All","Datasets","","","","","","","","2016","","","","","","","https://www.usenix.org/system/files/login/articles/login_spring16_02_tarasov.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tarasov et al. 2016 - Filebench - A flexible framework for file system benchmarking.pdf","","","",""
"Report","Mandal S","","Design and Implementation of an Open-Source Deduplication Platform for Research","","","","","","","","2015","","","","All","Platform;Implementations","Stony Brook University","","","","","","","2015-12","","","","","","","https://pdfs.semanticscholar.org/43c4/1c5eabbd1c69692d84b51468edf36ada5408.pdf","","","","","","","","","","","","","","","","","45","","","","","","","","","","","","","All Papers/M/Mandal 2015 - Design and Implementation of an Open-Source Deduplication Platform for Research.pdf","","","",""
"Conference paper","Fu M,Feng D,Hua Y,He X,Chen Z,Xia W,Zhang Y,Tan Y","","Design tradeoffs for data deduplication performance in backup workloads","","","","","13th USENIX Conference on File and Storage Technologies (FAST 15)","","","2015","","","331-344","All;Grouped by Publication/FAST Papers;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Platform;Chunking","","","","","","","","2015","","","","","","","","","","","","Data deduplication has become a standard component in modern backup systems. In order to understand the fundamental tradeoffs in each of its design choices (such as prefetching and sampling), we disassemble data deduplication into a large N-dimensional parameter space. Each point in the space is of various parameter settings, and performs a tradeoff among backup and restore performance, memory footprint, and storage cost. Existing and potential solutions can be considered as specific points in the space. Then, we propose a general-purpose framework to evaluate various deduplication solutions in the space. Given that no single solution is perfect in all metrics, our goal is to find some reasonable solutions that have sustained backup performance and perform a suitable tradeoff between deduplication ratio, memory footprints, and restore performance. Our findings from extensive experiments using real-world workloads provide a detailed guide to make efficient design decisions according to the desired tradeoff.","","Destor, fomy/destor on github. Model cost based on Amazon. Lookup overhead chart.","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fu et al. 2015 - Design tradeoffs for data deduplication performance in backup workloads.pdf","","","",""
"Journal article","Basin D,Cremers C,Dreier J,Sasse R","","Symbolically analyzing security protocols using tamarin","","","","","","","","2017","","","","All","Security Proof","","","","","","","","2017","","","","","","","https://hal.archives-ouvertes.fr/hal-01622110/document","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Basin et al. 2017 - Symbolically analyzing security protocols using tamarin.pdf","","","",""
"Conference paper","Meier S,Schmidt B,Cremers C,Basin D","","The TAMARIN Prover for the Symbolic Analysis of Security Protocols","","","","","Computer Aided Verification","","","2013","","","696-701","All","Security Proof","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-39799-8_48;https://link.springer.com/chapter/10.1007/978-3-642-39799-8_48;https://link.springer.com/content/pdf/10.1007/978-3-642-39799-8_48.pdf","10.1007/978-3-642-39799-8_48","","","","The Tamarin prover supports the automated, unbounded, symbolic analysis of security protocols. It features expressive languages for specifying protocols, adversary models, and properties, and support for efficient deduction and equational reasoning. We provide an overview of the tool and its applications.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Meier et al. 2013 - The TAMARIN Prover for the Symbolic Analysis of Security Protocols.pdf","","","",""
"Journal article","Singhal S,Kaushik A,Sharma P","","A Novel approach of data deduplication for distributed storage","International Journal of Engineering & Technology","International Journal of Engineering & Technology","","","","","","2018","7","2.4","46-52","All","Secure Dedup","","","","","","","","2018-03-10","","2019-12-09","","","2227-524X","2227-524X","https://www.sciencepubco.com/index.php/ijet/article/view/10040;http://dx.doi.org/10.14419/ijet.v7i2.4.10040","10.14419/ijet.v7i2.4.10040","","","","Due to drastic growth of digital data, data deduplication has become a standard component of modern backup systems. It reduces data redundancy, saves storage space, and simplifies the management of data chunks. This process is performed in three steps: chunking, fingerprinting, and indexing of fingerprints. In chunking, data files are divided into the chunks and the chunk boundary is decided by the value of the divisor. For each chunk, a unique identifying value is computed using a hash signature (i.e. MD-5, SHA-1, SHA-256), known as fingerprint. At last, these fingerprints are stored in the index to detect redundant chunks means chunks having the same fingerprint values. In chunking, the chunk size is an important factor that should be optimal for better performance of deduplication system. Genetic algorithm (GA) is gaining much popularity and can be applied to find the best value of the divisor. Secondly, indexing also enhances the performance of the system by reducing the search time. Binary search tree (BST) based indexing has the time complexity of which is minimum among the searching algorithm. A new model is proposed by associating GA to find the value of the divisor. It is the first attempt when GA is applied in the field of data deduplication. The second improvement in the proposed system is that BST index tree is applied to index the fingerprints. The performance of the proposed system is evaluated on VMDK, Linux, and Quanto datasets and a good improvement is achieved in deduplication ratio.","Data Deduplication, Chunking, Fingerprints, Indexing, Genetic Algorithm","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Singhal et al. 2018 - A Novel approach of data deduplication for distributed storage.pdf","","","",""
"Journal article","Xia W,Jiang H,Feng D,Hua Y","","Similarity and Locality Based Indexing for High Performance Data Deduplication","IEEE Trans. Comput.","IEEE transactions on computers. Institute of Electrical and Electronics Engineers","","","","","","2015","64","4","1162-1176","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;""Segment"" term sources;SCAIL Bibliography;Grouped by Publication/IEEE Transactions on Computers;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Fingerprint-Indexing;Accelerate;Similarity/Resemblance","","","","","","","","2015-04","","","","","0018-9340","2326-3814","http://dx.doi.org/10.1109/TC.2014.2308181;https://ieeexplore.ieee.org/abstract/document/6747963/","10.1109/TC.2014.2308181","","","","Data deduplication has gained increasing attention and popularity as a space-efficient approach in backup storage systems. One of the main challenges for centralized data deduplication is the scalability of fingerprint-index search. In this paper, we propose SiLo, a near-exact and scalable deduplication system that effectively and complementarily exploits similarity and locality of data streams to achieve high duplicate elimination, throughput, and well balanced load at extremely low RAM overhead. The main idea behind SiLo is to expose and exploit more similarity by grouping strongly correlated small files into a segment and segmenting large files, and to leverage the locality in the data stream by grouping contiguous segments into blocks to capture similar and duplicate data missed by the probabilistic similarity detection. SiLo also employs a locality based stateless routing algorithm to parallelize and distribute data blocks to multiple backup nodes. By judiciously enhancing similarity through the exploitation of locality and vice versa, SiLo is able to significantly reduce RAM usage for index-lookup, achieve the near-exact efficiency of duplicate elimination, maintain a high deduplication throughput, and obtain load balance among backup nodes.","database indexing;meta data;resource allocation;similarity based indexing;locality based indexing;high-performance data deduplication;space-efficient approach;backup storage systems;centralized data deduplication;fingerprint-index search scalability;SiLo;near-exact-scalable deduplication system;data stream similarity;data stream locality;load balancing;RAM overhead;strongly-correlated small-file grouping;large-file segmentation;locality leveraging;contiguous segment grouping;probabilistic similarity detection;locality based stateless routing algorithm;data block distribution;data block parallelization;backup nodes;RAM usage reduction;index-lookup;near-exact efficiency;duplicate elimination;deduplication throughput;Random access memory;Servers;Indexing;Throughput;Scalability;Probabilistic logic;Data deduplication;storage system;index structure;performance evaluation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2015 - Similarity and Locality Based Indexing for High Performance Data Deduplication.pdf","","","",""
"Journal article","Ferguson N,Schneier B,Kohno T","","Cryptography engineering","Design Princi","","","","","","","2010","","","","All","Encryption Techniques","","","Wiley Online Library","","","","","2010","","","","","","","https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118722367;https://www.schneier.com/academic/paperfiles/fortuna.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Ferguson et al. 2010 - Cryptography engineering.pdf","","","",""
"Conference paper","Lee S,Choi D","","Privacy-preserving cross-user source-based data deduplication in cloud storage","","","","","2012 International Conference on ICT Convergence (ICTC)","","","2012","","","329-330","All;Thesis","Secure Dedup;Thresholding","","","","","","","","2012-10","","","","","2162-1241","","http://dx.doi.org/10.1109/ICTC.2012.6386851;https://ieeexplore.ieee.org/abstract/document/6386851/","10.1109/ICTC.2012.6386851","","","","Cloud storage services possibly store only a single copy of redundant data to save disk space and provide links to that copy rather storing other actual copies of data. This is called deduplication. If deduplication is performed across different accounts at each client side, this is classified into cross-user source-based deduplication. However, cross-user source-based deduplication can server as a side-channel of breaching user's privacy. To protect user's privacy, Harnik et al. proposed a randomized solution for cross-user source-based deduplication, user's privacy is, however, still at risk with a high probability. In this paper, we propose a new cross-user source-based deduplication providing dramatically enhanced security.","client-server systems;cloud computing;data privacy;pattern classification;storage management;privacy-preserving cross-user source-based data deduplication;cloud storage services;data redundancy;disk space;data copy;client side;data classification;side-channel;user privacy breaching;user privacy protection;security enhancement;Security;Servers;Cloud computing;Privacy;Radiation detectors;Laboratories;Electronic mail;Cloud storage;deduplication;privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Lee and Choi 2012 - Privacy-preserving cross-user source-based data deduplication in cloud storage.pdf","","","",""
"Miscellaneous","Jiang S,Jiang T,Wang L","","Secure and Efficient Cloud Data Deduplication with Ownership Management","IEEE Transactions on Services Computing","","","","","","","2017","","","1-1","All","Secure Dedup","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1109/tsc.2017.2771280","10.1109/tsc.2017.2771280","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jiang et al. 2017 - Secure and Efficient Cloud Data Deduplication with Ownership Management.pdf","","","",""
"Journal article","Jiang T,Chen X,Wu Q,Ma J,Susilo W,Lou W","","Secure and Efficient Cloud Data Deduplication With Randomized Tag","IEEE Trans. Inf. Forensics Secur.","IEEE Transactions on Information Forensics and Security","","","","","","2017","12","3","532-543","All","Secure Dedup","","","","","","","","2017-03","","","","","1556-6021","","http://dx.doi.org/10.1109/TIFS.2016.2622013;https://ieeexplore.ieee.org/abstract/document/7707339/;http://www.cnsr.ictas.vt.edu/publication/t13.pdf;https://ieeexplore-ieee-org.ezproxy.lib.bbk.ac.uk/ielx7/10206/7782442/07707339.pdf?tp=&arnumber=7707339&isnumber=7782442&ref=","10.1109/TIFS.2016.2622013","","","","Cross-client data deduplication has been widely used to eliminate redundant storage overhead in cloud storage system. Recently, Abadi et al. introduced the primitive of MLE2 with nice security properties for secure and efficient data deduplication. However, besides the computationally expensive noninteractive zero-knowledge proofs, their fully randomized scheme (R-MLE2) requires the inefficient equality-testing algorithm to identify all duplicate ciphertexts. Thus, an interesting challenging problem is how to reduce the overhead of R-MLE2 and propose an efficient construction for R-MLE2. In this paper, we introduce a new primitive called μR-MLE2, which gives a partial positive answer for this challenging problem. We propose two schemes: static scheme and dynamic scheme, where the latter one allows tree adjustment by increasing some computation cost. Our main trick is to use the interactive protocol based on static or dynamic decision trees. The advantage gained from it is, by interacting with clients, the server will reduce the time complexity of deduplication equality test from linear time to efficient logarithmic time over the whole data items in the database. The security analysis and the performance evaluation show that our schemes are Path-PRV-CDA2 secure and achieve several orders of magnitude higher performance for data equality test than R-MLE2 scheme when the number of data items is relatively large.","cloud computing;computational complexity;security of data;storage management;Path-PRV-CDA2 security;efficient logarithmic time;linear time;deduplication equality test;dynamic decision trees;interactive protocol;μR-MLE2;ciphertexts;inefficient equality-testing algorithm;noninteractive zero-knowledge proofs;efficient cloud data deduplication;security properties;cloud storage system;cross-client data deduplication;randomized tag;Cloud computing;Encryption;Decision trees;Servers;Maximum likelihood estimation;Deduplication;convergent encryption;message-locked encryption;interactive protocol","Improves comparison performance of Abadi's MLE for Lock-Dependent messages using static and dynamic decision trees.","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jiang et al. 2017 - Secure and Efficient Cloud Data Deduplication With Randomized Tag.pdf","","","",""
"Journal article","Passricha V,Chopra A,Singhal S","","Secure Deduplication Scheme for Cloud Encrypted Data","IJAPUC","International Journal of Advanced Pervasive and Ubiquitous Computing (IJAPUC)","","","","","","2019","11","2","27-40","All;Thesis","Secure Dedup;Ownership Management","","","IGI Global","","","","","2019-04-01","","2019-12-06","","","","","https://www.igi-global.com/article/secure-deduplication-scheme-for-cloud-encrypted-data/228099;http://dx.doi.org/10.4018/IJAPUC.2019040103;https://pdfs.semanticscholar.org/bf08/0e6da20d83f2d78bc29edaec1ed3e1a9c614.pdf","10.4018/IJAPUC.2019040103","","","","Cloud storage (CS) is gaining much popularity nowadays because it offers low-cost and convenient network storage services. In this big data era, the explosive growth in digital data moves the users towards CS but this causes a lot of storage pressure on CS systems because a large volume of this data is redundant. Data deduplication is an effective data reduction technique. The dynamic nature of data makes security and ownership of data as a very important issue. Proof-of-ownership schemes are a robust way to check the ownership claimed by any owner. However, this method affects the deduplication process because encryption methods have varying characteristics. A convergent encryption (CE) scheme is widely used for secure data deduplication. The problem with the CE- based scheme is that the user can decrypt the cloud data while he has lost his ownership. This article addresses the problem of ownership revocation by proposing a secure deduplication scheme for encrypted data. The proposed scheme enhances the security against unauthorized encryption and poison attack on the predicted set of data.","","Looks like derivative 2016 Hur, Koo, Shin and Kang - Secure Data Deduplication with Dynamic Ownership Management in Cloud Storage. But no reference to previous work in this paper!<div>Use tree structure to create ownership keys.</div>","","","en","","","","","","","","","","","","","","","","","","","","All Papers/P/Passricha et al. 2019 - Secure Deduplication Scheme for Cloud Encrypted Data.pdf","","","",""
"Journal article","Wang XA,Ma J,Xhafa F,Qin B,Zhang M","","New efficient chosen ciphertext secure Elgamal encryption schemes for secure Cloud storage service","Int. J. Web Grid Serv.","International Journal of Web and Grid Services","","","","","","2017","13","3","246-269","All","Encryption Techniques","","","Inderscience Publishers","","","","","2017-01-01","","","","","1741-1106","","https://www.inderscienceonline.com/doi/abs/10.1504/IJWGS.2017.085168;http://dx.doi.org/10.1504/IJWGS.2017.085168","10.1504/IJWGS.2017.085168","","","","Nowadays Cloud computation has become a commonplace information service paradigm for all actors in ICT field, from individuals to big corporates. In particular, Cloud platforms and data centres are being used each time more for outsourcing data. However, data owners often worry about their data security and privacy before outsourcing the data to the Cloud, thus it is often a practice to first encrypt the data sets and then outsource them to the Cloud. The drawback of this approach is that, if the encryption scheme can only achieve chosen plaintext security, it cannot be assured to achieve strong security against many kinds of malicious adversaries in the Cloud setting. The chosen ciphertext security is essential for outsourcing ciphertexts to the Cloud, on the other hand, in most cases the data owners prefer to choose high-efficient encryption schemes for saving computation and communication costs. In this paper, we propose a new way to achieve chosen ciphertext security for Elgamal encryption scheme, which is a very basic and usual primitive for encapsulating block data encryption keys. We propose two new chosen ciphertext attack (CCA) secure schemes. The first one, which is a public key encryption proved secure in the random oracle based on the computational Diffie-Hellman (CDH) assumption, has almost no additional overhead compared with the traditional (indistinguishable under chosen plaintext attack secure Elgamal scheme, except one additional modular exponentiation for the decryption. The second scheme, which is a key encapsulation mechanism (KEM) proved secure in the standard model based on a new non-interactive assumption, has only two group elements as the encapsulations. Thus we solve the open problem left by Hanaoka et al. in Crypto'12, which consists in how to construct anindistinguishable under chosen ciphertext attack secure KEM without pairings based on a non-interactive assumption and with two group element encapsulations. To prove the scheme's security, we develop a new assumption called verifiable CDH assumption. We also generalise our technique to several existing well-known CCA secure KEMs, including the Boneh-Mei-Waters (BMW) KEM and the Hofheinz-Kiltz (HK) KEM, and show that our new schemes are even more efficient than these well-known schemes. Finally, we propose a new framework for efficient and secure data outsourcing to the Cloud based on our new schemes and present a rough analysis of its security.","","doi: 10.1504/IJWGS.2017.085168, Requested ILL from Birkbeck 6 Dec 2019","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Joice SA,Mohamed MA","","Cloud Storage: A Review on Secure Deduplication and Issues","J. Internet Technol.","Journal of Internet Technology","","","","","","2019","20","3","861-873","All;Fingerdiff Refs","Survey","","","","","","","","2019","","","","","1607-9264","","http://dx.doi.org/10.3966/160792642019052003019;https://pdfs.semanticscholar.org/e885/67838722560e9f69a3c6cb4d2cdca2db23fb.pdf?_ga=2.221882264.1609901586.1575622557-1384287861.1575622557","10.3966/160792642019052003019","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Joice and Mohamed 2019 - Cloud Storage - A Review on Secure Deduplication and Issues.pdf","","","",""
"Conference paper","Blakley GR,Meadows C","","Security of Ramp Schemes","","","","","Advances in Cryptology","","","1985","","","242-268","All","Encryption Techniques;SSSS/RSSS/Convergent Dispersal","","","Springer Berlin Heidelberg","","","","","1985","","","","","","","http://dx.doi.org/10.1007/3-540-39568-7_20;https://link.springer.com/chapter/10.1007/3-540-39568-7_20;https://link.springer.com/content/pdf/10.1007/3-540-39568-7_20.pdf","10.1007/3-540-39568-7_20","","","","A k out of n p/s/r process [AS81] is a very efficient way to convey information (k words suffice to reclaim k words). But it provides virtually no cryptographic security for the information it deals with.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Blakley and Meadows 1985 - Security of Ramp Schemes.pdf","","","",""
"Conference paper","Agarwal B,Akella A,Anand A,Balachandran A,Chitnis PV,Muthukrishnan C,Ramjee R,Varghese G","","Endre: An end-system redundancy elimination service for enterprises","","","","","NSDI","","","2010","10","","419-432","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Thesis","Fingerprint-Indexing;Chunking;Mobile Dedup","","","","","","","","2010","","","","","","","https://www.usenix.org/event/nsdi10/tech/full_papers/aggarwal.pdf","","","","","","","Also Aggarwal. EndRE (SampleByte) is more efficient than Rabin fingerprinting, but works on very small chunks, down to 32 bytes. Also it uses a customized dictionary of patterns that it has seen so far (idea for compression/deduplication detection?) Nice idea of adapting it's CPU usage based on server load. Server precomputes 2 sizes of chunks, depending upon signal strength on mobile device. Server appends to and tracks client cache of chunks, so client (cell phone) calculations are minimized, but server can reference chunks already on the phone.","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Agarwal et al. 2010 - Endre - An end-system redundancy elimination service for enterprises.pdf","","","",""
"Journal article","Reed IS,Solomon G","","Polynomial Codes Over Certain Finite Fields","Journal of the Society for Industrial and Applied Mathematics","Journal of the Society for Industrial and Applied Mathematics","","","","","","1960","8","2","300-304","All","PoW","","","Society for Industrial and Applied Mathematics","","","","","1960-06-01","","","","","0368-4245","","https://doi.org/10.1137/0108018;http://dx.doi.org/10.1137/0108018;https://epubs.siam.org/doi/abs/10.1137/0108018","10.1137/0108018","","","","","","doi: 10.1137/0108018","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Report","Jason K. Resch JS","","AONT-RS: Blending Security and Performance in Dispersed Storage Systems","","","","","","","","2011","","","","All","Encryption Techniques;SSSS/RSSS/Convergent Dispersal","","","","","","","","2011","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jason K. Resch 2011 - AONT-RS - Blending Security and Performance in Dispersed Storage Systems.pdf","","","",""
"Journal article","Rabin MO","","Efficient Dispersal of Information for Security, Load Balancing, and Fault Tolerance","J. ACM","Journal of the ACM","","","","","","1989","36","2","335-348","All","Maintaining Privacy;SSSS/RSSS/Convergent Dispersal","","","ACM","New York, NY, USA","","","","1989-04","","","","","0004-5411","","http://doi.acm.org/10.1145/62044.62050;http://dx.doi.org/10.1145/62044.62050;https://dl.acm.org/citation.cfm?id=62050","10.1145/62044.62050","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rabin 1989 - Efficient Dispersal of Information for Security, Load Balancing, and Fault Tolerance.pdf","","","",""
"Journal article","Li X,Li J,Huang F","","A secure cloud storage system supporting privacy-preserving fuzzy deduplication","Soft Computing","","","","","","","2016","20","4","1437-1448","All","Secure Dedup;Similarity/Resemblance","","","Springer","","","","","2016-04-01","","","","","1433-7479","","https://doi.org/10.1007/s00500-015-1596-6;http://dx.doi.org/10.1007/s00500-015-1596-6;https://link.springer.com/article/10.1007/s00500-015-1596-6","10.1007/s00500-015-1596-6","","","","Deduplication is an important technology in the cloud storage service. For protecting user privacy, sensitive data usually have to be encrypted before outsourcing. This makes secure data deduplication a challenging task. Although convergent encryption is used to securely eliminate duplicate copies on the encrypted data, these secure deduplication techniques support only exact data deduplication. That is, there is no tolerance of differences in traditional deduplication schemes. This requirement is too strict for multimedia data including image. For images, typical modifications such as resizing and compression only change their binary presentation but maintain human visual perceptions, which should be eliminated as duplicate copies. Those perceptual similar images occupy a lot of storage space on the remote server and greatly affect the efficiency of deduplication system. In this paper, we first formalize and solve the problem of effective fuzzy image deduplication while maintaining user privacy. Our solution eliminates duplicated images based on the measurement of image similarity over encrypted data. The robustness evaluation is given and demonstrates that this fuzzy deduplication system is able to duplicate perceptual similar images, which optimizes the storage and bandwidth overhead greatly in cloud storage service.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Li J,Chen X,Li M,Li J,Lee PP,Lou W","","Secure Deduplication with Efficient and Reliable Convergent Key Management","IEEE Trans. Parallel Distrib. Syst.","IEEE Transactions on Parallel and Distributed Systems","","","","","","2014","25","6","1615-1625","All;Grouped by Publication/IEEE Trans Parallel Distrib Syst","Secure Dedup;Key Management;SSSS/RSSS/Convergent Dispersal","","","","","","","","2014-06","","","","","1045-9219","2161-9883","http://dx.doi.org/10.1109/TPDS.2013.284;https://ieeexplore.ieee.org/abstract/document/6658753/;https://ieeexplore.ieee.org/iel7/71/4359390/06658753.pdf","10.1109/TPDS.2013.284","","","","Data deduplication is a technique for eliminating duplicate copies of data, and has been widely used in cloud storage to reduce storage space and upload bandwidth. Promising as it is, an arising challenge is to perform secure deduplication in cloud storage. Although convergent encryption has been extensively adopted for secure deduplication, a critical issue of making convergent encryption practical is to efficiently and reliably manage a huge number of convergent keys. This paper makes the first attempt to formally address the problem of achieving efficient and reliable key management in secure deduplication. We first introduce a baseline approach in which each user holds an independent master key for encrypting the convergent keys and outsourcing them to the cloud. However, such a baseline key management scheme generates an enormous number of keys with the increasing number of users and requires users to dedicatedly protect the master keys. To this end, we propose Dekey , a new construction in which users do not need to manage any keys on their own but instead securely distribute the convergent key shares across multiple servers. Security analysis demonstrates that Dekey is secure in terms of the definitions specified in the proposed security model. As a proof of concept, we implement Dekey using the Ramp secret sharing scheme and demonstrate that Dekey incurs limited overhead in realistic environments.","cloud computing;private key cryptography;public key cryptography;storage management;secure deduplication;data deduplication;reliable convergent key management;cloud storage;storage space reduction;convergent encryption;baseline key management scheme;Dekey;security model;Ramp secret sharing scheme;Encryption;Reliability;Servers;Data deduplication;Deduplication;proof of ownership;convergent encryption;key management","Start with baseline Secure Deduplicating - client keeps secret their symmetric key that they encrypt CE keys, and uses Tag(C)&nbsp; to id Files/Chunks. System works at the File level first, to reduce key mgmt burden. If File is not a duplicate, chunks are used. PoW is used to verify client actually owns File/Block.<div>DeKey enhancements.</div>","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Secure Deduplication with Efficient and Reliable Convergent Key Management.pdf","","","",""
"Conference paper","Rivest RL","","All-or-nothing encryption and the package transform","","","","","Fast Software Encryption","","","1997","","","210-218","All","Encryption Techniques;SSSS/RSSS/Convergent Dispersal","","","Springer Berlin Heidelberg","","","","","1997","","","","","","","http://dx.doi.org/10.1007/BFb0052348;https://link.springer.com/chapter/10.1007/BFb0052348;https://link.springer.com/content/pdf/10.1007/BFb0052348.pdf","10.1007/BFb0052348","","","","We present a new mode of encryption for block ciphers, which we call all-or-nothing encryption. This mode has the interesting defining property that one must decrypt the entire ciphertext before one can determine even one message block. This means that brute-force searches against all-or-nothing encryption are slowed down by a factor equal to the number of blocks in the ciphertext. We give a specific way of implementing all-or-nothing encryption using a “package transform≓ as a pre-processing step to an ordinary encryption mode. A package transform followed by ordinary codebook encryption also has the interesting property that it is very efficiently implemented in parallel. All-or-nothing encryption can also provide protection against chosen-plaintext and related-message attacks.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rivest 1997 - All-or-nothing encryption and the package transform.pdf","","","",""
"Miscellaneous","","","[MS-RDC].pdf","","","","","","","","","","","","All;Superchunking Paper Sources/CDC/Chunking","Chunking","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/[MS-RDC].pdf - [MS-RDC].pdf","","","",""
"Journal article","","","blake2_20130129.pdf","","","","","","","","","","","","All","Encryption Techniques","","","","","","","","","","","","","","","chrome-extension://dboboillnhldihjigadainihehfoackd/pdf/PMButton.html?https://blake2.net/blake2_20130129.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/blake2_20130129.pdf - blake2_20130129.pdf","","","",""
"Journal article","Meng W,Ge J,Jiang T","","Secure Data Deduplication with Reliable Data Deletion in Cloud","Internat. J. Found. Comput. Sci.","International Journal of Foundations of Computer Science","","","","","","2019","30","04","551-570","All","Ownership Management","","","World Scientific Publishing Co.","","","","","2019-06-01","","","","","0129-0541","","https://doi.org/10.1142/S0129054119400124;http://dx.doi.org/10.1142/S0129054119400124;https://www.worldscientific.com/doi/abs/10.1142/S0129054119400124","10.1142/S0129054119400124","","","","A cloud storage system which incorporates the deletion and deduplication functionalities will have both security and efficiency advantages over exiting solutions which provide only one of them. However, the security models of secure data deletion and data deduplication functionalities are not compatible with each other, which will cause security and efficiency vulnerability under coercive adversaries. To solve these security and efficiency challenges, we define and construct a scheme, whose security relies on the proper erasure of keys in the wrapped key tree and periodical update of the deduplication encryption keys. Moreover, we enhance the efficiency of the proposed scheme by introducing incremental data update, where only the changed part is encrypted/decrypted and uploaded/downloaded in data updating. Further security analysis shows that the proposed scheme is secure against coercive attack. Finally, the practical implementation shows that our scheme is performance efficient in computation, storage and communication for both the cloud storage server and users.","","doi: 10.1142/S0129054119400124","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Akbar M,Balachandrudu KE,Peddi P","","Encryption to decryption statistics for secure data with independent servers for deduplication","International Journal of Engineering and Advanced Technology","","","","","","","2019","9","1","6490-6493","All","Secure Dedup","","","Blue Eyes Intelligence Engineering and Sciences Publication","","","","","2019","","","","","2249-8958","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074572496&doi=10.35940%2fijeat.A1068.109119&partnerID=40&md5=3f542bb9bfeccc93be0edb89b6cd362e;https://www.scopus.com/record/display.uri?eid=2-s2.0-85074572496&origin=resultslist&sort=plf-f&src=s&st1=secure+deduplication&st2=&sid=5720af366ae097f007bf42428171191f&sot=b&sdt=b&sl=35&s=TITLE-ABS-KEY%28secure+deduplication%29&relpos=3&citeCnt=0&searchTerm=;https://doi.org/10.35940/ijeat.A1068.109119","","","","","Encoding information on the customer side before transferring it to distributed storage is fundamental for ensuring clients' protection. Anyway customer side encryption is inconsistent with the standard routine with regards to deduplication in distributed storage administrations. Deduplication of records on the capacity servers has been very much clarified on different arrangements which utilized autonomous servers. An answer has been proposed considering customer side encryption and server side deduplication and guaranteeing security while utilizing a solitary server for the entire procedure. This arrangement was tended to at record level and had the thought to actualize square level deduplication utilizing a similar deduplication convention. We examined the odds of executing square level deduplication considering the single server record level deduplication plan of the base convention. We executed and examined this situation and played out an unpleasant examination with the record level deduplication by recreation. Both the plans appeared to be similar. We exhibit that our plan gives preferable security ensures over past efforts. We present the main secure cross-client deduplication plot that supports customer side encryption without requiring any extra autonomous servers. Curiously, the plan depends on utilizing a PAKE (secret word confirmed key trade) convention. © 2019, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.","Deduplication; Encryption; File level deduplication; Hashing; Independent Servers","Export Date: 19 November 2019
Correspondence Address: Akbar, M.; Shri JJT UniversityIndia; email: akb.mtech@gmail.com
References: Jayapandian, N., Mdzubair Rahman, A.M.J., Secure Deduplication for Cloud Storage Using Interactive Message-Locked Encryption with Convergent Encryption, To Reduce Storage Space (2018) Brazilian Archives of Biology and Technology, 61 (5), pp. 1-9; Wenhai, S., Ning, Z., Wenjing, L., Thomas Hou, Y., Tapping the Potential: Secure Chunk-based Deduplication of Encrypted Data for Cloud Backup (2015) IEEE Trans Parallel Distrib Syst., 25 (6), pp. 1615-1625; Puzio, P., Molva, R., Onen, M., ClouDedup: Secure Deduplication with Encrypted Data for Cloud Storage (2014) IEEE Trans Parallel Distrib Systems, 26 (5), pp. 1206-1216; Deng, Z., Tan, X., Chen, S., An Encrypted File Deduplication Scheme with Permission in Cloud Storage (2018) Mathematical Problems in Engineering, p. 13. , 2018, pages; Storer, M., Greenan, K., Long, D., Miller, E., Secure data deduplication (2008) Proceedings of the 4Th ACM International Workshop on Storage Security and Survivability-Storagess '08; Xu, J., Chang, E., Zhou, J., Weak leakage-resilient client-side deduplication of encrypted data in cloud storage (2013) Proceedings of the 8Th ACM SIGSAC Symposium on Information, Computer and Communications Security-Asia CCS '13; Harnik, D., Pinkas, B., Shulman-Peleg, A., Side Channels in Cloud Services: Deduplication in Cloud Storage (2010) IEEE Security & Privacy Magazine, 8 (6), pp. 40-47; Ng, W., Wen, Y., Zhu, H., Private Data deduplication protocols in cloud storage (2012) Proceedings of the 27Th Annual ACM Symposium on Applied Computing-Sac '12; Liu, J., Asokan, N., Pinkas, B., Secure Deduplication of Encrypted Data without Additional Independent Servers, appeared in CCS ‘15 Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, , 2015 at New York, United States; Abdalla, M., Pointcheval, D., Simple password-based encrypted key exchange protocols (2005) Topics in Cryptology-Ct-Rsa 2005, the Cryptographers’ Track at the RSA Conference 2005, 3376, pp. 191-208. , A. Menezes, editor, San Francisco, CA, USA, February 14-18, 2005, Proceedings, volume, of Lecture Notes in Computer Science, pages, Springer; Bellare, M., Keelveedhi, S., Ristenpart, T., Dupless: Server-aided encryption for deduplicated storage (2013) Proceedings of the 22Nd USENIX Conference on Security, SEC’13, pp. 179-194. , Berkeley, CA, USA, USENIX Association; Clarke, I., Sandberg, O., Wiley, B., Hong, T., Freenet: A Distributed Anonymous Information Storage and Retrieval System Designing Privacy Enhancing Technologies, 3 (2), pp. 46-66","","Shri JJT UniversityRajasthan, India","","","","","","","","","","","","","","","","","","","","","All Papers/A/Akbar et al. 2019 - Encryption to decryption statistics for secure data with independent servers for deduplication.pdf","","","",""
"Journal article","Wang L,Wang B,Song W,Zhang Z","","A key-sharing based secure deduplication scheme in cloud storage","Inf. Sci. ","Information sciences","","","","","","2019","504","","48-60","All;Thesis","Secure Dedup;PoW","","","","","","","","2019-12-01","","","","","0020-0255","","http://www.sciencedirect.com/science/article/pii/S0020025519306644;http://dx.doi.org/10.1016/j.ins.2019.07.058","10.1016/j.ins.2019.07.058","","","","The data deduplication technique can efficiently eliminate redundant data by keeping only one copy of the duplicate data. Convergent encryption (CE) has been widely used in secure deduplication to save storage space and reduce data upload bandwidth, but it still faces two problems. One is that CE is not semantically secure, and suffers from an offline brute-force attack when the data is selected from a predictable set. Another is the convergent key (CK) management problem. CE requires each user holds an independent master key to encrypt its CK then stores them in the cloud, thus different users would store the same key for the duplicate copies. As the scale of users and data expand, the number of CK increase linearly. Therefore, so many keys stored repeatedly is a type of redundancy and brings about a key management issue. To enhance the security of CE, current schemes usually interact with a third party to generate a CK, but this brings an additional burden to the system. Recently, several schemes have been proposed for efficient CK management, but these schemes are confronted with a heavy computation and communication overhead and cannot resist the collusion attack. To deal with the above two problems, we propose a key-sharing method based on proof of ownership for secure deduplication. In the new scheme, only the initial uploader of the data owner encrypts the data with a randomly-chosen CK and then distributes the CK in the cloud, and only the users possessing the claimed data can retrieve the CK. The CK only needs to store once for a single duplicate data. Furthermore, our scheme adopts a deduplication check on the plaintexts and the consistency policy, and only a few owners need to encrypt the duplicate data. Analysis shows that our scheme is more efficient and remains secure in the proposed security model.","Cloud storage; Data deduplication; Proof of ownership; Key-sharing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/W/Wang et al. 2019 - A key-sharing based secure deduplication scheme in cloud storage.pdf","","","",""
"Journal article","Li S,Xu C,Zhang Y","","CSED: Client-Side encrypted deduplication scheme based on proofs of ownership for cloud storage","Journal of Information Security and Applications","","","","","","","2019","46","","250-258","All;Thesis","PoW","","","","","","","","2019-06-01","","","","","2214-2126","","http://www.sciencedirect.com/science/article/pii/S2214212618301790;http://dx.doi.org/10.1016/j.jisa.2019.03.015","10.1016/j.jisa.2019.03.015","","","","As digital data are explosively generated nowadays, data management becomes a critical problem, which makes cloud storage services important and popular. In reality, the storage overhead can be reduced significantly by performing date deduplication. Among the outsourced data, some of them are very personal and sensitive, and should be prevented for any leakage. Generally, if clients conventionally encrypt the data, deduplication is lost. Message-locked encryption (MLE) is a cryptographic primitive supporting encrypted data deduplication. A secure client-side deduplication scheme can be built upon MLE to reduce both communication and computation overhead for cloud storage systems, where a client interacts with the cloud server to check the duplicate data and only the data which has not been outsourced by other clients before is required to be uploaded. However, existing client-side encrypted data deduplication schemes are confronted with brute-force attacks that can recover files falling into a known set. Furthermore, existing schemes are vulnerable to illegal content distribution attacks, where the adversary can distribute data to other users via the cloud server without detecting. In this paper, we propose a secure and efficient client-side encrypted data deduplication scheme (CSED). In CSED, a dedicated key server is introduced in generating MLE keys to resist brute-force attacks. We propose a Bloom filter-based proofs of ownership (PoW) mechanism and integrate it into CSED to resist illegal content distribution attacks. Moreover, a hierarchical storage architecture is employed to improve the I/O efficiency on the cloud server. Security analysis and performance evaluation demonstrate that CSED is secure and efficient.","Cloud storage; Encrypted data deduplication; Proofs of ownership; Brute-force attacks; Illegal content distribution","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2019 - CSED - Client-Side encrypted deduplication scheme based on proofs of ownership for cloud storage.pdf","","","",""
"Journal article","Fan Y,Lin X,Liang W,Tan G,Nanda P","","A secure privacy preserving deduplication scheme for cloud computing","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2019","101","","127-135","All","Secure Dedup","","","","","","","","2019-12-01","","","","","0167-739X","","http://www.sciencedirect.com/science/article/pii/S0167739X18329649;http://dx.doi.org/10.1016/j.future.2019.04.046","10.1016/j.future.2019.04.046","","","","Data deduplication is a key technique to improve storage efficiency in cloud computing. By pointing redundant files to a single copy, cloud service providers greatly reduce their storage space as well as data transfer costs. Despite of the fact that the traditional deduplication approach has been adopted widely, it comes with a high risk of losing data confidentiality because of the data storage models in cloud computing. To deal with this issue in cloud storage, we first propose a TEE (trusted execution environment) based secure deduplication scheme. In our scheme, each cloud user is assigned a privilege set; the deduplication can be performed if and only if the cloud users have the correct privilege. Moreover, our scheme augments the convergent encryption with users’ privileges and relies on TEE to provide secure key management, which improves the ability of such cryptosystem to resist chosen plaintext attacks and chosen ciphertext attacks. A security analysis indicates that our scheme is secure enough to support data deduplication and to protect the confidentiality of sensitive data. Furthermore, we implement a prototype of our scheme and evaluate the performance of our prototype, experiments show that the overhead of our scheme is practical in realistic environments.","Deduplication; Trusted execution environment; Cloud storage; Encryption","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Fan et al. 2019 - A secure privacy preserving deduplication scheme for cloud computing.pdf","","","",""
"Computer program","","","librabinpoly","","","","","","","","","","","","All","Code/OpenSource","Github","","","","","","","","","2019-11-19","","","","","https://github.com/stevegt/librabinpoly;https://github.com/stevegt/librabinpoly/blob/master/src/rabinpoly.c","","","","","Rabin fingerprinting and deduplication library in C - stevegt/librabinpoly","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Book chapter","Broder AZ","","Some applications of Rabin’s fingerprinting method","","","Sequences II","","","","","1993","","","143-152","All;Superchunking Paper Sources/CDC/Chunking;Thesis","Chunking","","","Springer","","","","","1993","","","","","","","https://link.springer.com/chapter/10.1007/978-1-4613-9323-8_11;https://www.researchgate.net/profile/Andrei_Broder/publication/2688260_Some_applications_of_Rabin's_fingerprinting_method/links/0912f51487bc1b7e39000000/Some-applications-of-Rabins-fingerprinting-method.pdf","","","","","","","Can't get access to the chapter of this book.","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Broder 1993 - Some applications of Rabin’s fingerprinting method.pdf","","","",""
"Journal article","Cui Y,Lai Z,Wang X,Dai N","","QuickSync: Improving Synchronization Efficiency for Mobile Cloud Storage Services","IEEE Trans. Mob. Comput.","IEEE Transactions on Mobile Computing","","","","","","2017","16","12","3513-3526","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking","Chunking;Mobile Dedup;Synchronization","","","","","","","","2017-12","","","","","","","http://dx.doi.org/10.1109/TMC.2017.2693370;https://ieeexplore.ieee.org/abstract/document/7898362/;https://ieeexplore.ieee.org/iel7/7755/4358975/07898362.pdf","10.1109/TMC.2017.2693370","","","","Mobile cloud storage services have gained phenomenal success in recent few years. In this paper, we identify, analyze, and address the synchronization (sync) inefficiency problem of modern mobile cloud storage services. Our measurement results demonstrate that existing commercial sync services fail to make full use of available bandwidth, and generate a large amount of unnecessary sync traffic in certain circumstances even though the incremental sync is implemented. For example, a minor document editing process in Dropbox may result in sync traffic 10 times that of the modification. These issues are caused by the inherent limitations of the sync protocol and the distributed architecture. Based on our findings, we propose QuickSync, a system with three novel techniques to improve the sync efficiency for mobile cloud storage services, and build the system on two commercial sync services. Our experimental results using representative workloads show that QuickSync is able to reduce up to 73.1 percent sync time in our experiment settings.","cloud computing;mobile computing;storage management;synchronisation;sync protocol;QuickSync;sync efficiency;commercial sync services;73.1 percent sync time;synchronization inefficiency problem;modern mobile cloud storage services;unnecessary sync traffic;incremental sync;Dropbox;Synchronization;Cloud computing;Mobile communication;Servers;Protocols;Mobile computing;Synchronization;Storage management;Mobile cloud storage;mobile networks;measurement;synchronization efficiency","Read Fall 2019. Does not seem to handle secure deduplication, or unencrypted file synchronization. Improves file change syncing in Dropbox and Seafile. Services wait for confirmation between each chunk upload or retrieve. Also Delta encoding seems to be at the file level, not chunk level. QuickSync uses network aware chunker, too small a chunk size finds a lot of dups, but computation overhead is high, so it switches between 2 chunking strategies. Low bandwidth uses aggressive chunking to reduce transmission time. Sufficient bandwidth uses larger chunks for lower computation costs. Adds buffering/resume on network interruptions.","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cui et al. 2017 - QuickSync - Improving Synchronization Efficiency for Mobile Cloud Storage Services.pdf","","","",""
"Miscellaneous","Li M,Qin C,Li J,Lee PP","","CDStore: Toward Reliable, Secure, and Cost-Efficient Cloud Storage via Convergent Dispersal","IEEE Internet Computing","","","","","","","2016","20","3","45-53","All;Thesis;Superchunking Paper Sources;SCAIL Bibliography;p-scailbib","SSSS/RSSS/Convergent Dispersal;Two-stage","","","","","","","","2016","","","","","","","http://dx.doi.org/10.1109/mic.2016.45","10.1109/mic.2016.45","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2016 - CDStore - Toward Reliable, Secure, and Cost-Efficient Cloud Storage via Convergent Dispersal.pdf","","","",""
"Journal article","Kwon H,Hahn C,Kang K,Hur J","","Secure deduplication with reliable and revocable key management in fog computing","Peer-to-Peer Networking and Applications","","","","","","","2019","12","4","850-864","All","Secure Dedup","","","","","","","","2019-07-01","","","","","1936-6450","","https://doi.org/10.1007/s12083-018-0682-9;http://dx.doi.org/10.1007/s12083-018-0682-9;https://link.springer.com/article/10.1007/s12083-018-0682-9","10.1007/s12083-018-0682-9","","","","A secure deduplication technique removes duplicate data and stores only single copy to efficiently utilize the storage while guaranteeing the privacy of the data. Thus, it is a necessary technology for resource-limited for devices to save storages. However, most of the existing deduplication schemes based on convergent encryption suffer from 1) a convergent encryption key management problem and 2) a dynamic ownership management problem. In key management, convergent encryption generates a number of encryption keys whose size increases linearly with the number of distinct data. In terms of dynamic ownership management, although the ownership of data in a fog device or cloud storage frequently changes in real-world applications, supporting ownership changes are difficult because the convergent encryption keys are only bound to the data. In order to solve these problems, we present a secure deduplication scheme that features reliable and scalable key management based on pairing-based cryptography and supports dynamic ownership management. The proposed scheme avoids additional costs associated with distributing key components on secure channels and ownership keys on the user side yet guarantees secure key and ownership management.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Shin Y,Koo D,Yun J,Hur J","","Decentralized Server-aided Encryption for Secure Deduplication in Cloud Storage","IEEE Trans. Serv. Comput.","IEEE Transactions on Services Computing","","","","","","2017","","","1-1","All","Secure Dedup","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1109/TSC.2017.2748594;https://ieeexplore.ieee.org/abstract/document/8025407/;https://ieeexplore.ieee.org/iel7/4629386/4629387/08025407.pdf","10.1109/TSC.2017.2748594","","","","Cloud storage provides scalable and low cost resources featuring economies of scale based on multi-tenant architecture. As the amount of data outsourced grows explosively, data deduplication, a technique that eliminates data redundancy, becomes essential. However, deduplication leads to problems with data confidentiality, thereby necessitating secure deduplication solutions. Server-aided encryption schemes have been proposed to achieve the strongest confidentiality but with the cost of managing a key server (KS). Previous schemes, however, are based on a centralized KS that uses only a single secret key assuming a single KS in the system. In cloud storage where multi-tenancy and scalability are crucial, such schemes degrade not only the effectiveness of deduplication but also the scalability with increasing users. In this paper, we extend server-aided encryption to a decentralized setting that consists of multiple KSs. The key idea of our proposed scheme is to construct an inter-KS deduplication algorithm, by which a cloud storage service provider can perform deduplication over ciphertexts from different KSs within a tenant or across tenants. This way, our scheme simultaneously offers flexibility of KS management and cross-tenant deduplication over encrypted data. The novelty of the approach is using a decentralized architecture that does not require any centralized entities for the coordination or pre-sharing of secrets among KSs. Therefore, it allows cloud storage services to offer high deduplication efficiency and scalability while preserving strong data confidentiality. We show the result of performance analysis on the proposed scheme by conducting extensive experiments. In addition, our security analysis demonstrate that the proposed scheme satisfies all desired security properties.","Encryption;Cloud computing;Servers;Scalability;Redundancy;Cloud storage;Cross-tenant data deduplication;Server-aided encryption;Message-locked encryption","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shin et al. 2017 - Decentralized Server-aided Encryption for Secure Deduplication in Cloud Storage.pdf","","","",""
"Conference paper","Li M,Qin C,Lee PP,Li J","","Convergent Dispersal: Toward Storage-Efficient Security in a Cloud-of-Clouds","","","","","6th ${$USENIX$}$ Workshop on Hot Topics in Storage and File Systems (HotStorage 14)","","","2014","","","","All","SSSS/RSSS/Convergent Dispersal","","","","","","","","2014","","","","","","","https://www.usenix.org/conference/hotstorage14/workshop-program/presentation/li_mingqiang;https://www.usenix.org/system/files/conference/hotstorage14/hotstorage14-paper-li_mingqiang.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Li et al. 2014 - Convergent Dispersal - Toward Storage-Efficient Security in a Cloud-of-Clouds.pdf","","","",""
"Website","","","PBC Library - Pairing-Based Cryptography - About","","","","","","","","","","","","All","Code/OpenSource","","","","","","","","","","2019-11-10","","","","","https://crypto.stanford.edu/pbc/","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Dave J,Laxmi V,Faruki P,Gaur M,Shah B","","Bloom Filter Based Privacy Preserving Deduplication System","","","","","Security and Privacy","","","2019","","","17-34","All","Secure Dedup","","","Springer Singapore","","","","","2019","","","","","","","http://dx.doi.org/10.1007/978-981-13-7561-3_2;https://link.springer.com/chapter/10.1007/978-981-13-7561-3_2;https://link-springer-com.ezproxy.lib.bbk.ac.uk/content/pdf/10.1007%2F978-981-13-7561-3_2.pdf","10.1007/978-981-13-7561-3_2","","","","Deduplication is a data reduction technique which eliminates uploading and storing redundant data. Therefore, it is widely adopted in cloud storage services to reduce communication and storage overhead. However, deduplication can be used as a side channel to learn the existence of a particular data in cloud storage thereby raising significant privacy issues. The existing solutions delay deduplication process to hide information regarding the presence of data. These solutions increase communication overhead as the client needs to send data even if it is present on storage. In this paper, we present a novel privacy preserving deduplication approach using bloom filter. In our approach, bloom filter (containing blocks of data) is used as a deduplication identity. When a client sends an upload request, the server responds by sending a genuine bloom filter (if data exists) along with some dummy filters. Now, that client who genuinely owns the data, can learn the existence information by computing bloom filter of the data. Further, client does not need to send the data if it exists on storage. Security analysis proves that our approach provides privacy to data at deduplication system. We implement the approach and demonstrate that communication overhead is significantly less in our approach than the existing approaches.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Ateniese G,Camenisch J,Hohenberger S,De Medeiros B","","Practical Group Signatures without Random Oracles","IACR Cryptology ePrint Archive","","","","","","","2005","2005","","385","All","Sec Techniques/Protocols;Security Proof","","","Citeseer","","","","","2005","","","","","","","http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.60.5286&rep=rep1&type=pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Ateniese et al. 2005 - Practical Group Signatures without Random Oracles.pdf","","","",""
"Journal article","Shamir A","","How to share a secret","Communication of the ACM","Communications of the ACM","","","","","","1979","22","11","612-613","All","Security Proof;Sec Techniques/Protocols","","","ACM","","","","","1979","","","","","0001-0782","","https://dl.acm.org/citation.cfm?id=359176;https://apps.dtic.mil/dtic/tr/fulltext/u2/a069397.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shamir 1979 - How to share a secret.pdf","","","",""
"Miscellaneous","Goldwasser S,Micali S","","Probabilistic encryption","Journal of Computer and System Sciences","","","","","","","1984","28","2","270-299","All","Security Proof","","","","","","","","1984","","","","","","","http://dx.doi.org/10.1016/0022-0000(84)90070-9","10.1016/0022-0000(84)90070-9","","","","A new probabilistic model of data encryption is introduced. For this model, under suitable complexity assumptions, it is proved that extracting any information about the cleartext from the cyphertext is hard on the average for an adversary with polynomially bounded computational resources. The proof holds for any message space with any probability distribution. The first implementation of this model is presented. The security of this implementation is proved under the intractability assumption of deciding Quadratic Residuosity modulo composite numbers whose factorization is unknown.","","Original definition for Polynomial Security and Semantic Security. Polynomial Security - Adversary cannot distinguish two messages in Polynomial Time. RSA is not Polynomial Secure. Semantic Security - Whatever could be computed about the cleartext given the ciphertext can also be computed without the ciphertext.","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Goldwasser and Micali 1984 - Probabilistic encryption.pdf","","","",""
"Website","Dutch M","","Understanding_Data_Deduplication_Ratios-20080718.pdf","","","","","","","","2008","","","","All","Datasets","","","","","","","","2008","","","","","","","http://www.snia.org/sites/default/files/Understanding_Data_Deduplication_Ratios-20080718.pdf","","","","","Storage Networking Industry Association","","","","","","Article","","","","","","","","","","","","","","","","","","","All Papers/D/Dutch 2008 - Understanding_Data_Deduplication_Ratios-20080718.pdf","","","",""
"Conference paper","Cash D,Kiltz E,Shoup V","","The Twin Diffie-Hellman Problem and Applications","","","","","Advances in Cryptology – EUROCRYPT 2008","","","2008","","","127-145","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2008","","","","","","","http://dx.doi.org/10.1007/978-3-540-78967-3_8;https://link.springer.com/chapter/10.1007/978-3-540-78967-3_8;https://link.springer.com/content/pdf/10.1007/978-3-540-78967-3_8.pdf","10.1007/978-3-540-78967-3_8","","","","We propose a new computational problem called the twin Diffie-Hellman problem. This problem is closely related to the usual (computational) Diffie-Hellman problem and can be used in many of the same cryptographic constructions that are based on the Diffie-Hellman problem. Moreover, the twin Diffie-Hellman problem is at least as hard as the ordinary Diffie-Hellman problem. However, we are able to show that the twin Diffie-Hellman problem remains hard, even in the presence of a decision oracle that recognizes solutions to the problem — this is a feature not enjoyed by the ordinary Diffie-Hellman problem. In particular, we show how to build a certain “trapdoor test” which allows us to effectively answer such decision oracle queries, without knowing any of the corresponding discrete logarithms. Our new techniques have many applications. As one such application, we present a new variant of ElGamal encryption with very short ciphertexts, and with a very simple and tight security proof, in the random oracle model, under the assumption that the ordinary Diffie-Hellman problem is hard. We present several other applications as well, including: a new variant of Diffie and Hellman’s non-interactive key exchange protocol; a new variant of Cramer-Shoup encryption, with a very simple proof in the standard model; a new variant of Boneh-Franklin identity-based encryption, with very short ciphertexts; a more robust version of a password-authenticated key exchange protocol of Abdalla and Pointcheval.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cash et al. 2008 - The Twin Diffie-Hellman Problem and Applications.pdf","","","",""
"Journal article","Kaduk B,Network Working Group,Ladd Internet-Draft W","","SPAKE2, a PAKE","","","","","","","","2000","","","","All","Sec Techniques/Protocols","","","","","","","","2000","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaduk et al. 2000 - SPAKE2, a PAKE.pdf","","","",""
"Conference paper","Zhang Y,Jiang H,Feng D,Xia W,Fu M,Huang F,Zhou Y","","AE: An Asymmetric Extremum content defined chunking algorithm for fast and bandwidth-efficient data deduplication","","","","","2015 IEEE Conference on Computer Communications (INFOCOM)","","","2015","","","1337-1345","All;Superchunking Paper Sources/CDC/Chunking;Superchunking Paper Sources/CDC/Chunking/Seminal-Chunking;Thesis","Chunking","","","","","","","","2015-04","","","","","","","http://dx.doi.org/10.1109/INFOCOM.2015.7218510;https://ieeexplore.ieee.org/abstract/document/7218510/;https://www.researchgate.net/profile/Yukun_Zhou/publication/280036132_AE_An_Asymmetric_Extremum_Content_Defined_Chunking_Algorithm_for_Fast_and_Bandwidth-Efficient_Data_Deduplication/links/55a4c4e808ae00cf99c91b40.pdf","10.1109/INFOCOM.2015.7218510","","","","Data deduplication, a space-efficient and bandwidth-saving technology, plays an important role in bandwidth-efficient data transmission in various data-intensive network and cloud applications. Rabin-based and MAXP-based Content-Defined Chunking (CDC) algorithms, while robust in finding suitable cut-points for chunk-level redundancy elimination, face the key challenges of (1) low chunking throughput that renders the chunking stage the deduplication performance bottleneck and (2) large chunk-size variance that decreases deduplication efficiency. To address these challenges, this paper proposes a new CDC algorithm called the Asymmetric Extremum (AE) algorithm. The main idea behind AE is based on the observation that the extreme value in an asymmetric local range is not likely to be replaced by a new extreme value in dealing with the boundaries-shift problem, which motivates AE's use of asymmetric (rather than symmetric as in MAXP) local range to identify cut-points and simultaneously achieve high chunking throughput and low chunk-size variance. As a result, AE simultaneously addresses the problems of low chunking throughput in MAXP and Rabin and high chunk-size variance in Rabin. The experimental results based on four real-world datasets show that AE improves the throughput performance of the state-of-the-art CDC algorithms by 3x while attaining comparable or higher deduplication efficiency.","computer networks;data handling;AE algorithms;asymmetric extremum content defined chunking algorithm;fast data deduplication;bandwidth-efficient data deduplication;bandwidth saving technology;bandwidth efficient data transmission;cloud applications;content defined chunking algorithms;CDC algorithm;asymmetric extremum algorithm;Throughput;Algorithm design and analysis;Computers;Redundancy;Power capacitors;Conferences;Arrays","","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhang et al. 2015 - AE - An Asymmetric Extremum content defined chunking algorithm for fast and bandwidth-efficient data deduplication.pdf","","","",""
"Journal article","Xia W,Jiang H,Feng D,Tian L,Fu M,Zhou Y","","Ddelta: A deduplication-inspired fast delta compression approach","Performance Evaluation","","","","","","","2014","79","","258-272","All;Thesis","Secure Dedup;Delta Compression;Chunking","","","","","","","","2014-09-01","","","","","0166-5316","","http://www.sciencedirect.com/science/article/pii/S0166531614000790;http://dx.doi.org/10.1016/j.peva.2014.07.016;https://www.sciencedirect.com/science/article/pii/S0166531614000790;http://ranger.uta.edu/~jiang/publication/Journals/2014/2014-Perf%20Eval%20-Ddelta-%20A%20Deduplication-Inspired%20Fast%20Delta%20Compression%20Approach.pdf","10.1016/j.peva.2014.07.016","","","","Delta compression is an efficient data reduction approach to removing redundancy among similar data chunks and files in storage systems. One of the main challenges facing delta compression is its low encoding speed, a worsening problem in face of the steadily increasing storage and network bandwidth and speed. In this paper, we present Ddelta, a deduplication-inspired fast delta compression scheme that effectively leverages the simplicity and efficiency of data deduplication techniques to improve delta encoding/decoding performance. The basic idea behind Ddelta is to (1) accelerate the delta encoding and decoding processes by a novel approach of combining Gear-based chunking and Spooky-based fingerprinting for fast identification of duplicate strings for delta calculation, and (2) exploit content locality of redundant data to detect more duplicates by greedily scanning the areas immediately adjacent to already detected duplicate chunks/strings. Our experimental evaluation of a Ddelta prototype based on real-world datasets shows that Ddelta achieves an encoding speedup of 2.5×–8× and a decoding speedup of 2×–20× over the classic delta-compression approaches Xdelta and Zdelta while achieving a comparable level of compression ratio.","Delta compression; Deduplication; Content locality; Content-defined chunking; Fingerprinting","Gear based chunking and Spooky has fingerprinting. Authors evolve technology in FastCDC 2016?","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2014 - Ddelta - A deduplication-inspired fast delta compression approach.pdf","","","",""
"Conference paper","Freedman MJ,Ishai Y,Pinkas B,Reingold O","","Keyword Search and Oblivious Pseudorandom Functions","","","","","Theory of Cryptography","","","2005","","","303-324","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2005","","","","","","","http://dx.doi.org/10.1007/978-3-540-30576-7_17;https://link.springer.com/chapter/10.1007/978-3-540-30576-7_17;https://link.springer.com/content/pdf/10.1007/978-3-540-30576-7_17.pdf","10.1007/978-3-540-30576-7_17","","","","We study the problem of privacy-preserving access to a database. Particularly, we consider the problem of privacy-preserving keyword search (KS), where records in the database are accessed according to their associated keywords and where we care for the privacy of both the client and the server. We provide efficient solutions for various settings of KS, based either on specific assumptions or on general primitives (mainly oblivious transfer). Our general solutions rely on a new connection between KS and the oblivious evaluation of pseudorandom functions (OPRFs). We therefore study both the definition and construction of OPRFs and, as a corollary, give improved constructions of OPRFs that may be of independent interest.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/F/Freedman et al. 2005 - Keyword Search and Oblivious Pseudorandom Functions.pdf","","","",""
"Conference paper","Jarecki S,Liu X","","Efficient Oblivious Pseudorandom Function with Applications to Adaptive OT and Secure Computation of Set Intersection","","","","","Theory of Cryptography","","","2009","","","577-594","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2009","","","","","","","http://dx.doi.org/10.1007/978-3-642-00457-5_34;https://link.springer.com/chapter/10.1007/978-3-642-00457-5_34;https://link.springer.com/content/pdf/10.1007/978-3-642-00457-5_34.pdf","10.1007/978-3-642-00457-5_34","","","","An Oblivious Pseudorandom Function (OPRF) [15] is a two-party protocol between sender S and receiver R for securely computing a pseudorandom function fk(·) on key k contributed by S and input x contributed by R, in such a way that receiver R learns only the value fk(x) while sender S learns nothing from the interaction. In other words, an OPRF protocol for PRF fk(·) is a secure computation for functionality $\mathcal F_{\mathsf{OPRF}}:(k,x)\rightarrow(\perp,f_k(x))$.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/J/Jarecki and Liu 2009 - Efficient Oblivious Pseudorandom Function with Applications to Adaptive OT and Secure Computation of Set Intersection.pdf","","","",""
"Journal article","Elgamal T","","A public key cryptosystem and a signature scheme based on discrete logarithms","IEEE Trans. Inf. Theory","IEEE transactions on information theory / Professional Technical Group on Information Theory","","","","","","1985","31","4","469-472","All","Sec Techniques/Protocols","","","","","","","","1985-07","","","","","0018-9448","","http://dx.doi.org/10.1109/TIT.1985.1057074;https://ieeexplore.ieee.org/abstract/document/1057074/;https://link.springer.com/content/pdf/10.1007/3-540-39568-7_2.pdf","10.1109/TIT.1985.1057074","","","","A new signature scheme is proposed, together with an implementation of the Diffie-Hellman key distribution scheme that achieves a public key cryptosystem. The security of both systems relies on the difficulty of computing discrete logarithms over finite fields.","Cryptography;Logarithmic arithmetic","","","","","","","","","","","","","","","","","","","","","","","","All Papers/E/Elgamal 1985 - A public key cryptosystem and a signature scheme based on discrete logarithms.pdf","","","",""
"Conference paper","Ma J,Stones RJ,Ma Y,Wang J,Ren J,Wang G,Liu X","","Lazy exact deduplication","","","","","2016 32nd Symposium on Mass Storage Systems and Technologies (MSST)","","","2016","","","1-10","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Grouped by Publication/MSST","Secure Dedup;Fingerprint-Indexing","","","","","","","","2016-05","","","","","","","http://dx.doi.org/10.1109/MSST.2016.7897081;https://ieeexplore.ieee.org/document/7897081","10.1109/MSST.2016.7897081","","","","During data deduplication, on-disk fingerprint lookups lead to high disk traffic, resulting in a bottleneck. In this paper, we propose a “lazy” data deduplication method which buffers incoming fingerprints and performs on-disk lookups in batches, aiming to reduce the disk bottleneck. In deduplication in general, prefetching is used to improve the cache hit rate by exploiting locality within the incoming fingerprint stream. For lazy deduplication, we design a buffering strategy that preserves locality in order to similarly facilitate prefetching. Experimental results indicate that the lazy method improves fingerprint identification performance by over 50% compared with an “eager” method with the same data layout.","cache storage;lazy exact deduplication;on-disk fingerprint lookups;disk traffic;lazy data deduplication method;disk bottleneck;cache hit rate;fingerprint stream;buffering strategy;fingerprint identification performance;Prefetching;Graphics processing units;Fingerprint recognition;Metadata;Acceleration;Memory management;Indexes","Uses locality to pre-fetch groups chunk fingerprints. Uses GPU. Claims to speedup by 50%.","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Ma et al. 2016 - Lazy exact deduplication.pdf","","","",""
"Conference paper","Nassar M,Erradi A,Malluhi QM","","Paillier's encryption: Implementation and cloud applications","","","","","2015 International Conference on Applied Research in Computer Science and Engineering (ICAR)","","","2015","","","1-5","All","Sec Techniques/Protocols","","","","","","","","2015-10","","","","","","","http://dx.doi.org/10.1109/ARCSE.2015.7338149","10.1109/ARCSE.2015.7338149","","","","Paillier's additive homomorphic encryption is increasingly used in recent research in the field of cloud secure outsourcing and privacy-preserving computation in addition to other cryptographic tools such as garbled circuits. In this paper, we review Paillier's encryption and its application to privacy-preserving computation outsourcing and secure online voting. We present a new implementation of Paillier's cryptosystem using Python as for interface language and fast GMP C-routines for arithmetic operations.","cloud computing;cryptography;data privacy;cloud applications;Paillier additive homomorphic encryption;cloud secure outsourcing;cryptographic tools;garbled circuits;privacy-preserving computation outsourcing;secure online voting;Paillier cryptosystem;Python;interface language;GMP C-routines;arithmetic operations;Encryption;Ciphers;Cloud computing;Libraries;Additives;Electronic voting;Security and privacy;Privacy-preserving protocols;Paillier's cryptosystem;Cloud Computing","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Nassar et al. 2015 - Paillier's encryption - Implementation and cloud applications.pdf","","","",""
"Conference paper","Abdalla M,Pointcheval D","","Simple Password-Based Encrypted Key Exchange Protocols","","","","","Topics in Cryptology – CT-RSA 2005","","","2005","","","191-208","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2005","","","","","","","http://dx.doi.org/10.1007/978-3-540-30574-3_14;https://link.springer.com/chapter/10.1007/978-3-540-30574-3_14;https://nlnet.nl/project/spake2/AbPo05a-letter.pdf","10.1007/978-3-540-30574-3_14","","","","Password-based encrypted key exchange are protocols that are designed to provide pair of users communicating over an unreliable channel with a secure session key even when the secret key or password shared between two users is drawn from a small set of values. In this paper, we present two simple password-based encrypted key exchange protocols based on that of Bellovin and Merritt. While one protocol is more suitable to scenarios in which the password is shared across several servers, the other enjoys better security properties. Both protocols are as efficient, if not better, as any of the existing encrypted key exchange protocols in the literature, and yet they only require a single random oracle instance. The proof of security for both protocols is in the random oracle model and based on hardness of the computational Diffie-Hellman problem. However, some of the techniques that we use are quite different from the usual ones and make use of new variants of the Diffie-Hellman problem, which are of independent interest. We also provide concrete relations between the new variants and the standard Diffie-Hellman problem.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Abdalla and Pointcheval 2005 - Simple Password-Based Encrypted Key Exchange Protocols.pdf","","","",""
"Patent","","","Characterizing and modeling virtual synthetic backup workloads","","","","","","","","2014","","","","All","Datasets","","","","","","","","2014-09-02","","2019-10-27","","","","","https://patentimages.storage.googleapis.com/0a/2b/79/46485a9fdd2a6d/US8825653.pdf;https://patents.google.com/patent/US8825653B1/en","","","","","Embodiments of this invention are directed to a system and method for characterizing and modeling a virtual synthetic file system workload. In one embodiment, a virtual synthetic system is adapted to select a first location in a prior generation dataset of a first cluster and generate a first offset using a distance distribution function. Thereafter, the virtual synthetic system selects a second location in the prior generation dataset of a second cluster, wherein the second location is offset from the first cluster by the first offset. Finally, the virtual synthetic system modifies each cluster selected on the prior generation dataset thereby creating a next generation dataset. This process is repeated to generate multiple generations of a dataset. Other embodiments are also described herein.","","Cites Tarasov. Patent application.","","","","","","","","","","","","","","Wang Y,Shilane PN,Botelho FC,Ekambaram D","EMC Corp","USPTO","8825653","2012-09-14","US Patent","United States Patent and Trademark Office","US:13/616978","","All Papers/W/Wang et al. 2014 - Characterizing and modeling virtual synthetic backup workloads.pdf","","","",""
"Book","","","DEDISbench: A Benchmark for Deduplicated Storage Systems","","","","","","","","","","","","All","Platform;Implementations","","","","","","","","","","","","","","","","","","","","","","2012 micro-benchmarking of block based deduplicating systems. I don't think this covers secure systems.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Other/DEDISbench - A Benchmark for Deduplica... - DEDISbench - A Benchmark for Deduplicated Storage Systems.pdf","","","",""
"Preprint","Zuo P,Hua Y,Wang C,Xia W,Cao S,Zhou Y,Sun Y","","Bandwidth-efficient Storage Services for Mitigating Side Channel Attack","","","","","","arXiv [cs.CR]","","2017","","","","All;Thesis","Side Channels","","","","","","","","2017-03-15","","","","","","","http://arxiv.org/abs/1703.05126","","","1703.05126","","Data deduplication is able to effectively identify and eliminate redundant data and only maintain a single copy of files and chunks. Hence, it is widely used in cloud storage systems to save storage space and network bandwidth. However, the occurrence of deduplication can be easily identified by monitoring and analyzing network traffic, which leads to the risk of user privacy leakage. The attacker can carry out a very dangerous side channel attack, i.e., learn-the-remaining-information (LRI) attack, to reveal users' privacy information by exploiting the side channel of network traffic in deduplication. Existing work addresses the LRI attack at the cost of the high bandwidth efficiency of deduplication. In order to address this problem, we propose a simple yet effective scheme, called randomized redundant chunk scheme (RRCS), to significantly mitigate the risk of the LRI attack while maintaining the high bandwidth efficiency of deduplication. The basic idea behind RRCS is to add randomized redundant chunks to mix up the real deduplication states of files used for the LRI attack, which effectively obfuscates the view of the attacker, who attempts to exploit the side channel of network traffic for the LRI attack. Our security analysis shows that RRCS could significantly mitigate the risk of the LRI attack. We implement the RRCS prototype and evaluate it by using three large-scale real-world datasets. Experimental results demonstrate the efficiency and efficacy of RRCS.","","Attaches randomized redundant chunks to upload to obfuscate deduplication. Uses Tarasov dataset.","","","","","","","arXiv","1703.05126","cs.CR","","","","","","","","","","","","","","All Papers/Z/Zuo et al. 2017 - Bandwidth-efficient Storage Services for Mitigating Side Channel Attack.pdf","","","",""
"Journal article","Xia W,Feng D,Jiang H,Zhang Y,Chang V,Zou X","","Accelerating content-defined-chunking based data deduplication by exploiting parallelism","Future Gener. Comput. Syst.","Future generations computer systems: FGCS","","","","","","2019","98","","406-418","All;Thesis","Accelerate;Datasets","","","","","","","","2019-09-01","","","","","0167-739X","","http://www.sciencedirect.com/science/article/pii/S0167739X18320053;http://dx.doi.org/10.1016/j.future.2019.02.008","10.1016/j.future.2019.02.008","","","","Data deduplication, a data reduction technique that efficiently detects and eliminates redundant data chunks and files, has been widely applied in large-scale storage systems. Most existing deduplication-based storage systems employ content-defined chunking (CDC) and secure-hash-based fingerprinting (e.g., SHA1) to remove redundant data at the chunk level (e.g., 4 KB/8 KB chunks), which are extremely compute-intensive and thus time-consuming for storage systems. Therefore, we present P-Dedupe, a pipelined and parallelized data deduplication system that accelerates deduplication process by dividing the deduplication process into four stages (i.e., chunking, fingerprinting, indexing, and writing), pipelining these four stages with chunks & files (the processing data units for deduplication), and then parallelizing CDC and secure-hash-based fingerprinting stages to further alleviate the computation bottleneck. More important, to efficiently parallelize CDC with the requirements of both maximal and minimal chunk sizes and inspired by the MapReduce model, we first split the data stream into several segments (i.e., “Map”), where each segment will be running CDC in parallel with an independent thread, and then re-chunk and join the boundaries of these segments (i.e., “Reduce”) to ensure the chunking effectiveness of parallelized CDC. Experimental results of P-Dedupe with eight datasets on a quad-core Intel i7 processor suggest that P-Dedupe is able to accelerate the deduplication throughput near linearly by exploiting parallelism in the CDC-based deduplication process at the cost of only 0.02% decrease in the deduplication ratio. Our work provides contributions to big data science to ensure all files go through deduplication process quickly and thoroughly, and only process and analyze the same file once, rather than multiple times.","Content-defined chunking; Data deduplication; Backup storage systems; Performance evaluation","Describes 8 datasets include one that uses Tarasov's technique. ""Data segments"" are from a stream, so can span file boundaries.","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2019 - Accelerating content-defined-chunking based data deduplication by exploiting parallelism.pdf","","","",""
"Conference paper","Krawczyk H","","Cryptographic Extraction and Key Derivation: The HKDF Scheme","","","","","Advances in Cryptology – CRYPTO 2010","","","2010","","","631-648","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2010","","","","","","","http://dx.doi.org/10.1007/978-3-642-14623-7_34;https://link.springer.com/chapter/10.1007/978-3-642-14623-7_34;https://link.springer.com/content/pdf/10.1007/978-3-642-14623-7_34.pdf;https://eprint.iacr.org/2010/264.pdf","10.1007/978-3-642-14623-7_34","","","","In spite of the central role of key derivation functions (KDF) in applied cryptography, there has been little formal work addressing the design and analysis of general multi-purpose KDFs. In practice, most KDFs (including those widely standardized) follow ad-hoc approaches that treat cryptographic hash functions as perfectly random functions. In this paper we close some gaps between theory and practice by contributing to the study and engineering of KDFs in several ways. We provide detailed rationale for the design of KDFs based on the extract-then-expand approach; we present the first general and rigorous definition of KDFs and their security that we base on the notion of computational extractors; we specify a concrete fully practical KDF based on the HMAC construction; and we provide an analysis of this construction based on the extraction and pseudorandom properties of HMAC. The resultant KDF design can support a large variety of KDF applications under suitable assumptions on the underlying hash function; particular attention and effort is devoted to minimizing these assumptions as much as possible for each usage scenario.","","<span style=""color: rgb(34, 34, 34); font-family: sans-serif; font-size: 14px;"">Used by Liu ""Secure Deduplication of Encrypted Data without Additional Servers"" - It can be used, for example, to convert shared secrets exchanged via&nbsp;</span><a href=""https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman"" class=""mw-redirect"" title=""Diffie–Hellman"" style=""text-decoration-line: none; color: rgb(11, 0, 128); background-image: none; background-position: initial; background-size: initial; background-repeat: initial; background-attachment: initial; background-origin: initial; background-clip: initial; font-family: sans-serif; font-size: 14px;"">Diffie–Hellman</a><span style=""color: rgb(34, 34, 34); font-family: sans-serif; font-size: 14px;"">&nbsp;into key material suitable for use in encryption, integrity checking or authentication.</span>","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Krawczyk 2010 - Cryptographic Extraction and Key Derivation - The HKDF Scheme.pdf","","","",""
"Conference paper","Boyko V,MacKenzie P,Patel S","","Provably Secure Password-Authenticated Key Exchange Using Diffie-Hellman","","","","","Advances in Cryptology — EUROCRYPT 2000","","","2000","","","156-171","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2000","","","","","","","http://dx.doi.org/10.1007/3-540-45539-6_12;https://link.springer.com/chapter/10.1007/3-540-45539-6_12;https://link.springer.com/content/pdf/10.1007/3-540-45539-6_12.pdf","10.1007/3-540-45539-6_12","","","","When designing password-authenticated key exchange protocols (as opposed to key exchange protocols authenticated using crypto-graphically secure keys), one must not allow any information to be leaked that would allow verification of the password (a weak shared key), since an attacker who obtains this information may be able to run an off-line dictionary attack to determine the correct password. We present a new protocol called PAK which is the first Diffie-Hellman-based password-authenticated key exchange protocol to provide a formal proof of security (in the random oracle model) against both passive and active adversaries. In addition to the PAK protocol that provides mutual explicit authentication, we also show a more efficient protocol called PPK that is provably secure in the implicit-authentication model. We then extend PAK to a protocol called PAK-X, in which one side (the client) stores a plaintext version of the password, while the other side (the server) only stores a verifier for the password. We formally prove security of PAK-X, even when the server is compromised. Our formal model for password-authenticated key exchange is new, and may be of independent interest.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boyko et al. 2000 - Provably Secure Password-Authenticated Key Exchange Using Diffie-Hellman.pdf","","","",""
"Conference paper","Bellare M,Pointcheval D,Rogaway P","","Authenticated Key Exchange Secure against Dictionary Attacks","","","","","Advances in Cryptology — EUROCRYPT 2000","","","2000","","","139-155","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2000","","","","","","","http://dx.doi.org/10.1007/3-540-45539-6_11;https://link.springer.com/chapter/10.1007/3-540-45539-6_11;https://link.springer.com/content/pdf/10.1007/3-540-45539-6_11.pdf","10.1007/3-540-45539-6_11","","","","Password-based protocols for authenticated key exchange (AKE) are designed to work despite the use of passwords drawn from a space so small that an adversary might well enumerate, off line, all possible passwords. While several such protocols have been suggested, the underlying theory has been lagging. We begin by defining a model for this problem, one rich enough to deal with password guessing, forward secrecy, server compromise, and loss of session keys. The one model can be used to define various goals. We take AKE (with “implicit” authentication) as the “basic” goal, and we give definitions for it, and for entity-authentication goals as well. Then we prove correctness for the idea at the center of the Encrypted Key-Exchange (EKE) protocol of Bellovin and Merritt: we prove security, in an ideal-cipher model, of the two-flow protocol at the core of EKE.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 2000 - Authenticated Key Exchange Secure against Dictionary Attacks.pdf","","","",""
"Conference paper","Bellovin SM,Merritt M","","Encrypted key exchange: password-based protocols secure against dictionary attacks","","","","","Proceedings 1992 IEEE Computer Society Symposium on Research in Security and Privacy","","","1992","","","72-84","All","Sec Techniques/Protocols","","","","","","","","1992-05","","","","","","","http://dx.doi.org/10.1109/RISP.1992.213269;https://ieeexplore.ieee.org/abstract/document/213269/;https://academiccommons.columbia.edu/doi/10.7916/D8HH6RVF/download","10.1109/RISP.1992.213269","","","","Classic cryptographic protocols based on user-chosen keys allow an attacker to mount password-guessing attacks. A combination of asymmetric (public-key) and symmetric (secret-key) cryptography that allow two parties sharing a common password to exchange confidential and authenticated information over an insecure network is introduced. In particular, a protocol relying on the counter-intuitive motion of using a secret key to encrypt a public key is presented. Such protocols are secure against active attacks, and have the property that the password is protected against offline dictionary attacks.","authorisation;cryptography;data privacy;glossaries;message authentication;protocols;public-key cryptography;secret-key cryptography;authentication;password-based protocols;cryptographic protocols;user-chosen keys;insecure network;offline dictionary attacks;Dictionaries;Public key;Public key cryptography;Cryptographic protocols;Protection;Random number generation;Security;Authentication","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellovin and Merritt 1992 - Encrypted key exchange - password-based protocols secure against dictionary attacks.pdf","","","",""
"Journal article","Armknecht F,Katzenbeisser S,Peter A","","Group Homomorphic Encryption - Cryptology ePrint Archive","","","","","","","","","","","","All","Encryption Techniques","","","","","","","","","","","","","","","https://eprint.iacr.org/2010/501.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Armknecht et al. - Group Homomorphic Encryption - Cryptology ePrint Archive.pdf","","","",""
"Conference paper","Sun Z,Kuenning G,Mandal S,Shilane P,Tarasov V,Xiao N,Zadok E","","A long-term user-centric analysis of deduplication patterns","","","","","2016 32nd Symposium on Mass Storage Systems and Technologies (MSST)","","","2016","","","1-7","All;Grouped by Publication/MSST;FSL Traces;Thesis","Datasets","","","","","","","","2016-05","","","","","","","http://dx.doi.org/10.1109/MSST.2016.7897080;https://ieeexplore.ieee.org/abstract/document/7897080/;https://www.fsl.cs.sunysb.edu/docs/msst16dedup-study/data-set-analysis.pdf;https://storageconference.us/2016/Papers/Long-TermAnalysis.pdf","10.1109/MSST.2016.7897080","","","","Deduplication has become essential in disk-based backup systems, but there have been few long-term studies of backup workloads. Most past studies either were of a small static snapshot or covered only a short period that was not representative of how a backup system evolves over time. For this paper, we collected 21 months of data from a shared user file system; 33 users and over 4,000 snapshots are covered. We analyzed the data set for a variety of essential characteristics. However, our primary focus was individual user data. Despite apparently similar roles and behavior in all of our users, we found significant differences in their deduplication ratios. Moreover, the data that some users share with others had a much higher deduplication ratio than average. We analyze this behavior and make recommendations for future deduplication systems design.","file organisation;user-centric analysis;deduplication patterns;disk-based backup systems;backup workloads;shared user file system;deduplication ratios;Indexes;Metadata;Measurement;Redundancy;Market research;High performance computing;Computers","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Sun et al. 2016 - A long-term user-centric analysis of deduplication patterns.pdf","","","",""
"Thesis","Jiangwei Z","","Synthetically Scaling an Empirical Dataset","","","","","","","","2018","","","","All","Datasets","","","search.proquest.com","","","","","2018","","","","","","","http://search.proquest.com/openview/7c3c728b7da521b07e6eb76f1f129032/1?pq-origsite=gscholar&cbl=2026366&diss=y","","","","","Large-scale enterprises, like Amazon and Douban, have enormous datasets. For research and development, it is impractical to run experiments with such a large dataset. It is therefore often necessary to obtain a smaller version of the dataset for experiments. We call this the …","","","","","","Ph.D. Thesis","","","","","","","National University of Singapore (Singapore)","","","","","","","","","","","","","","","",""
"Journal article","Wang LX,Dong XS,Zhang XJ,Wang YF,Ju T,Feng GF","","TextGen: a realistic text data content generation method for modern storage system benchmarks","Frontiers of Information Technology & Electronic Engineering","","","","","","","2016","17","10","982-993","All","Datasets","","","Springer","","","","","2016-10-01","","","","","2095-9230","","https://doi.org/10.1631/FITEE.1500332;http://dx.doi.org/10.1631/FITEE.1500332;https://link.springer.com/article/10.1631/FITEE.1500332","10.1631/FITEE.1500332","","","","Modern storage systems incorporate data compressors to improve their performance and capacity. As a result, data content can significantly influence the result of a storage system benchmark. Because real-world proprietary datasets are too large to be copied onto a test storage system, and most data cannot be shared due to privacy issues, a benchmark needs to generate data synthetically. To ensure that the result is accurate, it is necessary to generate data content based on the characterization of real-world data properties that influence the storage system performance during the execution of a benchmark. The existing approach, called SDGen, cannot guarantee that the benchmark result is accurate in storage systems that have built-in word-based compressors. The reason is that SDGen characterizes the properties that influence compression performance only at the byte level, and no properties are characterized at the word level. To address this problem, we present TextGen, a realistic text data content generation method for modern storage system benchmarks. TextGen builds the word corpus by segmenting real-world text datasets, and creates a word-frequency distribution by counting each word in the corpus. To improve data generation performance, the word-frequency distribution is fitted to a lognormal distribution by maximum likelihood estimation. The Monte Carlo approach is used to generate synthetic data. The running time of TextGen generation depends only on the expected data size, which means that the time complexity of TextGen is O(n). To evaluate TextGen, four real-world datasets were used to perform an experiment. The experimental results show that, compared with SDGen, the compression performance and compression ratio of the datasets generated by TextGen deviate less from real-world datasets when end-tagged dense code, a representative of word-based compressors, is evaluated.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprint","Zhang JW,Tay YC","","A tool framework for tweaking features in synthetic datasets","","","","","","arXiv [cs.DB]","","2018","","","","All","Datasets","","","","","","","","2018-01-11","","","","","","","http://arxiv.org/abs/1801.03645","","","1801.03645","","Researchers and developers use benchmarks to compare their algorithms and products. A database benchmark must have a dataset D. To be application-specific, this dataset D should be empirical. However, D may be too small, or too large, for the benchmarking experiments. D must, therefore, be scaled to the desired size. To ensure the scaled D' is similar to D, previous work typically specifies or extracts a fixed set of features F = {F_1, F_2, . . . , F_n} from D, then uses F to generate synthetic data for D'. However, this approach (D -> F -> D') becomes increasingly intractable as F gets larger, so a new solution is necessary. Different from existing approaches, this paper proposes ASPECT to scale D to enforce similarity. ASPECT first uses a size-scaler (S0) to scale D to D'. Then the user selects a set of desired features F'_1, . . . , F'_n. For each desired feature F'_k, there is a tweaking tool T_k that tweaks D' to make sure D' has the required feature F'_k. ASPECT coordinates the tweaking of T_1,...,T_n to D', so T_n(...(T_1(D'))...) has the required features F'_1,...,F'_n. By shifting from D -> F -> D' to D -> D' -> F', data scaling becomes flexible. The user can customise the scaled dataset with their own interested features. Extensive experiments on real datasets show that ASPECT can enforce similarity in the dataset effectively and efficiently.","","","","","","","","","arXiv","1801.03645","cs.DB","","","","","","","","","","","","","","All Papers/Z/Zhang and Tay 2018 - A tool framework for tweaking features in synthetic datasets.pdf","","","",""
"Conference paper","Gracia-Tinedo R,Harnik D,Naor D,Sotnikov D,Toledo S,Zuck A","","SDGen: Mimicking datasets for content generation in storage benchmarks","","","","","13th ${$USENIX$}$ Conference on File and Storage Technologies (${$FAST$}$ 15)","","","2015","","","317-330","All","Datasets","","","","","","","","2015","","","","","","","https://www.usenix.org/system/files/conference/fast15/fast15-paper-gracia-tinedo.pdf;https://www.usenix.org/conference/fast15/technical-sessions/presentation/gracia-tinedo","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Gracia-Tinedo et al. 2015 - SDGen - Mimicking datasets for content generation in storage benchmarks.pdf","","","",""
"Journal article","Xia W,Jiang H,Feng D,Douglis F,Shilane P,Hua Y,Fu M,Zhang Y,Zhou Y","","A Comprehensive Study of the Past, Present, and Future of Data Deduplication","Proc. IEEE","Proceedings of the IEEE","","","","","","2016","104","9","1681-1710","All;Thesis;p-scailbib","Secure Dedup;Survey","","","","","","","","2016-09","","","","","","","http://dx.doi.org/10.1109/JPROC.2016.2571298;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7529062","10.1109/JPROC.2016.2571298","","","","Data deduplication, an efficient approach to data reduction, has gained increasing attention and popularity in large-scale storage systems due to the explosive growth of digital data. It eliminates redundant data at the file or subfile level and identifies duplicate content by its cryptographically secure hash signature (i.e., collision-resistant fingerprint), which is shown to be much more computationally efficient than the traditional compression approaches in large-scale storage systems. In this paper, we first review the background and key features of data deduplication, then summarize and classify the state-of-the-art research in data deduplication according to the key workflow of the data deduplication process. The summary and taxonomy of the state of the art on deduplication help identify and understand the most important design considerations for data deduplication systems. In addition, we discuss the main applications and industry trend of data deduplication, and provide a list of the publicly available sources for deduplication research and studies. Finally, we outline the open problems and future research directions facing deduplication-based storage systems.","cryptography;data reduction;digital signatures;storage management;data deduplication;data reduction;large-scale storage systems;digital data;cryptographically secure hash signature;content duplication;collision-resistant fingerprint;deduplication-based storage systems;file level;subfile level;redundant data;Fingerprint recognition;Data compression;Security of data;Data storage systems;Image coding;Data compression;Redundancy;Data compression;data deduplication;data reduction;delta compression;storage security;storage systems","","","","","","","","","","","","","","","","","","","","","","","","All Papers/X/Xia et al. 2016 - A Comprehensive Study of the Past, Present, and Future of Data Deduplication.pdf","","","",""
"Book","Chen L,Takabi H,Le-Khac NA","","Security, Privacy, and Digital Forensics in the Cloud","","","","","","","","2019","","","","All","Sec Techniques/Protocols","","","John Wiley & Sons","","","","","2019-04-29","","","9781119053286","","","","https://play.google.com/store/books/details?id=R5VPCwAAQBAJ;https://learning.oreilly.com/library/view/security-privacy-and/9781119053286/c03.xhtml;https://learning.oreilly.com/library/view/security-privacy-and/9781119053286/c03.xhtml#head-2-18","","","","","In a unique and systematic way, this book discusses the security and privacy aspects of the cloud, and the relevant cloud forensics. Cloud computing is an emerging yet revolutionary technology that has been changing the way people live and work. However, with the continuous growth of cloud computing and related services, security and privacy has become a critical issue. Written by some of the top experts in the field, this book specifically discusses security and privacy of the cloud, as well as the digital forensics of cloud data, applications, and services. The first half of the book enables readers to have a comprehensive understanding and background of cloud security, which will help them through the digital investigation guidance and recommendations found in the second half of the book. Part One of Security, Privacy and Digital Forensics in the Cloud covers cloud infrastructure security; confidentiality of data; access control in cloud IaaS; cloud security and privacy management; hacking and countermeasures; risk management and disaster recovery; auditing and compliance; and security as a service (SaaS). Part Two addresses cloud forensics – model, challenges, and approaches; cyberterrorism in the cloud; digital forensic process and model in the cloud; data acquisition; digital evidence management, presentation, and court preparation; analysis of digital evidence; and forensics as a service (FaaS). Thoroughly covers both security and privacy of cloud and digital forensics Contributions by top researchers from the U.S., the European and other countries, and professionals active in the field of information and network security, digital and computer forensics, and cloud and big data Of interest to those focused upon security and implementation, and incident management Logical, well-structured, and organized to facilitate comprehension Security, Privacy and Digital Forensics in the Cloud is an ideal book for advanced undergraduate and master's-level students in information systems, information technology, computer and network forensics, as well as computer science. It can also serve as a good reference book for security professionals, digital forensics practitioners and cloud service providers.","","","","","en","","","","","","","360","","","","","","","","","","","","","","","","",""
"Conference paper","Tarasov V,Mudrankit A,Buik W,Shilane P,Kuenning G,Zadok E","","Generating realistic datasets for deduplication analysis","","","","","Presented as part of the 2012 ${$USENIX$}$ Annual Technical Conference (${$USENIX$}$${$ATC$}$ 12)","","","2012","","","261-272","All;FSL Traces;Thesis","Datasets","","","","","","","","2012","","","","","","","https://www.usenix.org/system/files/conference/atc12/atc12-final129.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/T/Tarasov et al. 2012 - Generating realistic datasets for deduplication analysis.pdf","","","",""
"Conference paper","Liu J,Asokan N,Pinkas B","","Secure Deduplication of Encrypted Data without Additional Independent Servers","","","","","Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security - CCS '15","","","2015","","","874-885","All","Secure Dedup","","","ACM Press","New York, New York, USA","the 22nd ACM SIGSAC Conference","Denver, Colorado, USA","12/10/2015-16/10/2015","2015","","","9781450338325","","","","http://dl.acm.org/citation.cfm?doid=2810103.2813623;http://dl.acm.org/ft_gateway.cfm?id=2813623&amp;ftid=1630978&amp;dwn=1;http://dx.doi.org/10.1145/2810103.2813623;http://doi.acm.org/10.1145/2810103.2813623;https://dl.acm.org/citation.cfm?id=2813623;https://dl.acm.org/ft_gateway.cfm?id=2813623&ftid=1630978&dwn=1&CFID=104652784&CFTOKEN=3d5f7a5307ca00db-85CEF405-95E8-68D1-BD0DC8F72D24678B","10.1145/2810103.2813623","","","","","cloud storage, deduplication, pake, semantically secure encryption","","","","","","CCS '15","","","","","","","","","","","","","","","","","","All Papers/L/Liu et al. 2015 - Secure Deduplication of Encrypted Data without Additional Independent Servers.pdf;All Papers/L/Liu et al. 2015 - Secure Deduplication of Encrypted Data without Additional Independent Servers.pdf","","","",""
"Conference paper","Puzio P,Molva R,Önen M,Loureiro S","","ClouDedup: Secure Deduplication with Encrypted Data for Cloud Storage","","","","","2013 IEEE 5th International Conference on Cloud Computing Technology and Science","","","2013","1","","363-370","All;Thesis;p-scailbib","Secure Dedup;CE/MLE","","","","","","","","2013-12","","","","","","","http://dx.doi.org/10.1109/CloudCom.2013.54;https://ieeexplore.ieee.org/abstract/document/6753819/;http://www.eurecom.fr/fr/publication/4136/download/rs-publi-4136.pdf","10.1109/CloudCom.2013.54","","","","With the continuous and exponential increase of the number of users and the size of their data, data deduplication becomes more and more a necessity for cloud storage providers. By storing a unique copy of duplicate data, cloud providers greatly reduce their storage and data transfer costs. The advantages of deduplication unfortunately come with a high cost in terms of new security and privacy challenges. We propose ClouDedup, a secure and efficient storage service which assures block-level deduplication and data confidentiality at the same time. Although based on convergent encryption, ClouDedup remains secure thanks to the definition of a component that implements an additional encryption operation and an access control mechanism. Furthermore, as the requirement for deduplication at block-level raises an issue with respect to key management, we suggest to include a new component in order to implement the key management for each block together with the actual deduplication operation. We show that the overhead introduced by these new components is minimal and does not impact the overall storage and computational costs.","cloud computing;cryptography;storage management;ClouDedup;deduplication security;encrypted data;cloud storage;storage service;block-level deduplication;data confidentiality;encryption operation;access control mechanism;Encryption;Cloud computing;Servers;Access control;Bismuth;deduplication;cloud storage;convergent encryption;confidentiality;privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Puzio et al. 2013 - ClouDedup - Secure Deduplication with Encrypted Data for Cloud Storage.pdf","","","",""
"Conference paper","Boldyreva A","","Threshold Signatures, Multisignatures and Blind Signatures Based on the Gap-Diffie-Hellman-Group Signature Scheme","","","","","Public Key Cryptography — PKC 2003","","","2002","","","31-46","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2002","","","","","","","http://dx.doi.org/10.1007/3-540-36288-6_3;https://link.springer.com/content/pdf/10.1007/3-540-36288-6_3.pdf","10.1007/3-540-36288-6_3","","","","We propose a robust proactive threshold signature scheme, a multisignature scheme and a blind signature scheme which work in any Gap Diffie-Hellman (GDH) group (where the Computational Diffie- Hellman problem is hard but the Decisional Diffie-Hellman problem is easy). Our constructions are based on the recently proposed GDH signature scheme of Boneh et al. [8]. Due to the instrumental structure of GDH groups and of the base scheme, it turns out that most of our constructions are simpler, more efficient and have more useful properties than similar existing constructions. We support all the proposed schemes with proofs under the appropriate computational assumptions, using the corresponding notions of security.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boldyreva 2002 - Threshold Signatures, Multisignatures and Blind Signatures Based on the Gap-Diffie-Hellman-Group Signature Scheme.pdf","","","",""
"Conference paper","Chou T,Orlandi C","","The Simplest Protocol for Oblivious Transfer","","","","","Progress in Cryptology -- LATINCRYPT 2015","","","2015","","","40-58","All","Sec Techniques/Protocols","","","Springer International Publishing","","","","","2015","","","","","","","http://dx.doi.org/10.1007/978-3-319-22174-8_3;https://link.springer.com/chapter/10.1007/978-3-319-22174-8_3;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.920.8889&rep=rep1&type=pdf;https://eprint.iacr.org/2015/267.pdf","10.1007/978-3-319-22174-8_3","","","","Oblivious Transfer (OT) is the fundamental building block of cryptographic protocols. In this paper we describe the simplest and most efficient protocol for 1-out-of-n OT to date, which is obtained by tweaking the Diffie-Hellman key-exchange protocol. The protocol achieves UC-security against active and adaptive corruptions in the random oracle model. Due to its simplicity, the protocol is extremely efficient and it allows to perform m 1-out-of-n OTs using only:Computation: $$(n+1)m+2$$(n+1)m+2exponentiations (mn for the receiver, $$mn+2$$mn+2for the sender) andCommunication: $$32(m+1)$$32(m+1)bytes (for the group elements), and 2mn ciphertexts.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Chou and Orlandi 2015 - The Simplest Protocol for Oblivious Transfer.pdf","","","",""
"Conference paper","Ristenpart T,Shacham H,Shrimpton T","","Careful with Composition: Limitations of the Indifferentiability Framework","","","","","Advances in Cryptology – EUROCRYPT 2011","","","2011","","","487-506","All","Sec Techniques/Protocols;Encryption Techniques","","","Springer Berlin Heidelberg","","","","","2011","","","","","","","http://dx.doi.org/10.1007/978-3-642-20465-4_27;https://link.springer.com/chapter/10.1007/978-3-642-20465-4_27;https://link.springer.com/content/pdf/10.1007/978-3-642-20465-4_27.pdf","10.1007/978-3-642-20465-4_27","","","","We exhibit a hash-based storage auditing scheme which is provably secure in the random-oracle model (ROM), but easily broken when one instead uses typical indifferentiable hash constructions. This contradicts the widely accepted belief that the indifferentiability composition theorem from [27] applies to any cryptosystem. We characterize the uncovered limitations of indifferentiability by showing that the formalizations used thus far implicitly exclude security notions captured by experiments that have multiple, disjoint adversarial stages. Examples include deterministic public-key encryption (PKE), password-based cryptography, hash function nonmalleability, and more. We formalize a stronger notion, reset indifferentiability, that enables a composition theorem covering such multi-stage security notions, but our results show that practical hash constructions cannot be reset indifferentiable. We finish by giving direct security proofs for several important PKE schemes.","","Syntax and semantics for game based proofs referenced in MLE","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Ristenpart et al. 2011 - Careful with Composition - Limitations of the Indifferentiability Framework.pdf","","","",""
"Conference paper","Varia M,Yakoubov S,Yang Y","","HEtest: A Homomorphic Encryption Testing Framework","","","","","Financial Cryptography and Data Security","","","2015","","","213-230","All","Code/OpenSource","","","Springer Berlin Heidelberg","","","","","2015","","","","","","","http://dx.doi.org/10.1007/978-3-662-48051-9_16;https://link.springer.com/chapter/10.1007/978-3-662-48051-9_16;https://eprint.iacr.org/2015/416.pdf","10.1007/978-3-662-48051-9_16","","","","In this work, we present a generic open-source software framework that can evaluate the correctness and performance of homomorphic encryption software. Our framework, called HEtest, automates the entire process of a test: generation of data for testing (such as circuits and inputs), execution of a test, comparison of performance to an insecure baseline, statistical analysis of the test results, and production of a LaTeX report. To illustrate the capability of our framework, we present a case study of our analysis of the open-source HElib homomorphic encryption software. We stress though that HEtest is written in a modular fashion, so it can easily be adapted to test any homomorphic encryption software.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/V/Varia et al. 2015 - HEtest - A Homomorphic Encryption Testing Framework.pdf","","","",""
"Miscellaneous","Bellare M,Rogaway P","","The Security of Triple Encryption and a Framework for Code-Based Game-Playing Proofs","Advances in Cryptology - EUROCRYPT 2006","","","","","","","2006","","","409-426","All","Sec Techniques/Protocols","","","","","","","","2006","","","","","","","http://dx.doi.org/10.1007/11761679_25","10.1007/11761679_25","","","","","","Game playing framework used by MLE proofs","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare and Rogaway 2006 - The Security of Triple Encryption and a Framework for Code-Based Game-Playing Proofs.pdf","","","",""
"Conference paper","Bellare M,Boldyreva A,O’Neill A","","Deterministic and Efficiently Searchable Encryption","","","","","Advances in Cryptology - CRYPTO 2007","","","2007","","","535-552","All","Encryption Techniques","","","Springer Berlin Heidelberg","","","","","2007","","","","","","","http://dx.doi.org/10.1007/978-3-540-74143-5_30;https://link.springer.com/chapter/10.1007/978-3-540-74143-5_30;https://link.springer.com/content/pdf/10.1007/978-3-540-74143-5_30.pdf","10.1007/978-3-540-74143-5_30","","","","We present as-strong-as-possible definitions of privacy, and constructions achieving them, for public-key encryption schemes where the encryption algorithm is deterministic. We obtain as a consequence database encryption methods that permit fast (i.e. sub-linear, and in fact logarithmic, time) search while provably providing privacy that is as strong as possible subject to this fast search constraint. One of our constructs, called RSA-DOAEP, has the added feature of being length preserving, so that it is the first example of a public-key cipher. We generalize this to obtain a notion of efficiently-searchable encryption schemes which permit more flexible privacy to search-time trade-offs via a technique called bucketization. Our results answer much-asked questions in the database community and provide foundations for work done there.","","Used for MLE proofs.","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 2007 - Deterministic and Efficiently Searchable Encryption.pdf","","","",""
"Journal article","Kiraz MS","","A comprehensive meta-analysis of cryptographic security mechanisms for cloud computing","J. Ambient Intell. Humaniz. Comput.","Journal of ambient intelligence and humanized computing","","","","","","2016","7","5","731-760","All","Sec Techniques/Protocols","","","","","","","","2016-10-01","","","","","1868-5137","1868-5145","https://doi.org/10.1007/s12652-016-0385-0;http://dx.doi.org/10.1007/s12652-016-0385-0;https://link.springer.com/article/10.1007/s12652-016-0385-0","10.1007/s12652-016-0385-0","","","","The concept of cloud computing offers measurable computational or information resources as a service over the Internet. The major motivation behind the cloud setup is economic benefits, because it assures the reduction in expenditure for operational and infrastructural purposes. To transform it into a reality there are some impediments and hurdles which are required to be tackled, most profound of which are security, privacy and reliability issues. As the user data is revealed to the cloud, it departs the protection-sphere of the data owner. However, this brings partly new security and privacy concerns. This work focuses on these issues related to various cloud services and deployment models by spotlighting their major challenges. While the classical cryptography is an ancient discipline, modern cryptography, which has been mostly developed in the last few decades, is the subject of study which needs to be implemented so as to ensure strong security and privacy mechanisms in today’s real-world scenarios. The technological solutions, short and long term research goals of the cloud security will be described and addressed using various classical cryptographic mechanisms as well as modern ones. This work explores the new directions in cloud computing security, while highlighting the correct selection of these fundamental technologies from cryptographic point of view.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kiraz 2016 - A comprehensive meta-analysis of cryptographic security mechanisms for cloud computing.pdf","","","",""
"Book chapter","Bellare M,Keelveedhi S","","Interactive message-locked encryption and secure deduplication","","","Lecture Notes in Computer Science","","","","","2015","","","516-538","All","CE/MLE;Sec Techniques/Protocols","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","2015","","","9783662464465","9783662464472","0302-9743","1611-3349","https://eprint.iacr.org/2015/052.pdf;http://link.springer.com/10.1007/978-3-662-46447-2_23;http://dx.doi.org/10.1007/978-3-662-46447-2_23","10.1007/978-3-662-46447-2_23","","","","","","Full Version.","","","","","Lecture notes in computer science","","","","","","","","","","","","","","","","","","All Papers/B/Bellare and Keelveedhi 2015 - Interactive message-locked encryption and secure deduplication.pdf","","","",""
"Conference paper","Halevi S,Krawczyk H","","Security Under Key-dependent Inputs","","","","","Proceedings of the 14th ACM Conference on Computer and Communications Security","","","2007","","","466-475","All","Sec Techniques/Protocols","","","ACM","New York, NY, USA","","Alexandria, Virginia, USA","","2007","","","9781595937032","","","","http://doi.acm.org/10.1145/1315245.1315303;http://dx.doi.org/10.1145/1315245.1315303;https://dl.acm.org/citation.cfm?id=1315303;https://eprint.iacr.org/2007/315.pdf","10.1145/1315245.1315303","","","","","circular encryption, key-dependent input, self encryption","","","","","","CCS '07","","","","","","","","","","","","","","","","","","All Papers/H/Halevi and Krawczyk 2007 - Security Under Key-dependent Inputs.pdf","","","",""
"Journal article","Duong T,Rizzo J","","Here come the⊕ ninjas","Unpublished manuscript","","","","","","","2011","320","","","All","Sec Techniques/Protocols","","","","","","","","2011","","","","","","","https://nerdoholic.org/uploads/dergln/beast_part2/ssl_jun21.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Duong and Rizzo 2011 - Here come the⊕ ninjas.pdf","","","",""
"Conference paper","Bellare M,Hoang VT,Keelveedhi S","","Instantiating Random Oracles via UCEs","","","","","Advances in Cryptology – CRYPTO 2013","","","2013","","","398-415","All","Security Proof","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-40084-1_23;https://link.springer.com/chapter/10.1007/978-3-642-40084-1_23;https://link.springer.com/content/pdf/10.1007/978-3-642-40084-1_23.pdf","10.1007/978-3-642-40084-1_23","","","","This paper provides a (standard-model) notion of security for (keyed) hash functions, called UCE, that we show enables instantiation of random oracles (ROs) in a fairly broad and systematic way. Goals and schemes we consider include deterministic PKE; message-locked encryption; hardcore functions; point-function obfuscation; OAEP; encryption secure for key-dependent messages; encryption secure under related-key attack; proofs of storage; and adaptively-secure garbled circuits with short tokens. We can take existing, natural and efficient ROM schemes and show that the instantiated scheme resulting from replacing the RO with a UCE function is secure in the standard model. In several cases this results in the first standard-model schemes for these goals. The definition of UCE-security itself is quite simple, asking that outputs of the function look random given some “leakage,” even if the adversary knows the key, as long as the leakage does not permit the adversary to compute the inputs.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 2013 - Instantiating Random Oracles via UCEs.pdf","","","",""
"Miscellaneous","Bellare M,Goldreich O,Goldwasser S","","Incremental cryptography and application to virus protection","Proceedings of the twenty-seventh annual ACM symposium on Theory of computing  - STOC '95","","","","","","","1995","","","","All","Sec Techniques/Protocols","","","","","","","","1995","","","","","","","http://dx.doi.org/10.1145/225058.225080","10.1145/225058.225080","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 1995 - Incremental cryptography and application to virus protection.pdf","","","",""
"Conference paper","Bellare M,Goldreich O,Goldwasser S","","Incremental Cryptography: The Case of Hashing and Signing","","","","","Advances in Cryptology — CRYPTO ’94","","","1994","","","216-233","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","1994","","","","","","","http://dx.doi.org/10.1007/3-540-48658-5_22;https://link.springer.com/chapter/10.1007/3-540-48658-5_22;https://link.springer.com/content/pdf/10.1007/3-540-48658-5_22.pdf","10.1007/3-540-48658-5_22","","","","We initiate the investigation of a new kind of efficiency for cryptographic transformations. The idea is that having once applied the transformation to some document M, the time to update the result upon modification of M should be “proportional” to the “amount of modification” done to M. Thereby one obtains much faster cryptographic primitives for environments where closely related documents are undergoing the same cryptographic transformations.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 1994 - Incremental Cryptography - The Case of Hashing and Signing.pdf","","","",""
"Conference paper","Bitansky N,Canetti R","","On Strong Simulation and Composable Point Obfuscation","","","","","Advances in Cryptology – CRYPTO 2010","","","2010","","","520-537","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2010","","","","","","","http://dx.doi.org/10.1007/978-3-642-14623-7_28;https://link.springer.com/chapter/10.1007/978-3-642-14623-7_28;https://link.springer.com/content/pdf/10.1007/978-3-642-14623-7_28.pdf","10.1007/978-3-642-14623-7_28","","","","The Virtual Black Box (VBB) property for program obfuscators provides a strong guarantee: Anything computable by an efficient adversary given the obfuscated program can also be computed by an efficient simulator with only oracle access to the program. However, we know how to achieve this notion only for very restricted classes of programs.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bitansky and Canetti 2010 - On Strong Simulation and Composable Point Obfuscation.pdf","","","",""
"Miscellaneous","Canetti R,Raghuraman S,Richelson S,Vaikuntanathan V","","Chosen-Ciphertext Secure Fully Homomorphic Encryption","Lecture Notes in Computer Science","","","","","","","2017","","","213-240","All","Encryption Techniques","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1007/978-3-662-54388-7_8","10.1007/978-3-662-54388-7_8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Brakerski Z","","Fully Homomorphic Encryption without Modulus Switching from Classical GapSVP","","","","","Advances in Cryptology – CRYPTO 2012","","","2012","","","868-886","All","Encryption Techniques","","","Springer Berlin Heidelberg","","","","","2012","","","","","","","http://dx.doi.org/10.1007/978-3-642-32009-5_50;https://link.springer.com/chapter/10.1007/978-3-642-32009-5_50;https://link.springer.com/content/pdf/10.1007/978-3-642-32009-5_50.pdf","10.1007/978-3-642-32009-5_50","","","","We present a new tensoring technique for LWE-based fully homomorphic encryption. While in all previous works, the ciphertext noise grows quadratically ($$B \rightarrow B^2\cdot \text {poly}(n)$$) with every multiplication (before “refreshing”), our noise only grows linearly ($$B \rightarrow B\cdot \text {poly}(n)$$).","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Brakerski 2012 - Fully Homomorphic Encryption without Modulus Switching from Classical GapSVP.pdf","","","",""
"Conference paper","Bellare M,Keelveedhi S","","Interactive Message-Locked Encryption and Secure Deduplication","","","","","Public-Key Cryptography -- PKC 2015","","","2015","","","516-538","All","CE/MLE;Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2015","","","","","","","http://dx.doi.org/10.1007/978-3-662-46447-2_23;https://link.springer.com/chapter/10.1007/978-3-662-46447-2_23;https://link.springer.com/content/pdf/10.1007/978-3-662-46447-2_23.pdf","10.1007/978-3-662-46447-2_23","","","","This paper considers the problem of secure storage of outsourced data in a way that permits deduplication. We are for the first time able to provide privacy for messages that are both correlated and dependent on the public system parameters. The new ingredient that makes this possible is interaction. We extend the message-locked encryption (MLE) primitive of prior work to interactive message-locked encryption (iMLE) where upload and download are protocols. Our scheme, providing security for messages that are not only correlated but allowed to depend on the public system parameters, is in the standard model. We explain that interaction is not an extra assumption in practice because full, existing deduplication systems are already interactive.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare and Keelveedhi 2015 - Interactive Message-Locked Encryption and Secure Deduplication.pdf","","","",""
"Journal article","Kim M,Lee HT,Ling S,Tan BH,Wang H","","Private Compound Wildcard Queries Using Fully Homomorphic Encryption","IEEE Trans. Dependable Secure Comput.","IEEE Transactions on Dependable and Secure Computing","","","","","","2019","16","5","743-756","All","Encryption Techniques","","","","","","","","2019-09","","","","","","","http://dx.doi.org/10.1109/TDSC.2017.2763593;https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8070319","10.1109/TDSC.2017.2763593","","","","Fully homomorphic encryption (FHE) brings a paradigm shift in cryptographic engineering by enabling us to resolve various unsolved problems. Among them, this work solves the problem to design a private database query (PDQ) protocol that supports compound queries with wildcard conditions on encrypted databases using FHE. More precisely, we consider a setting where clients outsource an encrypted database using FHE to a remote server, and later request results of compound queries including a wildcard search condition-given a set of attribute values A1; A2; ...; An and a search pattern W, retrieve a set of all attribute values Ai's in which the pattern W occurs. To this end, we first develop an algorithm for testing whether an encrypted string contains an encrypted pattern without revealing any information of the pattern, taking auxiliary encryptions as additional inputs. Then, using this algorithm, we design PDQ protocols on encrypted databases, which support compound queries using wildcard search conditions. Finally, we demonstrate proof-of-concept implementation results of our protocols by exploiting single-instruction-multiple-data operations and multi-threading techniques.","cryptographic protocols;cryptography;database management systems;multi-threading;outsourcing;query processing;private compound wildcard queries;fully homomorphic encryption;FHE;private database query protocol;compound queries;encrypted database;attribute values;encrypted string;encrypted pattern;auxiliary encryptions;wildcard search conditions;search pattern;PDQ protocols;single-instruction-multiple-data operations;multithreading techniques;Databases;Protocols;Encryption;Pattern matching;Compounds;Servers;Private database queries;wildcard pattern matching;homomorphic encryption;encrypted databases","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kim et al. 2019 - Private Compound Wildcard Queries Using Fully Homomorphic Encryption.pdf","","","",""
"Conference paper","Garman C,Green M,Kaptchuk G,Miers I,Rushanan M","","Dancing on the lip of the volcano: Chosen ciphertext attacks on apple imessage","","","","","25th ${$USENIX$}$ Security Symposium (${$USENIX$}$ Security 16)","","","2016","","","655-672","All","Side Channels","","","","","","","","2016","","","","","","","https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_garman.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/G/Garman et al. 2016 - Dancing on the lip of the volcano - Chosen ciphertext attacks on apple imessage.pdf","","","",""
"Conference paper","Naor M,Yung M","","Public-key cryptosystems provably secure against chosen ciphertext attacks","","","","","Proceedings of the twenty-second annual ACM symposium on Theory of computing","","","1990","","","427-437","All","Side Channels","Citeseer","","","","","","","1990","","","","","","","http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.468.2192&rep=rep1&type=pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/N/Naor and Yung 1990 - Public-key cryptosystems provably secure against chosen ciphertext attacks.pdf","","","",""
"Miscellaneous","Prabhakaran M,Rosulek M","","Reconciling Non-malleability with Homomorphic Encryption","Journal of Cryptology","","","","","","","2017","30","3","601-671","All","Encryption Techniques","","","","","","","","2017","","","","","","","http://dx.doi.org/10.1007/s00145-016-9231-y","10.1007/s00145-016-9231-y","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Conference paper","Black J,Rogaway P,Shrimpton T","","Encryption-Scheme Security in the Presence of Key-Dependent Messages","","","","","Selected Areas in Cryptography","","","2003","","","62-75","All","Encryption Techniques","","","Springer Berlin Heidelberg","","","","","2003","","","","","","","http://dx.doi.org/10.1007/3-540-36492-7_6;https://link.springer.com/chapter/10.1007/3-540-36492-7_6;https://link.springer.com/content/pdf/10.1007/3-540-36492-7_6.pdf","10.1007/3-540-36492-7_6","","","","Encryption that is only semantically secure should not be used on messages that depend on the underlying secret key; all bets are off when, for example, one encrypts using a shared key K the value K. Here we introduce a new notion of security, KDM security, appropriate for key-dependent messages. The notion makes sense in both the publickey and shared-key settings. For the latter we show that KDM security is easily achievable within the random-oracle model. By developing and achieving stronger notions of encryption-scheme security it is hoped that protocols which are proven secure under “formal” models of security can, in time, be safely realized by generically instantiating their primitives.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Black et al. 2003 - Encryption-Scheme Security in the Presence of Key-Dependent Messages.pdf","","","",""
"Conference paper","Boneh D,Halevi S,Hamburg M,Ostrovsky R","","Circular-Secure Encryption from Decision Diffie-Hellman","","","","","Advances in Cryptology – CRYPTO 2008","","","2008","","","108-125","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2008","","","","","","","http://dx.doi.org/10.1007/978-3-540-85174-5_7;https://link.springer.com/chapter/10.1007/978-3-540-85174-5_7;https://link.springer.com/content/pdf/10.1007/978-3-540-85174-5_7.pdf;https://crypto.stanford.edu/~dabo/pubs/papers/circular.pdf","10.1007/978-3-540-85174-5_7","","","","We describe a public-key encryption system that remains secure even encrypting messages that depend on the secret keys in use. In particular, it remains secure under a “key cycle” usage, where we have a cycle of public/secret key-pairs (pki,ski) for i = 1,...,n, and we encrypt each skiunder ${\rm pk}_{(i \bmod n)+1}$. Such usage scenarios sometimes arise in key-management systems and in the context of anonymous credential systems. Also, security against key cycles plays a role when relating “axiomatic security” of protocols that use encryption to the “computational security” of concrete instantiations of these protocols.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Boneh et al. 2008 - Circular-Secure Encryption from Decision Diffie-Hellman.pdf","","","",""
"Journal article","Rabin MO","","Fingerprinting by random polynomials","Tech. Rep. NAVTRADEVCEN","Technical report: NAVTRADEVCEN. Naval Training Device Center","","","","","","1981","","","","All;Superchunking Paper Sources/CDC/Chunking;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Chunking","","","ci.nii.ac.jp","","","","","1981","","","","","","","https://ci.nii.ac.jp/naid/10015295584/;https://scholar.google.ca/scholar?cluster=598461536969635196&hl=en&as_sdt=0,5&sciodt=0,5","","","","","CiNii 国立情報学研究所 学術情報ナビゲータ[サイニィ]. メニュー 検索 …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rabin 1981 - Fingerprinting by random polynomials.pdf","","","",""
"Journal article","Rabin MO","","Digitalized signatures and public-key functions as intractable as factorization","","","","","","","","1979","","","","All","Sec Techniques/Protocols","","","apps.dtic.mil","","","","","1979","","","","","","","https://apps.dtic.mil/docs/citations/ADA078415;https://scholar.google.ca/scholar?cluster=8011255702453209432&hl=en&as_sdt=0,5&sciodt=0,5","","","","","We introduce a new class of public-key functions involving a number n= pq having two large prime factors. As usual, the key n is public, while p and q are the private key used by the issuer for production of signatures and function inversion. These functions can be used for …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/R/Rabin 1979 - Digitalized signatures and public-key functions as intractable as factorization.pdf","","","",""
"Thesis","Hohenberger S","","Advances in Signatures, Encryption, and E-Cash from Bilinear Groups","","","","","","","","","","","","All","Encryption Techniques","","","","","","","","","","","","","","","http://groups.csail.mit.edu/cis/theses/hohenberger-phd-thesis.pdf","","","","","","","","","","","Ph.D. Thesis","","","","","","","MIT","","","","","","","","","","","","All Papers/H/Hohenberger - Advances in Signatures, Encryption, and E-Cash from Bilinear Groups.pdf","","Phd","",""
"Conference paper","Dodis Y,Ristenpart T,Vadhan S","","Randomness Condensers for Efficiently Samplable, Seed-Dependent Sources","","","","","Theory of Cryptography","","","2012","","","618-635","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2012","","","","","","","http://dx.doi.org/10.1007/978-3-642-28914-9_35;https://link.springer.com/chapter/10.1007/978-3-642-28914-9_35;https://link.springer.com/content/pdf/10.1007/978-3-642-28914-9_35.pdf","10.1007/978-3-642-28914-9_35","","","","We initiate a study of randomness condensers for sources that are efficiently samplable but may depend on the seed of the condenser. That is, we seek functions Cond : {0,1}n×{0,1}d → {0,1}msuch that if we choose a random seed S ← {0,1}d, and a source $X={\mathcal A}(S)$is generated by a randomized circuit $\mathcal A$of size t such that X has min-entropy at least k given S, then Cond(X;S) should have min-entropy at least some k′ given S. The distinction from the standard notion of randomness condensers is that the source X may be correlated with the seed S (but is restricted to be efficiently samplable). Randomness extractors of this type (corresponding to the special case where k′ = m) have been implicitly studied in the past (by Trevisan and Vadhan, FOCS ‘00).","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dodis et al. 2012 - Randomness Condensers for Efficiently Samplable, Seed-Dependent Sources.pdf","","","",""
"Miscellaneous","Komargodski I,Yogev E","","Another Step Towards Realizing Random Oracles: Non-malleable Point Obfuscation","Advances in Cryptology – EUROCRYPT 2018","","","","","","","2018","","","259-279","All","Security Proof","","","","","","","","2018","","","","","","","http://dx.doi.org/10.1007/978-3-319-78381-9_10","10.1007/978-3-319-78381-9_10","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Canetti R","","Towards realizing random oracles: Hash functions that hide all partial information","Advances in Cryptology — CRYPTO '97","","","","","","","1997","","","455-469","All","Security Proof","","","","","","","","1997","","","","","","","http://dx.doi.org/10.1007/bfb0052255","10.1007/bfb0052255","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Canetti 1997 - Towards realizing random oracles - Hash functions that hide all partial information.pdf","","","",""
"Conference paper","Cramer R,Shoup V","","A practical public key cryptosystem provably secure against adaptive chosen ciphertext attack","","","","","Advances in Cryptology — CRYPTO '98","","","1998","","","13-25","All","Security Proof","","","Springer Berlin Heidelberg","","","","","1998","","","","","","","http://dx.doi.org/10.1007/BFb0055717;https://link.springer.com/chapter/10.1007/BFb0055717;https://link.springer.com/content/pdf/10.1007/BFb0055717.pdf","10.1007/BFb0055717","","","","A new public key cryptosystem is proposed and analyzed. The scheme is quite practical, and is provably secure against adaptive chosen ciphertext attack under standard intractability assumptions. There appears to be no previous cryptosystem in the literature that enjoys both of these properties simultaneously.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Cramer and Shoup 1998 - A practical public key cryptosystem provably secure against adaptive chosen ciphertext attack.pdf","","","",""
"Miscellaneous","Kaaniche N,Laurent M","","SHoPS: Set Homomorphic Proof of Data Possession Scheme in Cloud Storage Applications","2015 IEEE World Congress on Services","","","","","","","2015","","","","All","Encryption Techniques","","","","","","","","2015","","","","","","","http://dx.doi.org/10.1109/services.2015.29","10.1109/services.2015.29","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaaniche and Laurent 2015 - SHoPS - Set Homomorphic Proof of Data Possession Scheme in Cloud Storage Applications.pdf","","","",""
"Journal article","Kaaniche N,Laurent M,Canard S","","Cooperative Set Homomorphic Proofs for Data Possession Checking in Clouds","IEEE Transactions on Cloud Computing","","","","","","","2018","","","1-1","All","Encryption Techniques","","","","","","","","2018","","","","","","","http://dx.doi.org/10.1109/TCC.2018.2865343;https://ieeexplore.ieee.org/abstract/document/8434330/","10.1109/TCC.2018.2865343","","","","Outsourcing an increasing amount of data to a third party raises a number of security and privacy challenges, namely remote data integrity verification. Indeed, proofs for data possession checking address the verification that some previously outsourced data blocks across multiple storing nodes are correctly stored and fully available. In this paper, we propose a new set homomorphic proof of data possession, referred to as SHoPS, supporting several operations like aggregation of proofs. SHoPS is a deterministic Proof of Data Possession (PDP) scheme, based on an interactive proof protocol. Our approach has several advantages. First, it enables several proofs to be aggregated and a subset of data files' proofs to be verified, while providing an attractive communication overhead. Second, it supports public verifiability where the verification process can be delegated to another entity, thus releasing the data owner from the cumbersome task of periodical verifications. Third, SHoPS is efficient and provably secure, as it is resistant to the fraudulence of the prover and the leakage of verified data. Finally, a theoretical performances' analysis shows that SHoPS performs better in terms of functionality, communication and computation overhead compared to closely related works and experimental results point out the applicability of the proposed scheme in real world scenarios.","Cloud computing;Servers;Data integrity;Distributed databases;Cryptography;Data models;cloud storage;data integrity;Proof of Data Possession;set homomorphic relations;cryptographic cloud trends","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaaniche et al. 2018 - Cooperative Set Homomorphic Proofs for Data Possession Checking in Clouds.pdf","","","",""
"Conference paper","Kaaniche N,Laurent M","","A Secure Client Side Deduplication Scheme in Cloud Storage Environments","","","","","2014 6th International Conference on New Technologies, Mobility and Security (NTMS)","","","2014","","","1-7","All","Secure Dedup","","","","","","","","2014-03","","","","","","","http://dx.doi.org/10.1109/NTMS.2014.6814002;https://ieeexplore.ieee.org/abstract/document/6814002/;https://hal.archives-ouvertes.fr/hal-01263425/document","10.1109/NTMS.2014.6814002","","","","Recent years have witnessed the trend of leveraging cloud-based services for large scale content storage, processing, and distribution. Security and privacy are among top concerns for the public cloud environments. Towards these security challenges, we propose and implement, on OpenStack Swift, a new client-side deduplication scheme for securely storing and sharing outsourced data via the public cloud. The originality of our proposal is twofold. First, it ensures better confidentiality towards unauthorized users. That is, every client computes a per data key to encrypt the data that he intends to store in the cloud. As such, the data access is managed by the data owner. Second, by integrating access rights in metadata file, an authorized user can decipher an encrypted file only with his private key.","cloud computing;cryptography;data privacy;storage management;secure client side deduplication scheme;cloud storage environments;cloud-based services;large scale content storage;public cloud environments;OpenStack Swift;outsourced data sharing;metadata file;private key;data confidentiality;Servers;Cloud computing;Encryption;Permission;Protocols","","","","","","","","","","","","","","","","","","","","","","","","All Papers/K/Kaaniche and Laurent 2014 - A Secure Client Side Deduplication Scheme in Cloud Storage Environments.pdf","","","",""
"Journal article","Yang C,Ren J,Ma J","","Provable ownership of files in deduplication cloud storage","Security Comm. Networks","Security and Communication Networks","","","","","","2015","8","14","2457-2468","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","PoW","","","","","","","","2015-09-25","","","","","1939-0114","","http://doi.wiley.com/10.1002/sec.784;https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fsec.784;http://dx.doi.org/10.1002/sec.784;https://onlinelibrary.wiley.com/doi/abs/10.1002/sec.784;https://onlinelibrary.wiley.com/doi/pdf/10.1002/sec.784","10.1002/sec.784","","","","Abstract With the rapid adoption of cloud storage services, a great deal of data is being stored at remote servers, so a new technology, client-side deduplication, which stores only a single copy of repeating data, is proposed to identify the client's deduplication and save the bandwidth of uploading copies of existing files to the server. It was recently found, however, that this promising technology is vulnerable to a new kind of attack in which by learning just a small piece of information about the file, namely its hash value, an attacker is able to obtain the entire file from the server. In this paper, to solve this problem, we propose a cryptographically secure and efficient scheme for a client to prove to the server his ownership on the basis of actual possession of the entire original file instead of only partial information about it. Our scheme utilizes the technique of spot checking in which the client only needs to access small portions of the original file, dynamic coefficients and randomly chosen indices of the original files. Our extensive security analysis shows that the proposed scheme can generate provable ownership of the file and maintain high detection probability of client misbehavior. Both performance analysis and simulation results demonstrate that our proposed scheme is much more efficient than the existing schemes, especially in reducing the burden of the client. Copyright ? 2013 John Wiley & Sons, Ltd.","","Yang et al. also proposed a cryptographically secure and
efficient scheme to check the ownership of a file, in which a
client proves to the server that it indeed possesses the entire
file without uploading the file. By relying on dynamic
spot checking, a data holder only needs to access small but
dynamic portions of the original file to generate the proof of
possession of the original file, thus greatly reducing the burden of computation on the data holder and minimizing the
communication cost between the data holder and CSP. At
the same time, by utilizing dynamic coefficients and randomly chosen indices of the original files, the scheme mixes
the randomly sampled portions of the original file with the
dynamic coefficients to generate the unique proof in every
challenge. The work focuses on ownership proof of the
uploaded data during data deduplication.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Y/Yang et al. 2015 - Provable ownership of files in deduplication cloud storage.pdf","","","",""
"Miscellaneous","Jia XY,Xiao-Ying JIA,Bao LI,Ya-Min LIU","","Random Oracle Model","Journal of Software","","","","","","","2012","23","1","140-151","All","Security Proof","","","","","","","","2012","","","","","","","http://dx.doi.org/10.3724/sp.j.1001.2012.04092","10.3724/sp.j.1001.2012.04092","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Canetti R,Goldreich O,Halevi S","","The Random Oracle Methodology, Revisited","J. ACM","Journal of the ACM","","","","","","2004","51","4","557-594","All","Security Proof","","","ACM","New York, NY, USA","","","","2004-07","","","","","0004-5411","","http://doi.acm.org/10.1145/1008731.1008734;http://dx.doi.org/10.1145/1008731.1008734;https://dl.acm.org/citation.cfm?id=1008734;https://arxiv.org/pdf/cs/0010019","10.1145/1008731.1008734","","","","","CS-proofs, Correlation intractability, cryptography, diagonalization, the random-oracle model","","","","","","","","","","","","","","","","","","","","","","","","All Papers/C/Canetti et al. 2004 - The Random Oracle Methodology, Revisited.pdf","","","",""
"Conference paper","Maurer U,Renner R,Holenstein C","","Indifferentiability, Impossibility Results on Reductions, and Applications to the Random Oracle Methodology","","","","","Theory of Cryptography","","","2004","","","21-39","All","Security Proof","","","Springer Berlin Heidelberg","","","","","2004","","","","","","","http://dx.doi.org/10.1007/978-3-540-24638-1_2;https://link.springer.com/chapter/10.1007/978-3-540-24638-1_2;https://link.springer.com/content/pdf/10.1007/978-3-540-24638-1_2.pdf","10.1007/978-3-540-24638-1_2","","","","The goals of this paper are two-fold. First we introduce and motivate a generalization of the fundamental concept of the indistinguishability of two systems, called indifferentiability. This immediately leads to a generalization of the related notion of reducibility of one system to another. In contrast to the conventional notion of indistinguishability, indifferentiability is applicable in settings where a possible adversary is assumed to have access to additional information about the internal state of the involved systems, for instance the public parameter selecting a member from a family of hash functions.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Maurer et al. 2004 - Indifferentiability, Impossibility Results on Reductions, and Applications to the Random Oracle Methodology.pdf","","","",""
"Conference paper","Dziembowski S","","Intrusion-Resilience Via the Bounded-Storage Model","","","","","Theory of Cryptography","","","2006","","","207-224","All","Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2006","","","","","","","http://dx.doi.org/10.1007/11681878_11;https://link.springer.com/chapter/10.1007/11681878_11;https://link.springer.com/content/pdf/10.1007/11681878_11.pdf;https://eprint.iacr.org/2005/179.pdf","10.1007/11681878_11","","","","We introduce a new method of achieving intrusion-resilience in the cryptographic protocols. More precisely we show how to preserve security of such protocols, even if a malicious program (e.g. a virus) was installed on a computer of an honest user (and it was later removed). The security of our protocols relies on the assumption that the amount of data that the adversary can transfer from the infected machine is limited (however, we allow the adversary to perform any efficient computation on user’s private data, before deciding on what to transfer). We focus on two cryptographic tasks, namely: session-key generation and entity authentication. Our method is based on the results from the Bounded-Storage Model.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/D/Dziembowski 2006 - Intrusion-Resilience Via the Bounded-Storage Model.pdf","","","",""
"Journal article","Liskov M","","A practical, perfectly secure password scheme in the bounded retrieval model","IACR Cryptology ePrint Archive","","","","","","","2017","","","","All","Sec Techniques/Protocols","","","eprint.iacr.org","","","","","2017","","","","","","","https://eprint.iacr.org/2017/917.pdf","","","","","In this paper, we present a practical password scheme due to Spilman, which is perfectly secure in the bounded retrieval model, assuming ideal hash functions. The construction is based on a hash-like function computed by a third party “facilitator”. The facilitator is trusted, and security derives from the facilitator's long random secret, although the adversary is assumed to be able to retrieve a large fraction of that secret. Unlike the traditional “salted and hashed password” approach, this scheme is secure against an adversary capable of …","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Liskov 2017 - A practical, perfectly secure password scheme in the bounded retrieval model.pdf","","","",""
"Conference paper","Abadi M,Boneh D,Mironov I,Raghunathan A,Segev G","","Message-Locked Encryption for Lock-Dependent Messages","","","","","Advances in Cryptology – CRYPTO 2013","","","2013","","","374-391","All;Thesis","CE/MLE;Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","http://dx.doi.org/10.1007/978-3-642-40041-4_21;https://link.springer.com/chapter/10.1007/978-3-642-40041-4_21;https://link.springer.com/content/pdf/10.1007/978-3-642-40041-4_21.pdf;https://link.springer.com/content/pdf/10.1007%2F978-3-642-40041-4_21.pdf","10.1007/978-3-642-40041-4_21","","","","Motivated by the problem of avoiding duplication in storage systems, Bellare, Keelveedhi, and Ristenpart have recently put forward the notion of Message-Locked Encryption (MLE) schemes which subsumes convergent encryption and its variants. Such schemes do not rely on permanent secret keys, but rather encrypt messages using keys derived from the messages themselves.","","<div>Overleaf complains, have to remove unicode character from Martin in Abadi's name</div>Nice description of convergent encryption.<div>Bellare adds New primitive, MLE</div><div></div>","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Abadi et al. 2013 - Message-Locked Encryption for Lock-Dependent Messages.pdf","","","",""
"Miscellaneous","Stanek J,Kencl L","","Enhanced Secure Thresholded Data Deduplication Scheme for Cloud Storage","IEEE Transactions on Dependable and Secure Computing","","","","","","","2018","15","4","694-707","All","Maintaining Privacy;Thresholding","","","","","","","","2018","","","","","","","http://dx.doi.org/10.1109/tdsc.2016.2603501","10.1109/tdsc.2016.2603501","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Stanek and Kencl 2018 - Enhanced Secure Thresholded Data Deduplication Scheme for Cloud Storage.pdf","","","",""
"Miscellaneous","Singh P,Agarwal N,Raman B","","Secure data deduplication using secret sharing schemes over cloud","Future Generation Computer Systems","","","","","","","2018","88","","156-167","All","SSSS/RSSS/Convergent Dispersal","","","","","","","","2018","","","","","","","http://dx.doi.org/10.1016/j.future.2018.04.097","10.1016/j.future.2018.04.097","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Miscellaneous","Archana K,Patil MS,Department of Computer Science and Engineering,AIET College,Karnataka,India","","Secure Data Deduplication with Dynamic Ownership Management in Cloud Storage","International Journal of Trend in Scientific Research and Development","","","","","","","2018","-2","-4","2273-2277","All","Ownership Management","","","","","","","","2018","","","","","","","http://dx.doi.org/10.31142/ijtsrd14486","10.31142/ijtsrd14486","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/Archana et al. 2018 - Secure Data Deduplication with Dynamic Ownership Management in Cloud Storage.pdf","","","",""
"Miscellaneous","Manimekalai R,Chandra Kumar Peter M,Periyar Maniammai Institute of Science and Technology,Thanjavur,Nadu T,India","","Attribute-Based Storage Supporting Secure Deduplication of Encrypted Data in Cloud","International Journal of Trend in Scientific Research and Development","","","","","","","2018","-2","-4","252-254","All","Secure Dedup","","","","","","","","2018","","","","","","","http://dx.doi.org/10.31142/ijtsrd13014","10.31142/ijtsrd13014","","","","","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/M/Manimekalai et al. 2018 - Attribute-Based Storage Supporting Secure Deduplication of Encrypted Data in Cloud.pdf","","","",""
"Miscellaneous","A. R,D. R","","Survey on Data Deduplication for Cloud Storage to Reduce Fragmentation","International Journal of Computer Applications","","","","","","","2016","134","5","14-17","All","Secure Dedup","","","","","","","","2016","","","","","","","http://dx.doi.org/10.5120/ijca2016907942","10.5120/ijca2016907942","","","","","","No encryption here, just deduplicating plaintext.","","","","","","","","","","","","","","","","","","","","","","","All Papers/A/A. and D. 2016 - Survey on Data Deduplication for Cloud Storage to Reduce Fragmentation.pdf","","","",""
"Miscellaneous","AjitPatil A,Kulkarni D","","A Survey On: Secure Data Deduplication on Hybrid Cloud Storage Architecture","International Journal of Computer Applications","","","","","","","2015","110","3","29-32","All","Survey","","","","","","","","2015","","","","","","","http://dx.doi.org/10.5120/19298-0742","10.5120/19298-0742","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Shin Y,Koo D,Hur J","","A Survey of Secure Data Deduplication Schemes for Cloud Storage Systems","ACM Comput. Surv.","","","","","","","2017","49","4","74:1-74:38","All;Thesis;p-scailbib","Survey","","","ACM","New York, NY, USA","","","","2017-01","","","","","0360-0300","","http://doi.acm.org/10.1145/3017428;http://dx.doi.org/10.1145/3017428;https://dl.acm.org/citation.cfm?id=3017428","10.1145/3017428","","","","","Message-dependent encryption, deterministic information dispersal, proof of ownership, traffic obfuscation","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Shin et al. 2017 - A Survey of Secure Data Deduplication Schemes for Cloud Storage Systems.pdf","","","",""
"Conference paper","Zhou Y,Feng D,Xia W,Fu M,Huang F,Zhang Y,Li C","","SecDep: A user-aware efficient fine-grained secure deduplication scheme with multi-level key management","","","","","2015 31st Symposium on Mass Storage Systems and Technologies (MSST)","","","2015","","","1-14","All;Superchunking Paper Sources/CDC/Chunking;Grouped by Publication/MSST;p-scailbib","Secure Dedup;SSSS/RSSS/Convergent Dispersal;Chunking","","","","","","","","2015-05","","","","","2160-195X","","http://dx.doi.org/10.1109/MSST.2015.7208297;https://ieeexplore.ieee.org/abstract/document/7208297/;https://www.researchgate.net/profile/Yukun_Zhou/publication/278033383_SecDep_A_User-Aware_Efficient_Fine-Grained_Secure_Deduplication_Scheme_with_Multi-Level_Key_Management/links/557ada4808aee5c4604491af/SecDep-A-User-Aware-Efficient-Fine-Grained-Secure-Deduplication-Scheme-with-Multi-Level-Key-Management.pdf;http://storageconference.us/2015/Papers/24.Zhou.pdf","10.1109/MSST.2015.7208297","","","","Nowadays, many customers and enterprises backup their data to cloud storage that performs deduplication to save storage space and network bandwidth. Hence, how to perform secure deduplication becomes a critical challenge for cloud storage. According to our analysis, the state-of-the-art secure deduplication methods are not suitable for cross-user finegrained data deduplication. They either suffer brute-force attacks that can recover files falling into a known set, or incur large computation (time) overheads. Moreover, existing approaches of convergent key management incur large space overheads because of the huge number of chunks shared among users. Our observation that cross-user redundant data are mainly from the duplicate files, motivates us to propose an efficient secure deduplication scheme SecDep. SecDep employs User-Aware Convergent Encryption (UACE) and Multi-Level Key management (MLK) approaches. (1) UACE combines cross-user file-level and inside-user chunk-level deduplication, and exploits different secure policies among and inside users to minimize the computation overheads. Specifically, both of file-level and chunk-level deduplication use variants of Convergent Encryption (CE) to resist brute-force attacks. The major difference is that the file-level CE keys are generated by using a server-aided method to ensure security of cross-user deduplication, while the chunk-level keys are generated by using a user-aided method with lower computation overheads. (2) To reduce key space overheads, MLK uses file-level key to encrypt chunk-level keys so that the key space will not increase with the number of sharing users. Furthermore, MLK splits the file-level keys into share-level keys and distributes them to multiple key servers to ensure security and reliability of file-level keys. Our security analysis demonstrates that SecDep ensures data confidentiality and key security. Our experiment results based on several large real-world datasets show that SecDep is more time-efficient and key-space-efficient than the state-of-the-art secure deduplication approaches.","cloud computing;cryptography;data privacy;SecDep;user-aware efficient fine-grained secure deduplication scheme;cloud storage;cross-user finegrained data deduplication;brute-force attacks;user-aware convergent encryption;UACE;multilevel key management approaches;MLK approaches;cross-user file-level deduplication;inside-user chunk-level deduplication;server-aided method;cross-user deduplication security;user-aided method;computation overheads;key space overhead reduction;security analysis;data confidentiality;key security;Encryption;Servers;Protocols;Resists","See Abstract, great explanation of this clever scheme. Most dedup comes at file level. Once file is popular, allow client side dedup. Most unique chunks are private. Multi-level key mgmt, reduce key space by only needing key for entire file. Shares file level keys with RSSS. Chunk key includes user specific info, partition user chunks from one another.","","","","","","","","","","","","","","","","","","","","","","","All Papers/Z/Zhou et al. 2015 - SecDep - A user-aware efficient fine-grained secure deduplication scheme with multi-level key management.pdf","","","",""
"Conference paper","Ling CW,Datta A","","InterCloud RAIDer: A Do-It-Yourself Multi-cloud Private Data Backup System","","","","","Distributed Computing and Networking","","","2014","","","453-468","All","Code/OpenSource","","","Springer Berlin Heidelberg","","","","","2014","","","","","","","http://dx.doi.org/10.1007/978-3-642-45249-9_30;https://link.springer.com/chapter/10.1007/978-3-642-45249-9_30;https://pdfs.semanticscholar.org/8b32/7999a9e46369d7cf0fe4a6181e2176f70003.pdf","10.1007/978-3-642-45249-9_30","","","","In this paper, we introduce InterCloud RAIDer, which realizes a multi-cloud private data backup system by composing (i) a data deduplication technique to reduce the overall storage overhead, (ii) erasure coding to achieve redundancy at low overhead, which is dispersed across multiple cloud services to realize fault-tolerance against individual service providers, specifically we use non-systematic instances of erasure codes to provide a basic level of privacy from individual cloud stores, and finally, (iii) a proof of data possession mechanism to detect misbehaving services - where we optimize the implementation by exploiting hash digests that are created in the prior deduplication phase. Apart from the uniqueness and non-triviality of putting these modules together, the system design also had to deal with artefacts and heterogeneity across different cloud storage services we used, namely Dropbox, Google drive and SkyDrive.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/L/Ling and Datta 2014 - InterCloud RAIDer - A Do-It-Yourself Multi-cloud Private Data Backup System.pdf","","","",""
"Conference paper","Duan Y","","Distributed Key Generation for Encrypted Deduplication: Achieving the Strongest Privacy","","","","","Proceedings of the 6th Edition of the ACM Workshop on Cloud Computing Security","","","2014","","","57-68","All","CE/MLE;Sec Techniques/Protocols;Security Proof","","","ACM","New York, NY, USA","","Scottsdale, Arizona, USA","","2014","","","9781450332392","","","","http://doi.acm.org/10.1145/2664168.2664169;http://dx.doi.org/10.1145/2664168.2664169;https://dl.acm.org/citation.cfm?id=2664169;https://www.researchgate.net/profile/Yitao_Duan2/publication/267784291_Distributed_Key_Generation_for_Encrypted_Deduplication_Achieving_the_Strongest_Privacy/links/545a75ba0cf2c16efbbbbd07.pdf","10.1145/2664168.2664169","","","","Large-scale cloud storage systems often attempt to achieve two seemingly conflicting goals:(1) the systems need to reduce the copies of redundant data to save space, a process called deduplication; and (2) users demand encryption of their data to ensure privacy. Conventional encryption makes deduplication on ciphertexts ineffective, as it destroys data redundancy. A line of work, originated from Convergent Encryption [27], and evolved into Message Locked Encryption [13] and the latest DupLESS architecture [12], strives to solve …","cloud computing security, deduplication, deterministic encryption","","","","","","CCSW '14","","","","","","","","","","","","","","","","","","All Papers/D/Duan 2014 - Distributed Key Generation for Encrypted Deduplication - Achieving the Strongest Privacy.pdf","","","",""
"Conference paper","Boyd C,Davies GT,Gjøsteen K,Raddum H,Toorani M","","Definitions for Plaintext-Existence Hiding in Cloud Storage","","","","","Proceedings of the 13th International Conference on Availability, Reliability and Security","","","2018","","","41:1-41:7","All","Maintaining Privacy;Random Oracle;Security Proof","","","ACM","New York, NY, USA","","Hamburg, Germany","","2018","","","9781450364485","","","","http://doi.acm.org/10.1145/3230833.3234515;http://dx.doi.org/10.1145/3230833.3234515;https://dl.acm.org/citation.cfm?id=3234515;https://eprint.iacr.org/2018/748.pdf","10.1145/3230833.3234515","","","","Cloud storage services use deduplication for saving bandwidth and storage. An adversary can exploit side-channel information in several attack scenarios when deduplication takes place at the client side, leaking information on whether a specific plaintext exists in the cloud …","Cloud Storage, Information Leakage, Side-channel analysis","","","","","","ARES 2018","","","","","","","","","","","","","","","","","","All Papers/B/Boyd et al. 2018 - Definitions for Plaintext-Existence Hiding in Cloud Storage.pdf","","","",""
"Conference paper","Stanek J,Sorniotti A,Androulaki E,Kencl L","","A Secure Data Deduplication Scheme for Cloud Storage","","","","","Financial Cryptography and Data Security","","","2014","","","99-118","All;Thesis;p-scailbib","CE/MLE;Maintaining Privacy;Random Oracle","","","Springer Berlin Heidelberg","","","","","2014","","","","","","","http://dx.doi.org/10.1007/978-3-662-45472-5_8;https://link.springer.com/chapter/10.1007/978-3-662-45472-5_8;https://domino.research.ibm.com/library/cyberdig.nsf/papers/CEDD204E280E76A685257BDD004ED649/$File/rz3852.pdf","10.1007/978-3-662-45472-5_8","","","","As more corporate and private users outsource their data to cloud storage providers, recent data breach incidents make end-to-end encryption an increasingly prominent requirement. Unfortunately, semantically secure encryption schemes render various cost-effective storage optimization techniques, such as data deduplication, ineffective. We present a novel idea that differentiates data according to their popularity. Based on this idea, we design an encryption scheme that guarantees semantic security for unpopular data and provides weaker security and better storage and bandwidth benefits for popular data. This way, data deduplication can be effective for popular data, whilst semantically secure encryption protects unpopular content. We show that our scheme is secure under the Symmetric External Decisional Diffie-Hellman Assumption in the random oracle model.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/S/Stanek et al. 2014 - A Secure Data Deduplication Scheme for Cloud Storage.pdf","","","",""
"Conference paper","Mandagere N,Zhou P,Smith MA,Uttamchandani S","","Demystifying Data Deduplication","","","","","Proceedings of the ACM/IFIP/USENIX Middleware '08 Conference Companion","","","2008","","","12-17","All;Thesis","Secure Dedup","","","ACM","New York, NY, USA","","Leuven, Belgium","","2008","","","9781605583693","","","","http://doi.acm.org/10.1145/1462735.1462739;http://dx.doi.org/10.1145/1462735.1462739;https://dl.acm.org/citation.cfm?id=1462739;http://cs.brown.edu/courses/cs227/archives/2016/papers/p12-mandagere.pdf","10.1145/1462735.1462739","","","","Effectiveness and tradeoffs of deduplication technologies are not well understood--vendors tout Deduplication as a"" silver bullet"" that can help any enterprise optimize its deployed storage capacity. This paper aims to provide a comprehensive taxonomy and experimental …","compression, deduplication","","","","","","Companion '08","","","","","","","","","","","","","","","","","","All Papers/M/Mandagere et al. 2008 - Demystifying Data Deduplication.pdf","","","",""
"Journal article","Paulo J,Pereira J","","A Survey and Classification of Storage Deduplication Systems","ACM Comput. Surv.","","","","","","","2014","47","1","11:1-11:30","All;Fingerdiff Refs;""Super-chunk"" term sources;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Secure Dedup;Survey;Super-chunking","","","ACM","New York, NY, USA","","","","2014-06","","","","","0360-0300","","http://doi.acm.org/10.1145/2611778;http://dx.doi.org/10.1145/2611778;https://dl.acm.org/citation.cfm?id=2611778;https://repositorio.inesctec.pt/bitstream/123456789/4154/1/P-009-SED.pdf","10.1145/2611778","","","","The automatic elimination of duplicate data in a storage system, commonly known as deduplication, is increasingly accepted as an effective technique to reduce storage costs. Thus, it has been applied to different storage types, including archives and backups, primary storage, within solid-state drives, and even to random access memory. Although the general approach to deduplication is shared by all storage types, each poses specific challenges and leads to different trade-offs and solutions. This diversity is often misunderstood, thus …","Storage management, deduplication, file systems","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Paulo and Pereira 2014 - A Survey and Classification of Storage Deduplication Systems.pdf","","","",""
"Journal article","Eshghi K,Tang HH","","A framework for analyzing and improving content-based chunking algorithms","Hewlett-Packard Labs Technical Report TR","","","","","","","2005","30","2005","","All;Thesis","Secure Dedup;Chunking","","","","","","","","2005","","","","","","","https://www.hpl.hp.com/techreports/2005/HPL-2005-30R1.pdf","","","","","We present a framework for analyzing contentbased chunking algorithms, as used for example in the Low Bandwidth Networked File System. We use this framework for the evaluation of the basic sliding window algorithm, and its two known variants. We develop a new chunking algorithm that performs signi cantly better than the known algorithms on real, non-random data.","","Introduces TTTD","","","","","","","","","","","","","","","","","","","","","","","All Papers/E/Eshghi and Tang 2005 - A framework for analyzing and improving content-based chunking algorithms.pdf","","","",""
"Conference paper","Broder AZ","","Identifying and Filtering Near-Duplicate Documents","","","","","Proceedings of the 11th Annual Symposium on Combinatorial Pattern Matching","","","2000","","","1-10","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing;Thesis","Fingerprint-Indexing","","","Springer-Verlag","Berlin, Heidelberg","","","","2000","","","9783540676331","","","","http://dl.acm.org/citation.cfm?id=647819.736184;https://dl.acm.org/citation.cfm?id=736184","","","","","","","","","","","","COM '00","","","","","","","","","","","","","","","","","","All Papers/B/Broder 2000 - Identifying and Filtering Near-Duplicate Documents.pdf","","","",""
"Conference paper","Bellare M,Rogaway P","","Random Oracles Are Practical: A Paradigm for Designing Efficient Protocols","","","","","Proceedings of the 1st ACM Conference on Computer and Communications Security","","","1993","","","62-73","All","Security Proof","","","ACM","New York, NY, USA","","Fairfax, Virginia, USA","","1993","","","9780897916295","","","","http://doi.acm.org/10.1145/168588.168596;http://dx.doi.org/10.1145/168588.168596;https://dl.acm.org/citation.cfm?id=168596;http://caislab.kaist.ac.kr/lecture/2010/spring/cs548/basic/B11.pdf","10.1145/168588.168596","","","","We argue that the random oracle model—where all parties have access to a public random oracle—provides a bridge between cryptographic theory and cryptographic practice. In the paradigm we suggest, a practical protocol P is produced by first devising and proving correct a protocol PR for the random oracle model, and then replacing oracle accesses by the computation of an “appropriately chosen” function h. This paradigm yields protocols much more efficient than standard ones while retaining many of the advantages of provable …","","","","","","","CCS '93","","","","","","","","","","","","","","","","","","All Papers/B/Bellare and Rogaway 1993 - Random Oracles Are Practical - A Paradigm for Designing Efficient Protocols.pdf","","","",""
"Preprint","Pendlebury F,Pierazzi F,Jordaney R,Kinder J,Cavallaro L","","TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time","","","","","","arXiv [cs.CR]","","2018","","","","All","Machine Learning;Threat Detection","","","","","","","","2018-07-20","","","","","","","http://arxiv.org/abs/1807.07838","","","1807.07838","","Is Android malware classification a solved problem? Published F1 scores of up to 0.99 appear to leave very little room for improvement. In this paper, we argue that results are commonly inflated due to two pervasive sources of experimental bias: ""spatial bias"" caused by distributions of training and testing data that are not representative of a real-world deployment; and ""temporal bias"" caused by incorrect time splits of training and testing sets, leading to impossible configurations. We propose a set of space and time constraints for experiment design that eliminates both sources of bias. We introduce a new metric that summarizes the expected robustness of a classifier in a real-world setting, and we present an algorithm to tune its performance. Finally, we demonstrate how this allows us to evaluate mitigation strategies for time decay such as active learning. We have implemented our solutions in TESSERACT, an open source evaluation framework for comparing malware classifiers in a realistic setting. We used TESSERACT to evaluate three Android malware classifiers from the literature on a dataset of 129K applications spanning over three years. Our evaluation confirms that earlier published results are biased, while also revealing counter-intuitive performance and showing that appropriate tuning can lead to significant improvements.","","","","","","","","","arXiv","1807.07838","cs.CR","","","","","","","","","","","","","","All Papers/P/Pendlebury et al. 2018 - TESSERACT - Eliminating Experimental Bias in Malware Classification across Space and Time.pdf","","","",""
"Conference paper","Bellare M,Keelveedhi S,Ristenpart T","","Message-Locked Encryption and Secure Deduplication","","","","","Advances in Cryptology – EUROCRYPT 2013","","","2013","","","296-312","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","CE/MLE;Secure Dedup;Sec Techniques/Protocols","","","Springer Berlin Heidelberg","","","","","2013","","","","","","","https://link.springer.com/chapter/10.1007/978-3-642-38348-9_18;https://link.springer.com/content/pdf/10.1007%2F978-3-642-38348-9_18.pdf","10.1007/978-3-642-38348-9","","","","We formalize a new cryptographic primitive that we call Message-Locked Encryption (MLE), where the key under which encryption and decryption are performed is itself derived from the message. MLE provides a way to achieve secure deduplication (space-efficient secure outsourced storage), a goal currently targeted by numerous cloudstorage providers. We provide definitions both for privacy and for a form of integrity that we call tag consistency. Based on this foundation, we make both practical and theoretical contributions. On the practical side, we provide ROM security analyses of a natural family of MLE schemes that includes deployed schemes. On the theoretical side the challenge is standard model solutions, and we make connections with deterministic encryption, hash functions secure on correlated inputs and the sample-then-extract paradigm to deliver schemes under different assumptions and for different classes of message sources. Our work shows that MLE is a primitive of both practical and theoretical interest.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/B/Bellare et al. 2013 - Message-Locked Encryption and Secure Deduplication.pdf;All Papers/B/Bellare et al. 2013 - Message-Locked Encryption and Secure Deduplication.pdf","","","",""
"Journal article","Harnik D,Pinkas B,Shulman-Peleg A","","Side Channels in Cloud Services: Deduplication in Cloud Storage","IEEE Security Privacy","","","","","","","2010","8","6","40-47","All;SCAIL Bibliography;PR-SCAIL;2023PhDReport;p-scailbib;Thesis","Maintaining Privacy;CE/MLE;Secure Dedup","","","","","","","","2010-11","","","","","1540-7993","","http://dx.doi.org/10.1109/MSP.2010.187;https://ieeexplore.ieee.org/document/5655241","10.1109/MSP.2010.187","","","","As the volume of data increases, so does the demand for online storage services, from simple backup services to cloud storage infrastructures. Although deduplication is most effective when applied across multiple users, cross-user deduplication has serious privacy implications. Some simple mechanisms can enable cross-user deduplication while greatly reducing the risk of data leakage. Cloud storage refers to scalable and elastic storage capabilities delivered as a service using Internet technologies with elastic provisioning and usebased pricing that doesn't penalize users for changing their storage consumption without notice.","data compression;data privacy;Internet;security of data;cloud services;cloud storage;cross-user deduplication;data leakage risk;online storage service;Internet;storage consumption;data privacy;Cloud computing;Data storage;Data management;Cloud storage;deduplication;side channels;differential privacy","","","","","","","","","","","","","","","","","","","","","","","","All Papers/H/Harnik et al. 2010 - Side Channels in Cloud Services - Deduplication in Cloud Storage.pdf","","","",""
"Conference paper","Policroniades C,Pratt I","","Alternatives for Detecting Redundancy in Storage Systems Data","","","","","USENIX Annual Technical Conference, General Track","","","2004","","","73-86","All;Superchunking Paper Sources/CDC/Chunking;Grouped by Publication/Usenix ATC","Chunking","","","","","","","","2004","","","","","","","https://www.usenix.org/legacy/publications/library/proceedings/usenix04/tech/general/full_papers/policroniades/policroniades_html/rabinPaper.html","","","","","Storage systems frequently maintain identical copies of data. Identifying such data can assist in the design of solutions in which data storage, transmission, and management are optimised. In this paper we evaluate three methods used to discover identical portions of data: whole file content hashing, fixed size blocking, and a chunking strategy that uses Rabin fingerprints to delimit content-defined data chunks. We assess how effective each of these strategies is in finding identical sections of data. In our experiments, we analysed diverse data sets from a variety of different types of storage systems including a mirrored section of sunsite.org.uk, different data profiles in the file system infrastructure of the Cambridge University Computer Laboratory, source code distribution trees, compressed data, and packed files. We report our experimental results and present a comparative analysis of these techniques. This study also shows how levels of similarity differ between data sets and file types. Finally, we discuss the advantages and disadvantages in the application of these methods in the light of our experimental results.","","","","","","","","","","","","","","","","","","","","","","","","","All Papers/P/Policroniades and Pratt 2004 - Alternatives for Detecting Redundancy in Storage Systems Data.pdf","","","",""
"Conference paper","Almubairik NA,Wills G","","Automated penetration testing based on a threat model","","","","","2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)","","","2016","","","413-414","All","Threat Detection;Sec Techniques/Protocols","","","","","","","","2016-12","","","","","","","http://dx.doi.org/10.1109/ICITST.2016.7856742;https://ieeexplore.ieee.org/abstract/document/7856742/","10.1109/ICITST.2016.7856742","","","","The aim of this work is to propose a systematic penetration testing algorithm guided by a threat model. The use of the threat model in penetration testing ensures that all existing threats are checked and no threat is overlooked through the penetration test process. The objectives of this work are as follows: assembling a package of penetration testing tools (toolkit) to test the security of a equation system. Moreover, considering standard methodologies to design the automated penetration testing. A number of methodologies have been followed during the design of the algorithm. First, a threat model designed at the IT Innovation Centre was used extract threats. These threats were used as a starting point for the penetration testing. Second, the NIST 800-115 standard for penetration testing was followed. Applying the proposed automated penetration testing algorithm to a real system contributes to the reduction of consequences which can result from malicious attacks.","security of data;automated penetration testing algorithm;threat model;systematic penetration testing algorithm;equation system security;penetration testing tool package assembling;IT Innovation Centre;threat extraction;NIST 800-115 standard;malicious attacks;Testing;Systematics;Algorithm design and analysis;Automated penetration testing;system model;threat modelss","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Theodoridis S,Koutroumbas K","","Pattern Recognition, Academic Press","New York","New York","","","","","","1999","","","","All;Superchunking Paper Sources/CDC/Fingerprinting/Indexing","Fingerprint-Indexing","","","","","","","","1999","","","","","0028-7369","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Journal article","Stamatiou YC","","Formal correctness of security protocols by Giampaolo Bella, Springer-Verlag","ACM SIGACT News","ACM SIGACT News","","","","","","2010","41","1","47-50","All","Sec Techniques/Protocols;Security Proof","","","ACM","","","","","2010-03-01","","2019-02-24","","","0163-5700","","https://dl.acm.org/citation.cfm?doid=1753171.1753185;http://dx.doi.org/10.1145/1753171.1753185","10.1145/1753171.1753185","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Book","Bella G","","Formal Correctness of Security Protocols: With 62 Figures and 4 Tables","","","","","","","","2007","","","","All","Sec Techniques/Protocols;Security Proof","","","","","","","","2007","","","9783540681342","","","","http://link.springer.com/10.1007/978-3-540-68136-6;http://dx.doi.org/10.1007/978-3-540-68136-6","10.1007/978-3-540-68136-6","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Book","Cremers C,Mauw S","","Operational Semantics and Verification of Security Protocols","","","","","","","","2012","","","","All","Sec Techniques/Protocols;Security Proof","","","","","","","","2012","","","9783540786351","","","","http://link.springer.com/10.1007/978-3-540-78636-8;http://dx.doi.org/10.1007/978-3-540-78636-8","10.1007/978-3-540-78636-8","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Book chapter","Cremers CJ","Gupta A,Malik S","The Scyther Tool: Verification, Falsification, and Analysis of Security Protocols","","","Computer Aided Verification","","","","","2008","5123","","414-418","All","Sec Techniques/Protocols;Security Proof","","","Springer Berlin Heidelberg","Berlin, Heidelberg","","","","2008","","","9783540705437","9783540705451","0302-9743","1611-3349","http://link.springer.com/10.1007/978-3-540-70545-1_38;http://www.springerlink.com/index/pdf/10.1007/978-3-540-70545-1_38;http://dx.doi.org/10.1007/978-3-540-70545-1_38","10.1007/978-3-540-70545-1_38","","","","","","","","","","","Lecture Notes in Computer Science","","","","","","","","","","","","","","","","","","All Papers/C/Cremers 2008 - The Scyther Tool - Verification, Falsification, and Analysis of Security Protocols.pdf","","","",""
"Archival material","Suleyman Kardas AM","","Solving the Secure Storage Dilemma: An Efficient Scheme for Secure Deduplication with Privacy-Preserving Public Auditing","","","","","","","","2016","","","","All","Maintaining Privacy;Sec Techniques/Protocols;Secure Dedup","","","","","","","","2016","","2019-02-24","","","","","https://eprint.iacr.org/2016/696.pdf","","","","","","","","","","","","","","","","","","","","Cryptology ePrint Archive","","","","","","","","","","","","","",""
"Journal article","Chu KW,Lam SK,Wong MH","","An Efficient Hash-Based Algorithm for Sequence Data Searching","Comput. J.","Computer Journal","","","","","","1998","41","6","402-415","All","Secure Dedup","","","Oxford University Press","","","","","1998-01-01","","2019-02-24","","","0010-4620","","https://academic.oup.com/comjnl/article-abstract/41/6/402/397219?redirectedFrom=fulltext;http://dx.doi.org/10.1093/comjnl/41.6.402","10.1093/comjnl/41.6.402","","","","In real life, data collected day by day often appear in sequences and this type of data is called sequence data. The technique of searching for similar patterns among sequence data is very important in many applications. We first point out that there are some deficiencies in the existing definitions of sequence similarity. We then introduce a definition of sequence similarity based on the shape of sequences. The definition is also extended to handle sequence matching with linear scaling in both amplitude and time dimensions. A fast sequence searching algorithm based on extendable hashing is also proposed. The algorithm can match linearly scaled sequences and guarantee that no qualified data subsequence is falsely rejected. Several experiments are performed on real data (stock price movement) and synthetic data to measure the performance of the algorithm in different aspects.","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""